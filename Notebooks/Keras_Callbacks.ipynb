{"cells":[{"cell_type":"markdown","source":"# Using Callbacks in Keras\n\nIn this notebook, we well see how to use pre-defined and custom callbacks in Keras for tasks such as chekpointing, learning rate scheduling, etc.\n\nWe'll use the same simple dataset and linear model of the previous notebook.\n","metadata":{"cell_id":"4a27a9f673c64a228cf0f7770aa04ae7","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing","metadata":{"cell_id":"33d1a9e6629c4f45967e875bb90a00e1","source_hash":"3be3ab08","execution_start":1667313855343,"execution_millis":2992,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-11-01 14:44:16.341915: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-01 14:44:16.476169: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2022-11-01 14:44:16.481176: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2022-11-01 14:44:16.481198: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-11-01 14:44:16.506332: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2022-11-01 14:44:17.241538: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2022-11-01 14:44:17.241600: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2022-11-01 14:44:17.241617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"##### Download the Auto-MPG dataset\n \nDownload the Auto-MPG dataset (seen in a previous notebook).","metadata":{"cell_id":"35c2fbafe997412aa17d1e6d807c0574","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\ncolumn_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n                'Acceleration', 'Model Year', 'Origin']\n\ndataset = pd.read_csv(url, names=column_names, na_values='?', comment='\\t', sep=' ', skipinitialspace=True)\ndataset = dataset.dropna()\ndataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\ndataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\ndataset.tail()","metadata":{"cell_id":"4f512b4dc0144f80b5d022763fbdf889","source_hash":"81a3a6c9","execution_start":1667313860334,"execution_millis":352,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":10,"row_count":5,"columns":[{"name":"MPG","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"27.0","max":"44.0","histogram":[{"bin_start":27,"bin_end":28.7,"count":2},{"bin_start":28.7,"bin_end":30.4,"count":0},{"bin_start":30.4,"bin_end":32.1,"count":2},{"bin_start":32.1,"bin_end":33.8,"count":0},{"bin_start":33.8,"bin_end":35.5,"count":0},{"bin_start":35.5,"bin_end":37.2,"count":0},{"bin_start":37.2,"bin_end":38.9,"count":0},{"bin_start":38.9,"bin_end":40.6,"count":0},{"bin_start":40.6,"bin_end":42.3,"count":0},{"bin_start":42.3,"bin_end":44,"count":1}]}},{"name":"Cylinders","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"4","max":"4","histogram":[{"bin_start":3.5,"bin_end":3.6,"count":0},{"bin_start":3.6,"bin_end":3.7,"count":0},{"bin_start":3.7,"bin_end":3.8,"count":0},{"bin_start":3.8,"bin_end":3.9,"count":0},{"bin_start":3.9,"bin_end":4,"count":0},{"bin_start":4,"bin_end":4.1,"count":5},{"bin_start":4.1,"bin_end":4.2,"count":0},{"bin_start":4.2,"bin_end":4.3,"count":0},{"bin_start":4.3,"bin_end":4.4,"count":0},{"bin_start":4.4,"bin_end":4.5,"count":0}]}},{"name":"Displacement","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"97.0","max":"140.0","histogram":[{"bin_start":97,"bin_end":101.3,"count":1},{"bin_start":101.3,"bin_end":105.6,"count":0},{"bin_start":105.6,"bin_end":109.9,"count":0},{"bin_start":109.9,"bin_end":114.2,"count":0},{"bin_start":114.2,"bin_end":118.5,"count":0},{"bin_start":118.5,"bin_end":122.8,"count":2},{"bin_start":122.8,"bin_end":127.1,"count":0},{"bin_start":127.1,"bin_end":131.4,"count":0},{"bin_start":131.4,"bin_end":135.7,"count":1},{"bin_start":135.7,"bin_end":140,"count":1}]}},{"name":"Horsepower","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"52.0","max":"86.0","histogram":[{"bin_start":52,"bin_end":55.4,"count":1},{"bin_start":55.4,"bin_end":58.8,"count":0},{"bin_start":58.8,"bin_end":62.2,"count":0},{"bin_start":62.2,"bin_end":65.6,"count":0},{"bin_start":65.6,"bin_end":69,"count":0},{"bin_start":69,"bin_end":72.4,"count":0},{"bin_start":72.4,"bin_end":75.8,"count":0},{"bin_start":75.8,"bin_end":79.2,"count":1},{"bin_start":79.2,"bin_end":82.6,"count":1},{"bin_start":82.6,"bin_end":86,"count":2}]}},{"name":"Weight","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"2130.0","max":"2790.0","histogram":[{"bin_start":2130,"bin_end":2196,"count":1},{"bin_start":2196,"bin_end":2262,"count":0},{"bin_start":2262,"bin_end":2328,"count":1},{"bin_start":2328,"bin_end":2394,"count":0},{"bin_start":2394,"bin_end":2460,"count":0},{"bin_start":2460,"bin_end":2526,"count":0},{"bin_start":2526,"bin_end":2592,"count":0},{"bin_start":2592,"bin_end":2658,"count":1},{"bin_start":2658,"bin_end":2724,"count":1},{"bin_start":2724,"bin_end":2790,"count":1}]}},{"name":"Acceleration","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"11.6","max":"24.6","histogram":[{"bin_start":11.6,"bin_end":12.9,"count":1},{"bin_start":12.9,"bin_end":14.2,"count":0},{"bin_start":14.2,"bin_end":15.5,"count":0},{"bin_start":15.5,"bin_end":16.8,"count":1},{"bin_start":16.8,"bin_end":18.1,"count":0},{"bin_start":18.1,"bin_end":19.400000000000002,"count":2},{"bin_start":19.400000000000002,"bin_end":20.700000000000003,"count":0},{"bin_start":20.700000000000003,"bin_end":22,"count":0},{"bin_start":22,"bin_end":23.300000000000004,"count":0},{"bin_start":23.300000000000004,"bin_end":24.6,"count":1}]}},{"name":"Model Year","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"82","max":"82","histogram":[{"bin_start":81.5,"bin_end":81.6,"count":0},{"bin_start":81.6,"bin_end":81.7,"count":0},{"bin_start":81.7,"bin_end":81.8,"count":0},{"bin_start":81.8,"bin_end":81.9,"count":0},{"bin_start":81.9,"bin_end":82,"count":0},{"bin_start":82,"bin_end":82.1,"count":5},{"bin_start":82.1,"bin_end":82.2,"count":0},{"bin_start":82.2,"bin_end":82.3,"count":0},{"bin_start":82.3,"bin_end":82.4,"count":0},{"bin_start":82.4,"bin_end":82.5,"count":0}]}},{"name":"Europe","dtype":"uint8","stats":{"unique_count":2,"nan_count":0,"min":"0","max":"1","histogram":[{"bin_start":0,"bin_end":0.1,"count":4},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":1}]}},{"name":"Japan","dtype":"uint8","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"USA","dtype":"uint8","stats":{"unique_count":2,"nan_count":0,"min":"0","max":"1","histogram":[{"bin_start":0,"bin_end":0.1,"count":1},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":4}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"MPG":"27.0","Cylinders":"4","Displacement":"140.0","Horsepower":"86.0","Weight":"2790.0","Acceleration":"15.6","Model Year":"82","Europe":"0","Japan":"0","USA":"1","_deepnote_index_column":"393"},{"MPG":"44.0","Cylinders":"4","Displacement":"97.0","Horsepower":"52.0","Weight":"2130.0","Acceleration":"24.6","Model Year":"82","Europe":"1","Japan":"0","USA":"0","_deepnote_index_column":"394"},{"MPG":"32.0","Cylinders":"4","Displacement":"135.0","Horsepower":"84.0","Weight":"2295.0","Acceleration":"11.6","Model Year":"82","Europe":"0","Japan":"0","USA":"1","_deepnote_index_column":"395"},{"MPG":"28.0","Cylinders":"4","Displacement":"120.0","Horsepower":"79.0","Weight":"2625.0","Acceleration":"18.6","Model Year":"82","Europe":"0","Japan":"0","USA":"1","_deepnote_index_column":"396"},{"MPG":"31.0","Cylinders":"4","Displacement":"119.0","Horsepower":"82.0","Weight":"2720.0","Acceleration":"19.4","Model Year":"82","Europe":"0","Japan":"0","USA":"1","_deepnote_index_column":"397"}]},"text/plain":"      MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n393  27.0          4         140.0        86.0  2790.0          15.6   \n394  44.0          4          97.0        52.0  2130.0          24.6   \n395  32.0          4         135.0        84.0  2295.0          11.6   \n396  28.0          4         120.0        79.0  2625.0          18.6   \n397  31.0          4         119.0        82.0  2720.0          19.4   \n\n     Model Year  Europe  Japan  USA  \n393          82       0      0    1  \n394          82       1      0    0  \n395          82       0      0    1  \n396          82       0      0    1  \n397          82       0      0    1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MPG</th>\n      <th>Cylinders</th>\n      <th>Displacement</th>\n      <th>Horsepower</th>\n      <th>Weight</th>\n      <th>Acceleration</th>\n      <th>Model Year</th>\n      <th>Europe</th>\n      <th>Japan</th>\n      <th>USA</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>393</th>\n      <td>27.0</td>\n      <td>4</td>\n      <td>140.0</td>\n      <td>86.0</td>\n      <td>2790.0</td>\n      <td>15.6</td>\n      <td>82</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>394</th>\n      <td>44.0</td>\n      <td>4</td>\n      <td>97.0</td>\n      <td>52.0</td>\n      <td>2130.0</td>\n      <td>24.6</td>\n      <td>82</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>32.0</td>\n      <td>4</td>\n      <td>135.0</td>\n      <td>84.0</td>\n      <td>2295.0</td>\n      <td>11.6</td>\n      <td>82</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>28.0</td>\n      <td>4</td>\n      <td>120.0</td>\n      <td>79.0</td>\n      <td>2625.0</td>\n      <td>18.6</td>\n      <td>82</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>31.0</td>\n      <td>4</td>\n      <td>119.0</td>\n      <td>82.0</td>\n      <td>2720.0</td>\n      <td>19.4</td>\n      <td>82</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"Split training and test set, separate features and labels:","metadata":{"cell_id":"7b6791a2d96d40bba05451b58b287de4","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"train_dataset = dataset.sample(frac=0.8, random_state=0)\ntest_dataset = dataset.drop(train_dataset.index)\n\ntrain_features = train_dataset.copy()\ntest_features = test_dataset.copy()\n\ntrain_labels = train_features.pop('MPG')\ntest_labels = test_features.pop('MPG')","metadata":{"cell_id":"f9bad56d95134acf92c05c2371f63ecb","source_hash":"948727ef","execution_start":1667313862636,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"##### Build the model\n\nLet's build a simple linear regression model (seen in a previous notebook) to test different callbacks during its training.\n\nWe use a `get_model()` function so that we can re-create and re-compile the model from scratch multiple times easily:","metadata":{"cell_id":"3f08e79182464d44a26193a5bdaa8a08","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def get_model(train_features):\n    normalizer = preprocessing.Normalization(input_shape=(train_features.shape[1],))\n    normalizer.adapt(np.array(train_features))\n    \n    model = keras.Sequential([\n        normalizer,\n        layers.Dense(units=1)\n    ])\n    \n    model.compile(\n        optimizer=tf.optimizers.Adam(learning_rate=0.1),\n        loss='mse', metrics=['mae', 'mse']\n    )\n    \n    return model","metadata":{"cell_id":"c7e37a95103d4152ab2ecdf1c2cb03ee","source_hash":"9f013c58","execution_start":1667313875438,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Early Stopping callback\n\nUse an *early stopping* callback to stop training when it reaches stability.\n\nThe `monitor` parameter specifies the loss/metric to be monitored, and the `patience` parameters specifies the number of non-improving epochs to wait before stopping:","metadata":{"cell_id":"27f7887b0e0547b69f93dde33cf7f77e","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n\n# re-create the model to restart training every time\nmodel = get_model(train_features)\nhistory = model.fit(train_features, train_labels, epochs=200, validation_split = 0.2, callbacks=[es_callback])","metadata":{"cell_id":"12b2e523b7ef400fbe4a742809e13ecc","source_hash":"20de5b3c","execution_start":1667313889670,"execution_millis":4924,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-11-01 14:44:49.678214: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2022-11-01 14:44:49.678271: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2022-11-01 14:44:49.678294: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-7817b2ad-42a3-441f-8072-b020be286d3c): /proc/driver/nvidia/version does not exist\n2022-11-01 14:44:49.678780: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nEpoch 1/200\n8/8 [==============================] - 1s 30ms/step - loss: 557.5607 - mae: 22.8683 - mse: 557.5607 - val_loss: 545.2025 - val_mae: 22.9178 - val_mse: 545.2025\nEpoch 2/200\n8/8 [==============================] - 0s 4ms/step - loss: 506.8895 - mae: 22.0897 - mse: 506.8895 - val_loss: 504.9878 - val_mae: 22.1403 - val_mse: 504.9878\nEpoch 3/200\n8/8 [==============================] - 0s 9ms/step - loss: 469.0557 - mae: 21.2658 - mse: 469.0557 - val_loss: 471.7043 - val_mae: 21.3883 - val_mse: 471.7043\nEpoch 4/200\n8/8 [==============================] - 0s 5ms/step - loss: 438.0182 - mae: 20.5163 - mse: 438.0182 - val_loss: 440.4852 - val_mae: 20.6434 - val_mse: 440.4852\nEpoch 5/200\n8/8 [==============================] - 0s 5ms/step - loss: 404.2270 - mae: 19.7002 - mse: 404.2270 - val_loss: 410.9105 - val_mae: 19.9368 - val_mse: 410.9105\nEpoch 6/200\n8/8 [==============================] - 0s 4ms/step - loss: 373.7850 - mae: 18.9328 - mse: 373.7850 - val_loss: 382.8686 - val_mae: 19.2259 - val_mse: 382.8686\nEpoch 7/200\n8/8 [==============================] - 0s 4ms/step - loss: 347.3799 - mae: 18.2227 - mse: 347.3799 - val_loss: 358.5334 - val_mae: 18.5596 - val_mse: 358.5334\nEpoch 8/200\n8/8 [==============================] - 0s 5ms/step - loss: 320.0952 - mae: 17.4666 - mse: 320.0952 - val_loss: 330.7584 - val_mae: 17.8210 - val_mse: 330.7584\nEpoch 9/200\n8/8 [==============================] - 0s 6ms/step - loss: 294.9025 - mae: 16.7368 - mse: 294.9025 - val_loss: 305.9346 - val_mae: 17.1149 - val_mse: 305.9346\nEpoch 10/200\n8/8 [==============================] - 0s 7ms/step - loss: 271.3235 - mae: 16.0314 - mse: 271.3235 - val_loss: 283.1779 - val_mae: 16.4382 - val_mse: 283.1779\nEpoch 11/200\n8/8 [==============================] - 0s 4ms/step - loss: 250.2112 - mae: 15.3622 - mse: 250.2112 - val_loss: 261.4792 - val_mae: 15.7646 - val_mse: 261.4792\nEpoch 12/200\n8/8 [==============================] - 0s 7ms/step - loss: 229.8576 - mae: 14.6991 - mse: 229.8576 - val_loss: 240.9676 - val_mae: 15.1105 - val_mse: 240.9676\nEpoch 13/200\n8/8 [==============================] - 0s 5ms/step - loss: 211.1867 - mae: 14.0485 - mse: 211.1867 - val_loss: 222.1121 - val_mae: 14.4737 - val_mse: 222.1121\nEpoch 14/200\n8/8 [==============================] - 0s 6ms/step - loss: 193.8340 - mae: 13.4273 - mse: 193.8340 - val_loss: 205.0537 - val_mae: 13.8652 - val_mse: 205.0537\nEpoch 15/200\n8/8 [==============================] - 0s 5ms/step - loss: 177.5123 - mae: 12.8037 - mse: 177.5123 - val_loss: 188.5956 - val_mae: 13.2624 - val_mse: 188.5956\nEpoch 16/200\n8/8 [==============================] - 0s 7ms/step - loss: 162.3633 - mae: 12.2063 - mse: 162.3633 - val_loss: 173.3563 - val_mae: 12.6755 - val_mse: 173.3563\nEpoch 17/200\n8/8 [==============================] - 0s 4ms/step - loss: 148.5141 - mae: 11.6265 - mse: 148.5141 - val_loss: 159.1259 - val_mae: 12.1040 - val_mse: 159.1259\nEpoch 18/200\n8/8 [==============================] - 0s 5ms/step - loss: 135.7975 - mae: 11.0775 - mse: 135.7975 - val_loss: 146.0563 - val_mae: 11.5527 - val_mse: 146.0563\nEpoch 19/200\n8/8 [==============================] - 0s 9ms/step - loss: 123.9636 - mae: 10.5366 - mse: 123.9636 - val_loss: 133.3276 - val_mae: 11.0031 - val_mse: 133.3276\nEpoch 20/200\n8/8 [==============================] - 0s 6ms/step - loss: 113.3593 - mae: 10.0222 - mse: 113.3593 - val_loss: 121.4333 - val_mae: 10.4624 - val_mse: 121.4333\nEpoch 21/200\n8/8 [==============================] - 0s 5ms/step - loss: 103.0714 - mae: 9.5130 - mse: 103.0714 - val_loss: 111.8582 - val_mae: 9.9826 - val_mse: 111.8582\nEpoch 22/200\n8/8 [==============================] - 0s 8ms/step - loss: 93.8637 - mae: 9.0224 - mse: 93.8637 - val_loss: 102.0764 - val_mae: 9.4862 - val_mse: 102.0764\nEpoch 23/200\n8/8 [==============================] - 0s 4ms/step - loss: 85.4863 - mae: 8.5502 - mse: 85.4863 - val_loss: 93.3717 - val_mae: 9.0161 - val_mse: 93.3717\nEpoch 24/200\n8/8 [==============================] - 0s 8ms/step - loss: 77.8796 - mae: 8.1079 - mse: 77.8796 - val_loss: 85.1676 - val_mae: 8.5615 - val_mse: 85.1676\nEpoch 25/200\n8/8 [==============================] - 0s 4ms/step - loss: 71.0808 - mae: 7.6911 - mse: 71.0808 - val_loss: 77.7346 - val_mae: 8.1271 - val_mse: 77.7346\nEpoch 26/200\n8/8 [==============================] - 0s 8ms/step - loss: 64.6335 - mae: 7.2732 - mse: 64.6335 - val_loss: 70.9591 - val_mae: 7.7079 - val_mse: 70.9591\nEpoch 27/200\n8/8 [==============================] - 0s 5ms/step - loss: 58.7119 - mae: 6.8756 - mse: 58.7119 - val_loss: 64.7683 - val_mae: 7.3167 - val_mse: 64.7683\nEpoch 28/200\n8/8 [==============================] - 0s 7ms/step - loss: 53.6374 - mae: 6.4994 - mse: 53.6374 - val_loss: 59.3537 - val_mae: 6.9434 - val_mse: 59.3537\nEpoch 29/200\n8/8 [==============================] - 0s 4ms/step - loss: 48.9518 - mae: 6.1442 - mse: 48.9518 - val_loss: 54.5836 - val_mae: 6.5945 - val_mse: 54.5836\nEpoch 30/200\n8/8 [==============================] - 0s 4ms/step - loss: 44.9038 - mae: 5.8318 - mse: 44.9038 - val_loss: 49.0071 - val_mae: 6.2072 - val_mse: 49.0071\nEpoch 31/200\n8/8 [==============================] - 0s 8ms/step - loss: 40.8076 - mae: 5.5027 - mse: 40.8076 - val_loss: 45.0031 - val_mae: 5.8848 - val_mse: 45.0031\nEpoch 32/200\n8/8 [==============================] - 0s 4ms/step - loss: 37.5643 - mae: 5.2110 - mse: 37.5643 - val_loss: 41.5087 - val_mae: 5.5778 - val_mse: 41.5087\nEpoch 33/200\n8/8 [==============================] - 0s 8ms/step - loss: 34.4726 - mae: 4.9339 - mse: 34.4726 - val_loss: 38.1744 - val_mae: 5.2905 - val_mse: 38.1744\nEpoch 34/200\n8/8 [==============================] - 0s 5ms/step - loss: 31.5364 - mae: 4.6698 - mse: 31.5364 - val_loss: 35.0679 - val_mae: 5.0203 - val_mse: 35.0679\nEpoch 35/200\n8/8 [==============================] - 0s 9ms/step - loss: 29.1023 - mae: 4.4496 - mse: 29.1023 - val_loss: 32.3791 - val_mae: 4.7596 - val_mse: 32.3791\nEpoch 36/200\n8/8 [==============================] - 0s 4ms/step - loss: 26.9217 - mae: 4.2313 - mse: 26.9217 - val_loss: 30.0429 - val_mae: 4.5232 - val_mse: 30.0429\nEpoch 37/200\n8/8 [==============================] - 0s 8ms/step - loss: 24.9401 - mae: 4.0312 - mse: 24.9401 - val_loss: 27.6799 - val_mae: 4.2925 - val_mse: 27.6799\nEpoch 38/200\n8/8 [==============================] - 0s 4ms/step - loss: 23.3176 - mae: 3.8683 - mse: 23.3176 - val_loss: 25.5517 - val_mae: 4.0726 - val_mse: 25.5517\nEpoch 39/200\n8/8 [==============================] - 0s 6ms/step - loss: 21.8318 - mae: 3.7000 - mse: 21.8318 - val_loss: 24.1087 - val_mae: 3.8936 - val_mse: 24.1087\nEpoch 40/200\n8/8 [==============================] - 0s 6ms/step - loss: 20.4190 - mae: 3.5496 - mse: 20.4190 - val_loss: 22.4823 - val_mae: 3.7156 - val_mse: 22.4823\nEpoch 41/200\n8/8 [==============================] - 0s 7ms/step - loss: 19.3234 - mae: 3.4372 - mse: 19.3234 - val_loss: 21.0005 - val_mae: 3.5645 - val_mse: 21.0005\nEpoch 42/200\n8/8 [==============================] - 0s 4ms/step - loss: 18.2531 - mae: 3.3164 - mse: 18.2531 - val_loss: 19.8517 - val_mae: 3.4266 - val_mse: 19.8517\nEpoch 43/200\n8/8 [==============================] - 0s 5ms/step - loss: 17.3546 - mae: 3.2114 - mse: 17.3546 - val_loss: 18.7185 - val_mae: 3.3095 - val_mse: 18.7185\nEpoch 44/200\n8/8 [==============================] - 0s 9ms/step - loss: 16.5564 - mae: 3.1226 - mse: 16.5564 - val_loss: 17.7128 - val_mae: 3.2032 - val_mse: 17.7128\nEpoch 45/200\n8/8 [==============================] - 0s 4ms/step - loss: 15.9598 - mae: 3.0510 - mse: 15.9598 - val_loss: 16.8016 - val_mae: 3.0941 - val_mse: 16.8016\nEpoch 46/200\n8/8 [==============================] - 0s 11ms/step - loss: 15.4437 - mae: 2.9701 - mse: 15.4437 - val_loss: 16.2465 - val_mae: 3.0178 - val_mse: 16.2465\nEpoch 47/200\n8/8 [==============================] - 0s 5ms/step - loss: 14.7324 - mae: 2.8813 - mse: 14.7324 - val_loss: 15.4795 - val_mae: 2.9329 - val_mse: 15.4795\nEpoch 48/200\n8/8 [==============================] - 0s 5ms/step - loss: 14.2798 - mae: 2.8394 - mse: 14.2798 - val_loss: 14.6929 - val_mae: 2.8592 - val_mse: 14.6929\nEpoch 49/200\n8/8 [==============================] - 0s 4ms/step - loss: 13.9633 - mae: 2.8164 - mse: 13.9633 - val_loss: 14.1855 - val_mae: 2.8136 - val_mse: 14.1855\nEpoch 50/200\n8/8 [==============================] - 0s 8ms/step - loss: 13.5916 - mae: 2.7696 - mse: 13.5916 - val_loss: 13.9535 - val_mae: 2.7933 - val_mse: 13.9535\nEpoch 51/200\n8/8 [==============================] - 0s 4ms/step - loss: 13.3551 - mae: 2.7264 - mse: 13.3551 - val_loss: 13.4551 - val_mae: 2.7473 - val_mse: 13.4551\nEpoch 52/200\n8/8 [==============================] - 0s 9ms/step - loss: 13.0332 - mae: 2.6944 - mse: 13.0332 - val_loss: 13.0409 - val_mae: 2.7109 - val_mse: 13.0409\nEpoch 53/200\n8/8 [==============================] - 0s 4ms/step - loss: 12.8207 - mae: 2.6784 - mse: 12.8207 - val_loss: 12.6592 - val_mae: 2.6868 - val_mse: 12.6592\nEpoch 54/200\n8/8 [==============================] - 0s 4ms/step - loss: 12.6431 - mae: 2.6533 - mse: 12.6431 - val_loss: 12.4506 - val_mae: 2.6674 - val_mse: 12.4506\nEpoch 55/200\n8/8 [==============================] - 0s 4ms/step - loss: 12.4767 - mae: 2.6336 - mse: 12.4767 - val_loss: 12.2147 - val_mae: 2.6415 - val_mse: 12.2147\nEpoch 56/200\n8/8 [==============================] - 0s 5ms/step - loss: 12.3552 - mae: 2.6209 - mse: 12.3552 - val_loss: 11.9674 - val_mae: 2.6265 - val_mse: 11.9674\nEpoch 57/200\n8/8 [==============================] - 0s 9ms/step - loss: 12.2051 - mae: 2.6041 - mse: 12.2051 - val_loss: 11.8623 - val_mae: 2.6094 - val_mse: 11.8623\nEpoch 58/200\n8/8 [==============================] - 0s 9ms/step - loss: 12.1085 - mae: 2.5961 - mse: 12.1085 - val_loss: 11.6411 - val_mae: 2.5903 - val_mse: 11.6411\nEpoch 59/200\n8/8 [==============================] - 0s 4ms/step - loss: 12.0516 - mae: 2.6039 - mse: 12.0516 - val_loss: 11.4235 - val_mae: 2.5709 - val_mse: 11.4235\nEpoch 60/200\n8/8 [==============================] - 0s 4ms/step - loss: 11.9719 - mae: 2.5874 - mse: 11.9719 - val_loss: 11.3143 - val_mae: 2.5581 - val_mse: 11.3143\nEpoch 61/200\n8/8 [==============================] - 0s 9ms/step - loss: 12.0003 - mae: 2.5779 - mse: 12.0003 - val_loss: 11.3719 - val_mae: 2.5688 - val_mse: 11.3719\nEpoch 62/200\n8/8 [==============================] - 0s 4ms/step - loss: 11.8485 - mae: 2.5767 - mse: 11.8485 - val_loss: 11.0959 - val_mae: 2.5323 - val_mse: 11.0959\nEpoch 63/200\n8/8 [==============================] - 0s 5ms/step - loss: 11.7868 - mae: 2.5763 - mse: 11.7868 - val_loss: 10.9784 - val_mae: 2.5195 - val_mse: 10.9784\nEpoch 64/200\n8/8 [==============================] - 0s 8ms/step - loss: 11.7894 - mae: 2.5779 - mse: 11.7894 - val_loss: 10.9937 - val_mae: 2.5235 - val_mse: 10.9937\nEpoch 65/200\n8/8 [==============================] - 0s 4ms/step - loss: 11.7290 - mae: 2.5744 - mse: 11.7290 - val_loss: 10.9133 - val_mae: 2.5263 - val_mse: 10.9133\nEpoch 66/200\n8/8 [==============================] - 0s 5ms/step - loss: 11.7067 - mae: 2.5546 - mse: 11.7067 - val_loss: 10.9119 - val_mae: 2.5273 - val_mse: 10.9119\nEpoch 67/200\n8/8 [==============================] - 0s 4ms/step - loss: 11.6468 - mae: 2.5540 - mse: 11.6468 - val_loss: 10.7689 - val_mae: 2.5067 - val_mse: 10.7689\nEpoch 68/200\n8/8 [==============================] - 0s 8ms/step - loss: 11.6252 - mae: 2.5651 - mse: 11.6252 - val_loss: 10.6496 - val_mae: 2.4917 - val_mse: 10.6496\nEpoch 69/200\n8/8 [==============================] - 0s 6ms/step - loss: 11.6244 - mae: 2.5735 - mse: 11.6244 - val_loss: 10.6417 - val_mae: 2.4858 - val_mse: 10.6417\nEpoch 70/200\n8/8 [==============================] - 0s 9ms/step - loss: 11.5984 - mae: 2.5556 - mse: 11.5984 - val_loss: 10.6366 - val_mae: 2.4915 - val_mse: 10.6366\nEpoch 71/200\n8/8 [==============================] - 0s 5ms/step - loss: 11.5629 - mae: 2.5519 - mse: 11.5629 - val_loss: 10.6081 - val_mae: 2.4894 - val_mse: 10.6081\nEpoch 72/200\n8/8 [==============================] - 0s 4ms/step - loss: 11.5653 - mae: 2.5506 - mse: 11.5653 - val_loss: 10.5376 - val_mae: 2.4839 - val_mse: 10.5376\nEpoch 73/200\n8/8 [==============================] - 0s 5ms/step - loss: 11.6156 - mae: 2.5772 - mse: 11.6156 - val_loss: 10.4785 - val_mae: 2.4721 - val_mse: 10.4785\nEpoch 74/200\n8/8 [==============================] - 0s 5ms/step - loss: 11.5279 - mae: 2.5635 - mse: 11.5279 - val_loss: 10.4940 - val_mae: 2.4744 - val_mse: 10.4940\nEpoch 75/200\n8/8 [==============================] - 0s 5ms/step - loss: 11.5402 - mae: 2.5478 - mse: 11.5402 - val_loss: 10.5031 - val_mae: 2.4790 - val_mse: 10.5031\nEpoch 75: early stopping\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"As you can see, the training stopped after about 60/70 epochs, rather than running for the entire 200 epochs specified in `fit()`.","metadata":{"cell_id":"601075142f184a6aaf25d2bee175fc2a","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Checkpoint Callback\n\nLet's add a second callback to save a model checkpoint after every epoch. Notice that we can pass multiple callbacks at the same time to `fit()`.","metadata":{"cell_id":"6c5ed2125a7944aea420d4d8d08ceb22","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"cp_callback = keras.callbacks.ModelCheckpoint(\n     './callback_test_chkp/chkp_{epoch:02d}',\n    # './callback_test_chkp/chkp_best',\n    monitor='val_loss',\n    verbose=0, \n    save_best_only=False,\n    # save_best_only=True,\n    save_weights_only=False,\n    mode='auto',\n    save_freq='epoch'\n)","metadata":{"cell_id":"57e3a2bc92604ba3b193cfc12d402c6c","source_hash":"f90e602a","execution_start":1667313967944,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model = get_model(train_features)\nhistory = model.fit(train_features, train_labels, epochs=200, validation_split = 0.2,\n                                callbacks=[es_callback, cp_callback])","metadata":{"cell_id":"8527596e4d0a438eb799f1dd0e71e594","source_hash":"c93afdc0","execution_start":1667313985109,"execution_millis":29675,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch 1/200\n1/8 [==>...........................] - ETA: 1s - loss: 514.4294 - mae: 20.4803 - mse: 514.4294INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_01/assets\n8/8 [==============================] - 1s 92ms/step - loss: 577.9723 - mae: 22.6814 - mse: 577.9722 - val_loss: 556.0629 - val_mae: 22.8364 - val_mse: 556.0629\nEpoch 2/200\n1/8 [==>...........................] - ETA: 0s - loss: 479.4446 - mae: 21.1589 - mse: 479.4446INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_02/assets\n8/8 [==============================] - 0s 50ms/step - loss: 515.9682 - mae: 22.1406 - mse: 515.9682 - val_loss: 497.3733 - val_mae: 21.9585 - val_mse: 497.3733\nEpoch 3/200\n1/8 [==>...........................] - ETA: 0s - loss: 484.2189 - mae: 21.7836 - mse: 484.2189INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_03/assets\n8/8 [==============================] - 0s 57ms/step - loss: 469.5412 - mae: 21.2938 - mse: 469.5412 - val_loss: 462.6104 - val_mae: 21.2135 - val_mse: 462.6104\nEpoch 4/200\n1/8 [==>...........................] - ETA: 0s - loss: 467.8256 - mae: 21.1964 - mse: 467.8256INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_04/assets\n8/8 [==============================] - 0s 51ms/step - loss: 438.2903 - mae: 20.5582 - mse: 438.2903 - val_loss: 432.1886 - val_mae: 20.4687 - val_mse: 432.1886\nEpoch 5/200\n1/8 [==>...........................] - ETA: 0s - loss: 383.8685 - mae: 19.0767 - mse: 383.8685INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_05/assets\n8/8 [==============================] - 0s 50ms/step - loss: 404.5651 - mae: 19.7393 - mse: 404.5651 - val_loss: 402.6954 - val_mae: 19.7711 - val_mse: 402.6954\nEpoch 6/200\n1/8 [==>...........................] - ETA: 0s - loss: 381.0430 - mae: 19.3896 - mse: 381.0430INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_06/assets\n8/8 [==============================] - 0s 51ms/step - loss: 373.6405 - mae: 18.9623 - mse: 373.6405 - val_loss: 375.7574 - val_mae: 19.0803 - val_mse: 375.7574\nEpoch 7/200\n1/8 [==>...........................] - ETA: 0s - loss: 382.8220 - mae: 19.1730 - mse: 382.8220INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_07/assets\n8/8 [==============================] - 0s 55ms/step - loss: 345.2024 - mae: 18.1956 - mse: 345.2024 - val_loss: 350.9045 - val_mae: 18.4101 - val_mse: 350.9045\nEpoch 8/200\n1/8 [==>...........................] - ETA: 0s - loss: 341.7770 - mae: 18.0352 - mse: 341.7770INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_08/assets\n8/8 [==============================] - 0s 55ms/step - loss: 318.6603 - mae: 17.4550 - mse: 318.6603 - val_loss: 326.5237 - val_mae: 17.7251 - val_mse: 326.5237\nEpoch 9/200\n1/8 [==>...........................] - ETA: 0s - loss: 258.6723 - mae: 15.8626 - mse: 258.6723INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_09/assets\n8/8 [==============================] - 0s 51ms/step - loss: 294.0518 - mae: 16.7311 - mse: 294.0518 - val_loss: 303.3250 - val_mae: 17.0487 - val_mse: 303.3250\nEpoch 10/200\n1/8 [==>...........................] - ETA: 0s - loss: 295.1198 - mae: 16.7302 - mse: 295.1198INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_10/assets\n8/8 [==============================] - 1s 76ms/step - loss: 271.2976 - mae: 16.0348 - mse: 271.2976 - val_loss: 281.5832 - val_mae: 16.3893 - val_mse: 281.5832\nEpoch 11/200\n1/8 [==>...........................] - ETA: 0s - loss: 253.6204 - mae: 15.4122 - mse: 253.6204INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_11/assets\n8/8 [==============================] - 0s 51ms/step - loss: 249.8549 - mae: 15.3584 - mse: 249.8549 - val_loss: 259.2842 - val_mae: 15.7133 - val_mse: 259.2842\nEpoch 12/200\n1/8 [==>...........................] - ETA: 0s - loss: 197.3576 - mae: 13.4973 - mse: 197.3576INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_12/assets\n8/8 [==============================] - 0s 51ms/step - loss: 229.6076 - mae: 14.6990 - mse: 229.6076 - val_loss: 239.8644 - val_mae: 15.0792 - val_mse: 239.8644\nEpoch 13/200\n1/8 [==>...........................] - ETA: 0s - loss: 193.9720 - mae: 13.6528 - mse: 193.9720INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_13/assets\n8/8 [==============================] - 0s 52ms/step - loss: 210.9796 - mae: 14.0508 - mse: 210.9796 - val_loss: 220.4642 - val_mae: 14.4298 - val_mse: 220.4642\nEpoch 14/200\n1/8 [==>...........................] - ETA: 0s - loss: 186.9344 - mae: 13.2382 - mse: 186.9344INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_14/assets\n8/8 [==============================] - 0s 50ms/step - loss: 193.5283 - mae: 13.4203 - mse: 193.5283 - val_loss: 202.6422 - val_mae: 13.8036 - val_mse: 202.6422\nEpoch 15/200\n1/8 [==>...........................] - ETA: 0s - loss: 175.5235 - mae: 12.7045 - mse: 175.5235INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_15/assets\n8/8 [==============================] - 0s 48ms/step - loss: 177.3090 - mae: 12.8032 - mse: 177.3090 - val_loss: 187.2788 - val_mae: 13.2225 - val_mse: 187.2788\nEpoch 16/200\n1/8 [==>...........................] - ETA: 0s - loss: 188.4362 - mae: 13.2992 - mse: 188.4362INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_16/assets\n8/8 [==============================] - 0s 52ms/step - loss: 162.4514 - mae: 12.2052 - mse: 162.4514 - val_loss: 172.2604 - val_mae: 12.6404 - val_mse: 172.2604\nEpoch 17/200\n1/8 [==>...........................] - ETA: 0s - loss: 181.1599 - mae: 12.9563 - mse: 181.1599INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_17/assets\n8/8 [==============================] - 0s 53ms/step - loss: 148.9557 - mae: 11.6452 - mse: 148.9557 - val_loss: 157.3050 - val_mae: 12.0498 - val_mse: 157.3050\nEpoch 18/200\n1/8 [==>...........................] - ETA: 0s - loss: 134.9988 - mae: 11.2356 - mse: 134.9988INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_18/assets\n8/8 [==============================] - 0s 50ms/step - loss: 135.7528 - mae: 11.0744 - mse: 135.7528 - val_loss: 144.4838 - val_mae: 11.5027 - val_mse: 144.4838\nEpoch 19/200\n1/8 [==>...........................] - ETA: 0s - loss: 100.0725 - mae: 9.3332 - mse: 100.0725INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_19/assets\n8/8 [==============================] - 0s 58ms/step - loss: 123.7481 - mae: 10.5333 - mse: 123.7481 - val_loss: 132.7331 - val_mae: 10.9737 - val_mse: 132.7331\nEpoch 20/200\n1/8 [==>...........................] - ETA: 0s - loss: 85.7866 - mae: 8.7590 - mse: 85.7866INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_20/assets\n8/8 [==============================] - 0s 52ms/step - loss: 113.0814 - mae: 10.0104 - mse: 113.0814 - val_loss: 121.5607 - val_mae: 10.4563 - val_mse: 121.5607\nEpoch 21/200\n1/8 [==>...........................] - ETA: 0s - loss: 104.1425 - mae: 9.5213 - mse: 104.1425INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_21/assets\n8/8 [==============================] - 0s 63ms/step - loss: 103.1619 - mae: 9.5175 - mse: 103.1619 - val_loss: 110.2040 - val_mae: 9.9266 - val_mse: 110.2040\nEpoch 22/200\n1/8 [==>...........................] - ETA: 0s - loss: 98.9602 - mae: 9.4622 - mse: 98.9602INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_22/assets\n8/8 [==============================] - 0s 53ms/step - loss: 93.8742 - mae: 9.0314 - mse: 93.8742 - val_loss: 100.9600 - val_mae: 9.4468 - val_mse: 100.9600\nEpoch 23/200\n1/8 [==>...........................] - ETA: 0s - loss: 100.0080 - mae: 9.1033 - mse: 100.0080INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_23/assets\n8/8 [==============================] - 0s 52ms/step - loss: 85.4715 - mae: 8.5594 - mse: 85.4715 - val_loss: 91.9379 - val_mae: 8.9680 - val_mse: 91.9379\nEpoch 24/200\n1/8 [==>...........................] - ETA: 0s - loss: 61.4887 - mae: 7.4178 - mse: 61.4887INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_24/assets\n8/8 [==============================] - 0s 63ms/step - loss: 77.7715 - mae: 8.1135 - mse: 77.7715 - val_loss: 83.9890 - val_mae: 8.5181 - val_mse: 83.9890\nEpoch 25/200\n1/8 [==>...........................] - ETA: 0s - loss: 73.2980 - mae: 7.9474 - mse: 73.2980INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_25/assets\n8/8 [==============================] - 0s 51ms/step - loss: 70.9021 - mae: 7.6905 - mse: 70.9021 - val_loss: 76.3869 - val_mae: 8.0706 - val_mse: 76.3869\nEpoch 26/200\n1/8 [==>...........................] - ETA: 0s - loss: 94.7467 - mae: 9.1435 - mse: 94.7467INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_26/assets\n8/8 [==============================] - 0s 67ms/step - loss: 64.5577 - mae: 7.2727 - mse: 64.5577 - val_loss: 70.2455 - val_mae: 7.6703 - val_mse: 70.2455\nEpoch 27/200\n1/8 [==>...........................] - ETA: 0s - loss: 61.8515 - mae: 6.8538 - mse: 61.8515INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_27/assets\n8/8 [==============================] - 1s 79ms/step - loss: 58.6432 - mae: 6.8675 - mse: 58.6432 - val_loss: 64.1195 - val_mae: 7.2808 - val_mse: 64.1195\nEpoch 28/200\n1/8 [==>...........................] - ETA: 0s - loss: 47.0958 - mae: 6.1038 - mse: 47.0958INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_28/assets\n8/8 [==============================] - 0s 52ms/step - loss: 53.5104 - mae: 6.4983 - mse: 53.5104 - val_loss: 58.3153 - val_mae: 6.8948 - val_mse: 58.3153\nEpoch 29/200\n1/8 [==>...........................] - ETA: 0s - loss: 40.6485 - mae: 5.8249 - mse: 40.6485INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_29/assets\n8/8 [==============================] - 0s 56ms/step - loss: 48.7548 - mae: 6.1463 - mse: 48.7548 - val_loss: 53.2792 - val_mae: 6.5343 - val_mse: 53.2792\nEpoch 30/200\n1/8 [==>...........................] - ETA: 0s - loss: 46.2615 - mae: 5.9112 - mse: 46.2615INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_30/assets\n8/8 [==============================] - 0s 48ms/step - loss: 44.5617 - mae: 5.8168 - mse: 44.5617 - val_loss: 48.5739 - val_mae: 6.1831 - val_mse: 48.5739\nEpoch 31/200\n1/8 [==>...........................] - ETA: 0s - loss: 34.8167 - mae: 5.1098 - mse: 34.8167INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_31/assets\n8/8 [==============================] - 0s 55ms/step - loss: 40.6698 - mae: 5.4932 - mse: 40.6698 - val_loss: 44.6739 - val_mae: 5.8642 - val_mse: 44.6739\nEpoch 32/200\n1/8 [==>...........................] - ETA: 0s - loss: 34.5298 - mae: 4.9406 - mse: 34.5298INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_32/assets\n8/8 [==============================] - 0s 54ms/step - loss: 37.2598 - mae: 5.1891 - mse: 37.2598 - val_loss: 41.2508 - val_mae: 5.5622 - val_mse: 41.2508\nEpoch 33/200\n1/8 [==>...........................] - ETA: 0s - loss: 29.4652 - mae: 4.9146 - mse: 29.4652INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_33/assets\n8/8 [==============================] - 0s 52ms/step - loss: 34.2333 - mae: 4.9161 - mse: 34.2333 - val_loss: 37.6583 - val_mae: 5.2649 - val_mse: 37.6583\nEpoch 34/200\n1/8 [==>...........................] - ETA: 0s - loss: 25.3717 - mae: 4.4648 - mse: 25.3717INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_34/assets\n8/8 [==============================] - 0s 52ms/step - loss: 31.5342 - mae: 4.6735 - mse: 31.5342 - val_loss: 34.4486 - val_mae: 4.9800 - val_mse: 34.4486\nEpoch 35/200\n1/8 [==>...........................] - ETA: 0s - loss: 21.8877 - mae: 3.9255 - mse: 21.8877INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_35/assets\n8/8 [==============================] - 0s 55ms/step - loss: 29.0054 - mae: 4.4488 - mse: 29.0054 - val_loss: 31.8161 - val_mae: 4.7239 - val_mse: 31.8161\nEpoch 36/200\n1/8 [==>...........................] - ETA: 0s - loss: 20.0404 - mae: 3.7477 - mse: 20.0404INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_36/assets\n8/8 [==============================] - 0s 51ms/step - loss: 26.9607 - mae: 4.2293 - mse: 26.9607 - val_loss: 29.6368 - val_mae: 4.5002 - val_mse: 29.6368\nEpoch 37/200\n1/8 [==>...........................] - ETA: 0s - loss: 23.9612 - mae: 3.9623 - mse: 23.9612INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_37/assets\n8/8 [==============================] - 0s 55ms/step - loss: 24.9078 - mae: 4.0252 - mse: 24.9078 - val_loss: 27.3439 - val_mae: 4.2701 - val_mse: 27.3439\nEpoch 38/200\n1/8 [==>...........................] - ETA: 0s - loss: 22.1692 - mae: 3.8326 - mse: 22.1692INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_38/assets\n8/8 [==============================] - 0s 47ms/step - loss: 23.1459 - mae: 3.8536 - mse: 23.1459 - val_loss: 25.4164 - val_mae: 4.0646 - val_mse: 25.4164\nEpoch 39/200\n1/8 [==>...........................] - ETA: 0s - loss: 20.5168 - mae: 3.6002 - mse: 20.5168INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_39/assets\n8/8 [==============================] - 0s 58ms/step - loss: 21.6913 - mae: 3.7077 - mse: 21.6913 - val_loss: 23.5182 - val_mae: 3.8596 - val_mse: 23.5182\nEpoch 40/200\n1/8 [==>...........................] - ETA: 0s - loss: 18.8814 - mae: 3.7280 - mse: 18.8814INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_40/assets\n8/8 [==============================] - 0s 59ms/step - loss: 20.3674 - mae: 3.5640 - mse: 20.3674 - val_loss: 22.1080 - val_mae: 3.6936 - val_mse: 22.1080\nEpoch 41/200\n1/8 [==>...........................] - ETA: 0s - loss: 22.8505 - mae: 3.8067 - mse: 22.8505INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_41/assets\n8/8 [==============================] - 0s 52ms/step - loss: 19.1846 - mae: 3.4339 - mse: 19.1846 - val_loss: 20.7014 - val_mae: 3.5398 - val_mse: 20.7014\nEpoch 42/200\n1/8 [==>...........................] - ETA: 0s - loss: 18.3188 - mae: 3.6200 - mse: 18.3188INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_42/assets\n8/8 [==============================] - 0s 51ms/step - loss: 18.1586 - mae: 3.3110 - mse: 18.1586 - val_loss: 19.4706 - val_mae: 3.3972 - val_mse: 19.4706\nEpoch 43/200\n1/8 [==>...........................] - ETA: 0s - loss: 19.7583 - mae: 3.2360 - mse: 19.7583INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_43/assets\n8/8 [==============================] - 0s 51ms/step - loss: 17.2315 - mae: 3.1989 - mse: 17.2315 - val_loss: 18.3476 - val_mae: 3.2743 - val_mse: 18.3476\nEpoch 44/200\n1/8 [==>...........................] - ETA: 0s - loss: 17.6027 - mae: 3.2470 - mse: 17.6027INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_44/assets\n8/8 [==============================] - 0s 54ms/step - loss: 16.5310 - mae: 3.1218 - mse: 16.5310 - val_loss: 17.2950 - val_mae: 3.1615 - val_mse: 17.2950\nEpoch 45/200\n1/8 [==>...........................] - ETA: 0s - loss: 13.2286 - mae: 3.0173 - mse: 13.2286INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_45/assets\n8/8 [==============================] - 1s 79ms/step - loss: 15.7889 - mae: 3.0348 - mse: 15.7889 - val_loss: 16.5119 - val_mae: 3.0575 - val_mse: 16.5119\nEpoch 46/200\n1/8 [==>...........................] - ETA: 0s - loss: 12.3780 - mae: 2.6091 - mse: 12.3780INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_46/assets\n8/8 [==============================] - 0s 50ms/step - loss: 15.1474 - mae: 2.9532 - mse: 15.1474 - val_loss: 15.7930 - val_mae: 2.9736 - val_mse: 15.7930\nEpoch 47/200\n1/8 [==>...........................] - ETA: 0s - loss: 22.8602 - mae: 3.3023 - mse: 22.8602INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_47/assets\n8/8 [==============================] - 0s 53ms/step - loss: 14.6301 - mae: 2.8885 - mse: 14.6301 - val_loss: 15.1721 - val_mae: 2.9050 - val_mse: 15.1721\nEpoch 48/200\n1/8 [==>...........................] - ETA: 0s - loss: 12.2025 - mae: 2.5928 - mse: 12.2025INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_48/assets\n8/8 [==============================] - 0s 56ms/step - loss: 14.1936 - mae: 2.8385 - mse: 14.1936 - val_loss: 14.6364 - val_mae: 2.8493 - val_mse: 14.6364\nEpoch 49/200\n1/8 [==>...........................] - ETA: 0s - loss: 13.4917 - mae: 2.5969 - mse: 13.4917INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_49/assets\n8/8 [==============================] - 0s 57ms/step - loss: 13.8173 - mae: 2.8012 - mse: 13.8173 - val_loss: 14.0122 - val_mae: 2.7946 - val_mse: 14.0122\nEpoch 50/200\n1/8 [==>...........................] - ETA: 0s - loss: 11.2642 - mae: 2.6491 - mse: 11.2642INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_50/assets\n8/8 [==============================] - 0s 53ms/step - loss: 13.5951 - mae: 2.7571 - mse: 13.5951 - val_loss: 13.8365 - val_mae: 2.7777 - val_mse: 13.8365\nEpoch 51/200\n1/8 [==>...........................] - ETA: 0s - loss: 16.3381 - mae: 3.2540 - mse: 16.3381INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_51/assets\n8/8 [==============================] - 0s 53ms/step - loss: 13.1680 - mae: 2.7194 - mse: 13.1680 - val_loss: 13.1876 - val_mae: 2.7325 - val_mse: 13.1876\nEpoch 52/200\n1/8 [==>...........................] - ETA: 0s - loss: 15.9656 - mae: 2.9498 - mse: 15.9656INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_52/assets\n8/8 [==============================] - 0s 54ms/step - loss: 12.9854 - mae: 2.7159 - mse: 12.9854 - val_loss: 12.8130 - val_mae: 2.7044 - val_mse: 12.8130\nEpoch 53/200\n1/8 [==>...........................] - ETA: 0s - loss: 14.0264 - mae: 2.6857 - mse: 14.0264INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_53/assets\n8/8 [==============================] - 0s 49ms/step - loss: 12.7113 - mae: 2.6845 - mse: 12.7113 - val_loss: 12.5328 - val_mae: 2.6806 - val_mse: 12.5328\nEpoch 54/200\n1/8 [==>...........................] - ETA: 0s - loss: 13.1000 - mae: 2.7754 - mse: 13.1000INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_54/assets\n8/8 [==============================] - 0s 50ms/step - loss: 12.5302 - mae: 2.6432 - mse: 12.5302 - val_loss: 12.3538 - val_mae: 2.6617 - val_mse: 12.3538\nEpoch 55/200\n1/8 [==>...........................] - ETA: 0s - loss: 9.5433 - mae: 2.2967 - mse: 9.5433INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_55/assets\n8/8 [==============================] - 0s 55ms/step - loss: 12.4273 - mae: 2.6365 - mse: 12.4273 - val_loss: 11.9506 - val_mae: 2.6266 - val_mse: 11.9506\nEpoch 56/200\n1/8 [==>...........................] - ETA: 0s - loss: 12.1169 - mae: 2.5143 - mse: 12.1169INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_56/assets\n8/8 [==============================] - 0s 50ms/step - loss: 12.2387 - mae: 2.6145 - mse: 12.2387 - val_loss: 11.8555 - val_mae: 2.6130 - val_mse: 11.8555\nEpoch 57/200\n1/8 [==>...........................] - ETA: 0s - loss: 12.8011 - mae: 2.6848 - mse: 12.8011INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_57/assets\n8/8 [==============================] - 0s 54ms/step - loss: 12.1196 - mae: 2.6113 - mse: 12.1196 - val_loss: 11.7226 - val_mae: 2.5926 - val_mse: 11.7226\nEpoch 58/200\n1/8 [==>...........................] - ETA: 0s - loss: 9.0783 - mae: 2.4669 - mse: 9.0783INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_58/assets\n8/8 [==============================] - 0s 52ms/step - loss: 12.0499 - mae: 2.6055 - mse: 12.0499 - val_loss: 11.4808 - val_mae: 2.5749 - val_mse: 11.4808\nEpoch 59/200\n1/8 [==>...........................] - ETA: 0s - loss: 9.8200 - mae: 2.2845 - mse: 9.8200INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_59/assets\n8/8 [==============================] - 0s 49ms/step - loss: 11.9208 - mae: 2.5900 - mse: 11.9208 - val_loss: 11.3550 - val_mae: 2.5629 - val_mse: 11.3550\nEpoch 60/200\n1/8 [==>...........................] - ETA: 0s - loss: 12.6439 - mae: 2.7012 - mse: 12.6439INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_60/assets\n8/8 [==============================] - 0s 49ms/step - loss: 12.0452 - mae: 2.5989 - mse: 12.0452 - val_loss: 11.3545 - val_mae: 2.5611 - val_mse: 11.3545\nEpoch 61/200\n1/8 [==>...........................] - ETA: 0s - loss: 9.9017 - mae: 2.5261 - mse: 9.9017INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_61/assets\n8/8 [==============================] - 0s 49ms/step - loss: 11.8486 - mae: 2.5763 - mse: 11.8486 - val_loss: 11.0566 - val_mae: 2.5250 - val_mse: 11.0566\nEpoch 62/200\n1/8 [==>...........................] - ETA: 0s - loss: 10.6333 - mae: 2.5944 - mse: 10.6333INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_62/assets\n8/8 [==============================] - 0s 53ms/step - loss: 11.8025 - mae: 2.5794 - mse: 11.8025 - val_loss: 11.0052 - val_mae: 2.5195 - val_mse: 11.0052\nEpoch 63/200\n1/8 [==>...........................] - ETA: 0s - loss: 16.2862 - mae: 3.1491 - mse: 16.2862INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_63/assets\n8/8 [==============================] - 0s 50ms/step - loss: 11.7373 - mae: 2.5673 - mse: 11.7373 - val_loss: 10.8995 - val_mae: 2.5206 - val_mse: 10.8995\nEpoch 64/200\n1/8 [==>...........................] - ETA: 0s - loss: 10.0461 - mae: 2.4182 - mse: 10.0461INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_64/assets\n8/8 [==============================] - 1s 80ms/step - loss: 11.7314 - mae: 2.5741 - mse: 11.7314 - val_loss: 10.7373 - val_mae: 2.4992 - val_mse: 10.7373\nEpoch 65/200\n1/8 [==>...........................] - ETA: 0s - loss: 10.0214 - mae: 2.4594 - mse: 10.0214INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_65/assets\n8/8 [==============================] - 0s 53ms/step - loss: 11.7174 - mae: 2.5730 - mse: 11.7174 - val_loss: 10.8478 - val_mae: 2.5190 - val_mse: 10.8478\nEpoch 66/200\n1/8 [==>...........................] - ETA: 0s - loss: 13.5659 - mae: 2.5696 - mse: 13.5659INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_66/assets\n8/8 [==============================] - 0s 52ms/step - loss: 11.5958 - mae: 2.5575 - mse: 11.5958 - val_loss: 10.7032 - val_mae: 2.4991 - val_mse: 10.7032\nEpoch 67/200\n1/8 [==>...........................] - ETA: 0s - loss: 15.8165 - mae: 2.7688 - mse: 15.8165INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_67/assets\n8/8 [==============================] - 0s 57ms/step - loss: 11.5930 - mae: 2.5704 - mse: 11.5930 - val_loss: 10.6013 - val_mae: 2.4892 - val_mse: 10.6013\nEpoch 68/200\n1/8 [==>...........................] - ETA: 0s - loss: 15.6410 - mae: 2.8214 - mse: 15.6410INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_68/assets\n8/8 [==============================] - 0s 56ms/step - loss: 11.5680 - mae: 2.5659 - mse: 11.5680 - val_loss: 10.5655 - val_mae: 2.4828 - val_mse: 10.5655\nEpoch 69/200\n1/8 [==>...........................] - ETA: 0s - loss: 18.4869 - mae: 3.0754 - mse: 18.4869INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_69/assets\n8/8 [==============================] - 0s 52ms/step - loss: 11.5486 - mae: 2.5636 - mse: 11.5486 - val_loss: 10.5253 - val_mae: 2.4848 - val_mse: 10.5253\nEpoch 70/200\n1/8 [==>...........................] - ETA: 0s - loss: 11.8388 - mae: 2.5247 - mse: 11.8388INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_70/assets\n8/8 [==============================] - 0s 56ms/step - loss: 11.5824 - mae: 2.5558 - mse: 11.5824 - val_loss: 10.6044 - val_mae: 2.4970 - val_mse: 10.6044\nEpoch 71/200\n1/8 [==>...........................] - ETA: 0s - loss: 11.1561 - mae: 2.6346 - mse: 11.1561INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_71/assets\n8/8 [==============================] - 0s 54ms/step - loss: 11.5491 - mae: 2.5593 - mse: 11.5491 - val_loss: 10.5076 - val_mae: 2.4794 - val_mse: 10.5076\nEpoch 72/200\n1/8 [==>...........................] - ETA: 0s - loss: 13.9467 - mae: 2.7907 - mse: 13.9467INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_72/assets\n8/8 [==============================] - 0s 54ms/step - loss: 11.5355 - mae: 2.5726 - mse: 11.5355 - val_loss: 10.4480 - val_mae: 2.4767 - val_mse: 10.4480\nEpoch 73/200\n1/8 [==>...........................] - ETA: 0s - loss: 15.4899 - mae: 2.8183 - mse: 15.4899INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_73/assets\n8/8 [==============================] - 0s 51ms/step - loss: 11.4914 - mae: 2.5726 - mse: 11.4914 - val_loss: 10.4036 - val_mae: 2.4721 - val_mse: 10.4036\nEpoch 74/200\n1/8 [==>...........................] - ETA: 0s - loss: 6.3373 - mae: 2.1595 - mse: 6.3373INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_74/assets\n8/8 [==============================] - 0s 53ms/step - loss: 11.5141 - mae: 2.5531 - mse: 11.5141 - val_loss: 10.4781 - val_mae: 2.4742 - val_mse: 10.4781\nEpoch 75/200\n1/8 [==>...........................] - ETA: 0s - loss: 10.8175 - mae: 2.4767 - mse: 10.8175INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_75/assets\n8/8 [==============================] - 0s 49ms/step - loss: 11.4926 - mae: 2.5482 - mse: 11.4926 - val_loss: 10.4905 - val_mae: 2.4701 - val_mse: 10.4905\nEpoch 75: early stopping\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"##### Restore a saved checkpoint\n\nLet's try loading back two different models, and let's evaluate them on training data.","metadata":{"cell_id":"78523c6f81554936bfa48a93ae1fb1f7","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# model_epoch1 = keras.models.load_model('./callback_test_chkp/chkp_best')\nmodel_epoch1 = keras.models.load_model('./callback_test_chkp/chkp_73')\nmodel_epoch1.evaluate(train_features, train_labels,)","metadata":{"cell_id":"46c2c0cc185145178c81b7eb9a24f0b0","source_hash":"df6acf16","execution_start":1667314116886,"execution_millis":338,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"10/10 [==============================] - 0s 1ms/step - loss: 11.2335 - mae: 2.5411 - mse: 11.2335\n","output_type":"stream"},{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"[11.233522415161133, 2.541062355041504, 11.233522415161133]"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"model_epoch10 = keras.models.load_model('./callback_test_chkp/chkp_10')\nmodel_epoch10.evaluate(train_features, train_labels,)","metadata":{"cell_id":"d214ab26e6d84aaba5538119b0b0ec3d","source_hash":"3349b3c7","execution_start":1667314128854,"execution_millis":1092,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"10/10 [==============================] - 0s 1ms/step - loss: 263.1689 - mae: 15.7865 - mse: 263.1689\n","output_type":"stream"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"[263.16888427734375, 15.786527633666992, 263.16888427734375]"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Learning Rate Scheduling\n\nLet's try to change the learning rate by reducing it by 0.01 after every epoch. This is just to demonstrate LR scheduling, it is not a particularly useful scheduling mechanism.","metadata":{"cell_id":"a2b3ed4574e34bafa4386c5651d9f8ac","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def my_schedule(epoch, lr):\n    return max(lr - 0.01, 0.01)","metadata":{"cell_id":"6bbed3b2f68b44888e448493dd809794","source_hash":"aa3ef491","execution_start":1667314233623,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Test if the schedule works for different input LR values:","metadata":{"cell_id":"585957b7edc8426b92ccb2b0bd81de2e","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"print(my_schedule(1, 0.05))\nprint(my_schedule(1, 0.01))","metadata":{"cell_id":"4bf1941eede3433c9d3ed7ada69f7b3b","source_hash":"54090d09","execution_start":1667314236051,"execution_millis":223,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"0.04\n0.01\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"lr_callback = keras.callbacks.LearningRateScheduler(my_schedule, verbose=1)","metadata":{"cell_id":"6862f69d5e494553b6c33c3c833fd732","source_hash":"b2b8cd20","execution_start":1667314253857,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":13},{"cell_type":"code","source":"model = get_model(train_features)\nhistory = model.fit(train_features, train_labels, epochs=200, validation_split = 0.2,\n                                callbacks=[lr_callback, es_callback])","metadata":{"cell_id":"371bb2c6b5854eabaf9069a2c09975fa","source_hash":"68668f1c","execution_start":1667314255562,"execution_millis":10375,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\nEpoch 1: LearningRateScheduler setting learning rate to 0.09000000149011612.\nEpoch 1/200\n8/8 [==============================] - 0s 19ms/step - loss: 581.6021 - mae: 22.9333 - mse: 581.6021 - val_loss: 563.3663 - val_mae: 23.0046 - val_mse: 563.3663 - lr: 0.0900\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.08000000357627869.\nEpoch 2/200\n8/8 [==============================] - 0s 4ms/step - loss: 521.6508 - mae: 22.1865 - mse: 521.6508 - val_loss: 519.0758 - val_mae: 22.3302 - val_mse: 519.0758 - lr: 0.0800\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.07000000566244126.\nEpoch 3/200\n8/8 [==============================] - 0s 8ms/step - loss: 487.2977 - mae: 21.6106 - mse: 487.2977 - val_loss: 491.0674 - val_mae: 21.7793 - val_mse: 491.0674 - lr: 0.0700\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.06000000774860382.\nEpoch 4/200\n8/8 [==============================] - 0s 4ms/step - loss: 464.6830 - mae: 21.1211 - mse: 464.6830 - val_loss: 469.8981 - val_mae: 21.3168 - val_mse: 469.8981 - lr: 0.0600\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.05000000610947609.\nEpoch 5/200\n8/8 [==============================] - 0s 10ms/step - loss: 444.9922 - mae: 20.6792 - mse: 444.9922 - val_loss: 453.3927 - val_mae: 20.9373 - val_mse: 453.3927 - lr: 0.0500\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.040000004470348356.\nEpoch 6/200\n8/8 [==============================] - 0s 4ms/step - loss: 429.6323 - mae: 20.3152 - mse: 429.6323 - val_loss: 441.4315 - val_mae: 20.6609 - val_mse: 441.4315 - lr: 0.0400\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.030000002831220625.\nEpoch 7/200\n8/8 [==============================] - 0s 7ms/step - loss: 418.0881 - mae: 20.0369 - mse: 418.0881 - val_loss: 432.4696 - val_mae: 20.4511 - val_mse: 432.4696 - lr: 0.0300\n\nEpoch 8: LearningRateScheduler setting learning rate to 0.020000003054738043.\nEpoch 8/200\n8/8 [==============================] - 0s 4ms/step - loss: 409.9249 - mae: 19.8394 - mse: 409.9249 - val_loss: 426.3792 - val_mae: 20.3054 - val_mse: 426.3792 - lr: 0.0200\n\nEpoch 9: LearningRateScheduler setting learning rate to 0.010000003278255462.\nEpoch 9/200\n8/8 [==============================] - 0s 10ms/step - loss: 404.8212 - mae: 19.7135 - mse: 404.8212 - val_loss: 423.4629 - val_mae: 20.2353 - val_mse: 423.4629 - lr: 0.0100\n\nEpoch 10: LearningRateScheduler setting learning rate to 0.01.\nEpoch 10/200\n8/8 [==============================] - 0s 4ms/step - loss: 401.7196 - mae: 19.6366 - mse: 401.7196 - val_loss: 420.5508 - val_mae: 20.1646 - val_mse: 420.5508 - lr: 0.0100\n\nEpoch 11: LearningRateScheduler setting learning rate to 0.01.\nEpoch 11/200\n8/8 [==============================] - 0s 6ms/step - loss: 398.6574 - mae: 19.5590 - mse: 398.6574 - val_loss: 417.6178 - val_mae: 20.0928 - val_mse: 417.6178 - lr: 0.0100\n\nEpoch 12: LearningRateScheduler setting learning rate to 0.01.\nEpoch 12/200\n8/8 [==============================] - 0s 4ms/step - loss: 395.6701 - mae: 19.4840 - mse: 395.6701 - val_loss: 414.6823 - val_mae: 20.0217 - val_mse: 414.6823 - lr: 0.0100\n\nEpoch 13: LearningRateScheduler setting learning rate to 0.01.\nEpoch 13/200\n8/8 [==============================] - 0s 4ms/step - loss: 392.5119 - mae: 19.4051 - mse: 392.5119 - val_loss: 411.6892 - val_mae: 19.9482 - val_mse: 411.6892 - lr: 0.0100\n\nEpoch 14: LearningRateScheduler setting learning rate to 0.01.\nEpoch 14/200\n8/8 [==============================] - 0s 4ms/step - loss: 389.4899 - mae: 19.3287 - mse: 389.4899 - val_loss: 408.7310 - val_mae: 19.8751 - val_mse: 408.7310 - lr: 0.0100\n\nEpoch 15: LearningRateScheduler setting learning rate to 0.01.\nEpoch 15/200\n8/8 [==============================] - 0s 9ms/step - loss: 386.5701 - mae: 19.2543 - mse: 386.5701 - val_loss: 405.9441 - val_mae: 19.8060 - val_mse: 405.9441 - lr: 0.0100\n\nEpoch 16: LearningRateScheduler setting learning rate to 0.01.\nEpoch 16/200\n8/8 [==============================] - 0s 5ms/step - loss: 383.4708 - mae: 19.1754 - mse: 383.4708 - val_loss: 402.9437 - val_mae: 19.7316 - val_mse: 402.9437 - lr: 0.0100\n\nEpoch 17: LearningRateScheduler setting learning rate to 0.01.\nEpoch 17/200\n8/8 [==============================] - 0s 9ms/step - loss: 380.5319 - mae: 19.0994 - mse: 380.5319 - val_loss: 400.2055 - val_mae: 19.6628 - val_mse: 400.2055 - lr: 0.0100\n\nEpoch 18: LearningRateScheduler setting learning rate to 0.01.\nEpoch 18/200\n8/8 [==============================] - 0s 6ms/step - loss: 377.4962 - mae: 19.0216 - mse: 377.4962 - val_loss: 397.3037 - val_mae: 19.5901 - val_mse: 397.3037 - lr: 0.0100\n\nEpoch 19: LearningRateScheduler setting learning rate to 0.01.\nEpoch 19/200\n8/8 [==============================] - 0s 6ms/step - loss: 374.5575 - mae: 18.9452 - mse: 374.5575 - val_loss: 394.3262 - val_mae: 19.5154 - val_mse: 394.3262 - lr: 0.0100\n\nEpoch 20: LearningRateScheduler setting learning rate to 0.01.\nEpoch 20/200\n8/8 [==============================] - 0s 4ms/step - loss: 371.6335 - mae: 18.8687 - mse: 371.6335 - val_loss: 391.5207 - val_mae: 19.4440 - val_mse: 391.5207 - lr: 0.0100\n\nEpoch 21: LearningRateScheduler setting learning rate to 0.01.\nEpoch 21/200\n8/8 [==============================] - 0s 8ms/step - loss: 368.6239 - mae: 18.7910 - mse: 368.6239 - val_loss: 388.6725 - val_mae: 19.3715 - val_mse: 388.6725 - lr: 0.0100\n\nEpoch 22: LearningRateScheduler setting learning rate to 0.01.\nEpoch 22/200\n8/8 [==============================] - 0s 4ms/step - loss: 365.7641 - mae: 18.7159 - mse: 365.7641 - val_loss: 385.8532 - val_mae: 19.2996 - val_mse: 385.8532 - lr: 0.0100\n\nEpoch 23: LearningRateScheduler setting learning rate to 0.01.\nEpoch 23/200\n8/8 [==============================] - 0s 4ms/step - loss: 362.8784 - mae: 18.6401 - mse: 362.8784 - val_loss: 382.9461 - val_mae: 19.2256 - val_mse: 382.9461 - lr: 0.0100\n\nEpoch 24: LearningRateScheduler setting learning rate to 0.01.\nEpoch 24/200\n8/8 [==============================] - 0s 4ms/step - loss: 359.9991 - mae: 18.5641 - mse: 359.9991 - val_loss: 380.3233 - val_mae: 19.1570 - val_mse: 380.3233 - lr: 0.0100\n\nEpoch 25: LearningRateScheduler setting learning rate to 0.01.\nEpoch 25/200\n8/8 [==============================] - 0s 4ms/step - loss: 357.1971 - mae: 18.4882 - mse: 357.1971 - val_loss: 377.6100 - val_mae: 19.0863 - val_mse: 377.6100 - lr: 0.0100\n\nEpoch 26: LearningRateScheduler setting learning rate to 0.01.\nEpoch 26/200\n8/8 [==============================] - 0s 10ms/step - loss: 354.2729 - mae: 18.4111 - mse: 354.2729 - val_loss: 374.6349 - val_mae: 19.0100 - val_mse: 374.6349 - lr: 0.0100\n\nEpoch 27: LearningRateScheduler setting learning rate to 0.01.\nEpoch 27/200\n8/8 [==============================] - 0s 4ms/step - loss: 351.4584 - mae: 18.3355 - mse: 351.4584 - val_loss: 371.8798 - val_mae: 18.9384 - val_mse: 371.8798 - lr: 0.0100\n\nEpoch 28: LearningRateScheduler setting learning rate to 0.01.\nEpoch 28/200\n8/8 [==============================] - 0s 7ms/step - loss: 348.6075 - mae: 18.2595 - mse: 348.6075 - val_loss: 369.0272 - val_mae: 18.8642 - val_mse: 369.0272 - lr: 0.0100\n\nEpoch 29: LearningRateScheduler setting learning rate to 0.01.\nEpoch 29/200\n8/8 [==============================] - 0s 4ms/step - loss: 345.8451 - mae: 18.1845 - mse: 345.8451 - val_loss: 366.3033 - val_mae: 18.7927 - val_mse: 366.3033 - lr: 0.0100\n\nEpoch 30: LearningRateScheduler setting learning rate to 0.01.\nEpoch 30/200\n8/8 [==============================] - 0s 9ms/step - loss: 343.0290 - mae: 18.1089 - mse: 343.0290 - val_loss: 363.4987 - val_mae: 18.7191 - val_mse: 363.4987 - lr: 0.0100\n\nEpoch 31: LearningRateScheduler setting learning rate to 0.01.\nEpoch 31/200\n8/8 [==============================] - 0s 4ms/step - loss: 340.2385 - mae: 18.0331 - mse: 340.2385 - val_loss: 360.8644 - val_mae: 18.6488 - val_mse: 360.8644 - lr: 0.0100\n\nEpoch 32: LearningRateScheduler setting learning rate to 0.01.\nEpoch 32/200\n8/8 [==============================] - 0s 7ms/step - loss: 337.5209 - mae: 17.9579 - mse: 337.5209 - val_loss: 358.2081 - val_mae: 18.5776 - val_mse: 358.2081 - lr: 0.0100\n\nEpoch 33: LearningRateScheduler setting learning rate to 0.01.\nEpoch 33/200\n8/8 [==============================] - 0s 4ms/step - loss: 334.7693 - mae: 17.8825 - mse: 334.7693 - val_loss: 355.4555 - val_mae: 18.5046 - val_mse: 355.4555 - lr: 0.0100\n\nEpoch 34: LearningRateScheduler setting learning rate to 0.01.\nEpoch 34/200\n8/8 [==============================] - 0s 8ms/step - loss: 332.0748 - mae: 17.8084 - mse: 332.0748 - val_loss: 352.8074 - val_mae: 18.4336 - val_mse: 352.8074 - lr: 0.0100\n\nEpoch 35: LearningRateScheduler setting learning rate to 0.01.\nEpoch 35/200\n8/8 [==============================] - 0s 4ms/step - loss: 329.4009 - mae: 17.7342 - mse: 329.4009 - val_loss: 350.1130 - val_mae: 18.3613 - val_mse: 350.1130 - lr: 0.0100\n\nEpoch 36: LearningRateScheduler setting learning rate to 0.01.\nEpoch 36/200\n8/8 [==============================] - 0s 10ms/step - loss: 326.6703 - mae: 17.6583 - mse: 326.6703 - val_loss: 347.5240 - val_mae: 18.2908 - val_mse: 347.5240 - lr: 0.0100\n\nEpoch 37: LearningRateScheduler setting learning rate to 0.01.\nEpoch 37/200\n8/8 [==============================] - 0s 5ms/step - loss: 324.0017 - mae: 17.5838 - mse: 324.0017 - val_loss: 344.7991 - val_mae: 18.2174 - val_mse: 344.7991 - lr: 0.0100\n\nEpoch 38: LearningRateScheduler setting learning rate to 0.01.\nEpoch 38/200\n8/8 [==============================] - 0s 8ms/step - loss: 321.5022 - mae: 17.5133 - mse: 321.5022 - val_loss: 342.0170 - val_mae: 18.1426 - val_mse: 342.0170 - lr: 0.0100\n\nEpoch 39: LearningRateScheduler setting learning rate to 0.01.\nEpoch 39/200\n8/8 [==============================] - 0s 5ms/step - loss: 318.7364 - mae: 17.4359 - mse: 318.7364 - val_loss: 339.5008 - val_mae: 18.0731 - val_mse: 339.5008 - lr: 0.0100\n\nEpoch 40: LearningRateScheduler setting learning rate to 0.01.\nEpoch 40/200\n8/8 [==============================] - 0s 8ms/step - loss: 316.2316 - mae: 17.3639 - mse: 316.2316 - val_loss: 337.0720 - val_mae: 18.0051 - val_mse: 337.0720 - lr: 0.0100\n\nEpoch 41: LearningRateScheduler setting learning rate to 0.01.\nEpoch 41/200\n8/8 [==============================] - 0s 5ms/step - loss: 313.5677 - mae: 17.2878 - mse: 313.5677 - val_loss: 334.4585 - val_mae: 17.9331 - val_mse: 334.4585 - lr: 0.0100\n\nEpoch 42: LearningRateScheduler setting learning rate to 0.01.\nEpoch 42/200\n8/8 [==============================] - 0s 9ms/step - loss: 310.9364 - mae: 17.2131 - mse: 310.9364 - val_loss: 331.8530 - val_mae: 17.8610 - val_mse: 331.8530 - lr: 0.0100\n\nEpoch 43: LearningRateScheduler setting learning rate to 0.01.\nEpoch 43/200\n8/8 [==============================] - 0s 4ms/step - loss: 308.4008 - mae: 17.1404 - mse: 308.4008 - val_loss: 329.2969 - val_mae: 17.7900 - val_mse: 329.2969 - lr: 0.0100\n\nEpoch 44: LearningRateScheduler setting learning rate to 0.01.\nEpoch 44/200\n8/8 [==============================] - 0s 8ms/step - loss: 305.8717 - mae: 17.0670 - mse: 305.8717 - val_loss: 326.7378 - val_mae: 17.7186 - val_mse: 326.7378 - lr: 0.0100\n\nEpoch 45: LearningRateScheduler setting learning rate to 0.01.\nEpoch 45/200\n8/8 [==============================] - 0s 5ms/step - loss: 303.3745 - mae: 16.9952 - mse: 303.3745 - val_loss: 324.2413 - val_mae: 17.6482 - val_mse: 324.2413 - lr: 0.0100\n\nEpoch 46: LearningRateScheduler setting learning rate to 0.01.\nEpoch 46/200\n8/8 [==============================] - 0s 9ms/step - loss: 300.8025 - mae: 16.9204 - mse: 300.8025 - val_loss: 321.6563 - val_mae: 17.5756 - val_mse: 321.6563 - lr: 0.0100\n\nEpoch 47: LearningRateScheduler setting learning rate to 0.01.\nEpoch 47/200\n8/8 [==============================] - 0s 4ms/step - loss: 298.3115 - mae: 16.8471 - mse: 298.3115 - val_loss: 319.1552 - val_mae: 17.5048 - val_mse: 319.1552 - lr: 0.0100\n\nEpoch 48: LearningRateScheduler setting learning rate to 0.01.\nEpoch 48/200\n8/8 [==============================] - 0s 8ms/step - loss: 295.8225 - mae: 16.7743 - mse: 295.8225 - val_loss: 316.7479 - val_mae: 17.4358 - val_mse: 316.7479 - lr: 0.0100\n\nEpoch 49: LearningRateScheduler setting learning rate to 0.01.\nEpoch 49/200\n8/8 [==============================] - 0s 4ms/step - loss: 293.4188 - mae: 16.7023 - mse: 293.4188 - val_loss: 314.2579 - val_mae: 17.3649 - val_mse: 314.2579 - lr: 0.0100\n\nEpoch 50: LearningRateScheduler setting learning rate to 0.01.\nEpoch 50/200\n8/8 [==============================] - 0s 9ms/step - loss: 290.9630 - mae: 16.6301 - mse: 290.9630 - val_loss: 311.9046 - val_mae: 17.2965 - val_mse: 311.9046 - lr: 0.0100\n\nEpoch 51: LearningRateScheduler setting learning rate to 0.01.\nEpoch 51/200\n8/8 [==============================] - 0s 4ms/step - loss: 288.5037 - mae: 16.5572 - mse: 288.5037 - val_loss: 309.4032 - val_mae: 17.2249 - val_mse: 309.4032 - lr: 0.0100\n\nEpoch 52: LearningRateScheduler setting learning rate to 0.01.\nEpoch 52/200\n8/8 [==============================] - 0s 4ms/step - loss: 286.0928 - mae: 16.4851 - mse: 286.0928 - val_loss: 306.9072 - val_mae: 17.1531 - val_mse: 306.9072 - lr: 0.0100\n\nEpoch 53: LearningRateScheduler setting learning rate to 0.01.\nEpoch 53/200\n8/8 [==============================] - 0s 8ms/step - loss: 283.6925 - mae: 16.4132 - mse: 283.6925 - val_loss: 304.5434 - val_mae: 17.0843 - val_mse: 304.5434 - lr: 0.0100\n\nEpoch 54: LearningRateScheduler setting learning rate to 0.01.\nEpoch 54/200\n8/8 [==============================] - 0s 5ms/step - loss: 281.3156 - mae: 16.3413 - mse: 281.3156 - val_loss: 302.1090 - val_mae: 17.0135 - val_mse: 302.1090 - lr: 0.0100\n\nEpoch 55: LearningRateScheduler setting learning rate to 0.01.\nEpoch 55/200\n8/8 [==============================] - 0s 5ms/step - loss: 278.9957 - mae: 16.2710 - mse: 278.9957 - val_loss: 299.5748 - val_mae: 16.9406 - val_mse: 299.5748 - lr: 0.0100\n\nEpoch 56: LearningRateScheduler setting learning rate to 0.01.\nEpoch 56/200\n8/8 [==============================] - 0s 6ms/step - loss: 276.6102 - mae: 16.1987 - mse: 276.6102 - val_loss: 297.2256 - val_mae: 16.8712 - val_mse: 297.2256 - lr: 0.0100\n\nEpoch 57: LearningRateScheduler setting learning rate to 0.01.\nEpoch 57/200\n8/8 [==============================] - 0s 5ms/step - loss: 274.2728 - mae: 16.1270 - mse: 274.2728 - val_loss: 294.8837 - val_mae: 16.8018 - val_mse: 294.8837 - lr: 0.0100\n\nEpoch 58: LearningRateScheduler setting learning rate to 0.01.\nEpoch 58/200\n8/8 [==============================] - 0s 4ms/step - loss: 271.9679 - mae: 16.0557 - mse: 271.9679 - val_loss: 292.5168 - val_mae: 16.7318 - val_mse: 292.5168 - lr: 0.0100\n\nEpoch 59: LearningRateScheduler setting learning rate to 0.01.\nEpoch 59/200\n8/8 [==============================] - 0s 6ms/step - loss: 269.6436 - mae: 15.9843 - mse: 269.6436 - val_loss: 290.2585 - val_mae: 16.6637 - val_mse: 290.2585 - lr: 0.0100\n\nEpoch 60: LearningRateScheduler setting learning rate to 0.01.\nEpoch 60/200\n8/8 [==============================] - 0s 4ms/step - loss: 267.3265 - mae: 15.9125 - mse: 267.3265 - val_loss: 287.8632 - val_mae: 16.5927 - val_mse: 287.8632 - lr: 0.0100\n\nEpoch 61: LearningRateScheduler setting learning rate to 0.01.\nEpoch 61/200\n8/8 [==============================] - 0s 8ms/step - loss: 265.0454 - mae: 15.8418 - mse: 265.0454 - val_loss: 285.6516 - val_mae: 16.5253 - val_mse: 285.6516 - lr: 0.0100\n\nEpoch 62: LearningRateScheduler setting learning rate to 0.01.\nEpoch 62/200\n8/8 [==============================] - 0s 6ms/step - loss: 262.8846 - mae: 15.7730 - mse: 262.8846 - val_loss: 283.4899 - val_mae: 16.4585 - val_mse: 283.4899 - lr: 0.0100\n\nEpoch 63: LearningRateScheduler setting learning rate to 0.01.\nEpoch 63/200\n8/8 [==============================] - 0s 6ms/step - loss: 260.5685 - mae: 15.7006 - mse: 260.5685 - val_loss: 281.2455 - val_mae: 16.3903 - val_mse: 281.2455 - lr: 0.0100\n\nEpoch 64: LearningRateScheduler setting learning rate to 0.01.\nEpoch 64/200\n8/8 [==============================] - 0s 4ms/step - loss: 258.3433 - mae: 15.6292 - mse: 258.3433 - val_loss: 278.8006 - val_mae: 16.3174 - val_mse: 278.8006 - lr: 0.0100\n\nEpoch 65: LearningRateScheduler setting learning rate to 0.01.\nEpoch 65/200\n8/8 [==============================] - 0s 9ms/step - loss: 256.1460 - mae: 15.5608 - mse: 256.1460 - val_loss: 276.6071 - val_mae: 16.2498 - val_mse: 276.6071 - lr: 0.0100\n\nEpoch 66: LearningRateScheduler setting learning rate to 0.01.\nEpoch 66/200\n8/8 [==============================] - 0s 5ms/step - loss: 253.9199 - mae: 15.4895 - mse: 253.9199 - val_loss: 274.3810 - val_mae: 16.1813 - val_mse: 274.3810 - lr: 0.0100\n\nEpoch 67: LearningRateScheduler setting learning rate to 0.01.\nEpoch 67/200\n8/8 [==============================] - 0s 5ms/step - loss: 251.7287 - mae: 15.4194 - mse: 251.7287 - val_loss: 272.1074 - val_mae: 16.1116 - val_mse: 272.1074 - lr: 0.0100\n\nEpoch 68: LearningRateScheduler setting learning rate to 0.01.\nEpoch 68/200\n8/8 [==============================] - 0s 4ms/step - loss: 249.5719 - mae: 15.3498 - mse: 249.5719 - val_loss: 269.7564 - val_mae: 16.0400 - val_mse: 269.7564 - lr: 0.0100\n\nEpoch 69: LearningRateScheduler setting learning rate to 0.01.\nEpoch 69/200\n8/8 [==============================] - 0s 9ms/step - loss: 247.4792 - mae: 15.2825 - mse: 247.4792 - val_loss: 267.6359 - val_mae: 15.9733 - val_mse: 267.6359 - lr: 0.0100\n\nEpoch 70: LearningRateScheduler setting learning rate to 0.01.\nEpoch 70/200\n8/8 [==============================] - 0s 4ms/step - loss: 245.2628 - mae: 15.2109 - mse: 245.2628 - val_loss: 265.4616 - val_mae: 15.9053 - val_mse: 265.4616 - lr: 0.0100\n\nEpoch 71: LearningRateScheduler setting learning rate to 0.01.\nEpoch 71/200\n8/8 [==============================] - 0s 10ms/step - loss: 243.1836 - mae: 15.1428 - mse: 243.1836 - val_loss: 263.2750 - val_mae: 15.8368 - val_mse: 263.2750 - lr: 0.0100\n\nEpoch 72: LearningRateScheduler setting learning rate to 0.01.\nEpoch 72/200\n8/8 [==============================] - 0s 5ms/step - loss: 241.0190 - mae: 15.0722 - mse: 241.0190 - val_loss: 261.1830 - val_mae: 15.7703 - val_mse: 261.1830 - lr: 0.0100\n\nEpoch 73: LearningRateScheduler setting learning rate to 0.01.\nEpoch 73/200\n8/8 [==============================] - 0s 7ms/step - loss: 239.0462 - mae: 15.0070 - mse: 239.0462 - val_loss: 258.8118 - val_mae: 15.6977 - val_mse: 258.8118 - lr: 0.0100\n\nEpoch 74: LearningRateScheduler setting learning rate to 0.01.\nEpoch 74/200\n8/8 [==============================] - 0s 5ms/step - loss: 236.8969 - mae: 14.9359 - mse: 236.8969 - val_loss: 256.6466 - val_mae: 15.6292 - val_mse: 256.6466 - lr: 0.0100\n\nEpoch 75: LearningRateScheduler setting learning rate to 0.01.\nEpoch 75/200\n8/8 [==============================] - 0s 4ms/step - loss: 234.7752 - mae: 14.8662 - mse: 234.7752 - val_loss: 254.5639 - val_mae: 15.5623 - val_mse: 254.5639 - lr: 0.0100\n\nEpoch 76: LearningRateScheduler setting learning rate to 0.01.\nEpoch 76/200\n8/8 [==============================] - 0s 9ms/step - loss: 232.7622 - mae: 14.7982 - mse: 232.7622 - val_loss: 252.4791 - val_mae: 15.4950 - val_mse: 252.4791 - lr: 0.0100\n\nEpoch 77: LearningRateScheduler setting learning rate to 0.01.\nEpoch 77/200\n8/8 [==============================] - 0s 4ms/step - loss: 230.7179 - mae: 14.7301 - mse: 230.7179 - val_loss: 250.3875 - val_mae: 15.4276 - val_mse: 250.3875 - lr: 0.0100\n\nEpoch 78: LearningRateScheduler setting learning rate to 0.01.\nEpoch 78/200\n8/8 [==============================] - 0s 4ms/step - loss: 228.7260 - mae: 14.6625 - mse: 228.7260 - val_loss: 248.2682 - val_mae: 15.3593 - val_mse: 248.2682 - lr: 0.0100\n\nEpoch 79: LearningRateScheduler setting learning rate to 0.01.\nEpoch 79/200\n8/8 [==============================] - 0s 9ms/step - loss: 226.6669 - mae: 14.5934 - mse: 226.6669 - val_loss: 246.3478 - val_mae: 15.2955 - val_mse: 246.3478 - lr: 0.0100\n\nEpoch 80: LearningRateScheduler setting learning rate to 0.01.\nEpoch 80/200\n8/8 [==============================] - 0s 5ms/step - loss: 224.7482 - mae: 14.5270 - mse: 224.7482 - val_loss: 244.2784 - val_mae: 15.2282 - val_mse: 244.2784 - lr: 0.0100\n\nEpoch 81: LearningRateScheduler setting learning rate to 0.01.\nEpoch 81/200\n8/8 [==============================] - 0s 6ms/step - loss: 222.7519 - mae: 14.4588 - mse: 222.7519 - val_loss: 242.2663 - val_mae: 15.1615 - val_mse: 242.2663 - lr: 0.0100\n\nEpoch 82: LearningRateScheduler setting learning rate to 0.01.\nEpoch 82/200\n8/8 [==============================] - 0s 5ms/step - loss: 220.7823 - mae: 14.3908 - mse: 220.7823 - val_loss: 240.2341 - val_mae: 15.0945 - val_mse: 240.2341 - lr: 0.0100\n\nEpoch 83: LearningRateScheduler setting learning rate to 0.01.\nEpoch 83/200\n8/8 [==============================] - 0s 4ms/step - loss: 218.7963 - mae: 14.3233 - mse: 218.7963 - val_loss: 238.1186 - val_mae: 15.0258 - val_mse: 238.1186 - lr: 0.0100\n\nEpoch 84: LearningRateScheduler setting learning rate to 0.01.\nEpoch 84/200\n8/8 [==============================] - 0s 4ms/step - loss: 216.8444 - mae: 14.2566 - mse: 216.8444 - val_loss: 236.0563 - val_mae: 14.9581 - val_mse: 236.0563 - lr: 0.0100\n\nEpoch 85: LearningRateScheduler setting learning rate to 0.01.\nEpoch 85/200\n8/8 [==============================] - 0s 9ms/step - loss: 214.9065 - mae: 14.1880 - mse: 214.9065 - val_loss: 234.1602 - val_mae: 14.8934 - val_mse: 234.1602 - lr: 0.0100\n\nEpoch 86: LearningRateScheduler setting learning rate to 0.01.\nEpoch 86/200\n8/8 [==============================] - 0s 4ms/step - loss: 213.0877 - mae: 14.1238 - mse: 213.0877 - val_loss: 232.3004 - val_mae: 14.8292 - val_mse: 232.3004 - lr: 0.0100\n\nEpoch 87: LearningRateScheduler setting learning rate to 0.01.\nEpoch 87/200\n8/8 [==============================] - 0s 8ms/step - loss: 211.1161 - mae: 14.0538 - mse: 211.1161 - val_loss: 230.2093 - val_mae: 14.7603 - val_mse: 230.2093 - lr: 0.0100\n\nEpoch 88: LearningRateScheduler setting learning rate to 0.01.\nEpoch 88/200\n8/8 [==============================] - 0s 4ms/step - loss: 209.2070 - mae: 13.9877 - mse: 209.2070 - val_loss: 228.2038 - val_mae: 14.6931 - val_mse: 228.2038 - lr: 0.0100\n\nEpoch 89: LearningRateScheduler setting learning rate to 0.01.\nEpoch 89/200\n8/8 [==============================] - 0s 7ms/step - loss: 207.3579 - mae: 13.9214 - mse: 207.3579 - val_loss: 226.3024 - val_mae: 14.6282 - val_mse: 226.3024 - lr: 0.0100\n\nEpoch 90: LearningRateScheduler setting learning rate to 0.01.\nEpoch 90/200\n8/8 [==============================] - 0s 5ms/step - loss: 205.5246 - mae: 13.8556 - mse: 205.5246 - val_loss: 224.3526 - val_mae: 14.5620 - val_mse: 224.3526 - lr: 0.0100\n\nEpoch 91: LearningRateScheduler setting learning rate to 0.01.\nEpoch 91/200\n8/8 [==============================] - 0s 9ms/step - loss: 203.6388 - mae: 13.7884 - mse: 203.6388 - val_loss: 222.4706 - val_mae: 14.4971 - val_mse: 222.4706 - lr: 0.0100\n\nEpoch 92: LearningRateScheduler setting learning rate to 0.01.\nEpoch 92/200\n8/8 [==============================] - 0s 5ms/step - loss: 201.8164 - mae: 13.7228 - mse: 201.8164 - val_loss: 220.5902 - val_mae: 14.4320 - val_mse: 220.5902 - lr: 0.0100\n\nEpoch 93: LearningRateScheduler setting learning rate to 0.01.\nEpoch 93/200\n8/8 [==============================] - 0s 4ms/step - loss: 199.9874 - mae: 13.6564 - mse: 199.9874 - val_loss: 218.6500 - val_mae: 14.3654 - val_mse: 218.6500 - lr: 0.0100\n\nEpoch 94: LearningRateScheduler setting learning rate to 0.01.\nEpoch 94/200\n8/8 [==============================] - 0s 5ms/step - loss: 198.1891 - mae: 13.5907 - mse: 198.1891 - val_loss: 216.7668 - val_mae: 14.3000 - val_mse: 216.7668 - lr: 0.0100\n\nEpoch 95: LearningRateScheduler setting learning rate to 0.01.\nEpoch 95/200\n8/8 [==============================] - 0s 5ms/step - loss: 196.4132 - mae: 13.5264 - mse: 196.4132 - val_loss: 214.9366 - val_mae: 14.2357 - val_mse: 214.9366 - lr: 0.0100\n\nEpoch 96: LearningRateScheduler setting learning rate to 0.01.\nEpoch 96/200\n8/8 [==============================] - 0s 10ms/step - loss: 194.6484 - mae: 13.4607 - mse: 194.6484 - val_loss: 213.0537 - val_mae: 14.1698 - val_mse: 213.0537 - lr: 0.0100\n\nEpoch 97: LearningRateScheduler setting learning rate to 0.01.\nEpoch 97/200\n8/8 [==============================] - 0s 4ms/step - loss: 192.8610 - mae: 13.3947 - mse: 192.8610 - val_loss: 211.3083 - val_mae: 14.1070 - val_mse: 211.3083 - lr: 0.0100\n\nEpoch 98: LearningRateScheduler setting learning rate to 0.01.\nEpoch 98/200\n8/8 [==============================] - 0s 4ms/step - loss: 191.1226 - mae: 13.3304 - mse: 191.1226 - val_loss: 209.4115 - val_mae: 14.0410 - val_mse: 209.4115 - lr: 0.0100\n\nEpoch 99: LearningRateScheduler setting learning rate to 0.01.\nEpoch 99/200\n8/8 [==============================] - 0s 4ms/step - loss: 189.3608 - mae: 13.2648 - mse: 189.3608 - val_loss: 207.6028 - val_mae: 13.9764 - val_mse: 207.6028 - lr: 0.0100\n\nEpoch 100: LearningRateScheduler setting learning rate to 0.01.\nEpoch 100/200\n8/8 [==============================] - 0s 5ms/step - loss: 187.6189 - mae: 13.1991 - mse: 187.6189 - val_loss: 205.8577 - val_mae: 13.9131 - val_mse: 205.8577 - lr: 0.0100\n\nEpoch 101: LearningRateScheduler setting learning rate to 0.01.\nEpoch 101/200\n8/8 [==============================] - 0s 7ms/step - loss: 185.9503 - mae: 13.1356 - mse: 185.9503 - val_loss: 204.1292 - val_mae: 13.8500 - val_mse: 204.1292 - lr: 0.0100\n\nEpoch 102: LearningRateScheduler setting learning rate to 0.01.\nEpoch 102/200\n8/8 [==============================] - 0s 4ms/step - loss: 184.2052 - mae: 13.0699 - mse: 184.2052 - val_loss: 202.3689 - val_mae: 13.7865 - val_mse: 202.3689 - lr: 0.0100\n\nEpoch 103: LearningRateScheduler setting learning rate to 0.01.\nEpoch 103/200\n8/8 [==============================] - 0s 7ms/step - loss: 182.5497 - mae: 13.0063 - mse: 182.5497 - val_loss: 200.5676 - val_mae: 13.7218 - val_mse: 200.5676 - lr: 0.0100\n\nEpoch 104: LearningRateScheduler setting learning rate to 0.01.\nEpoch 104/200\n8/8 [==============================] - 0s 7ms/step - loss: 180.8280 - mae: 12.9409 - mse: 180.8280 - val_loss: 198.8274 - val_mae: 13.6583 - val_mse: 198.8274 - lr: 0.0100\n\nEpoch 105: LearningRateScheduler setting learning rate to 0.01.\nEpoch 105/200\n8/8 [==============================] - 0s 4ms/step - loss: 179.1556 - mae: 12.8776 - mse: 179.1556 - val_loss: 197.0253 - val_mae: 13.5933 - val_mse: 197.0253 - lr: 0.0100\n\nEpoch 106: LearningRateScheduler setting learning rate to 0.01.\nEpoch 106/200\n8/8 [==============================] - 0s 9ms/step - loss: 177.5430 - mae: 12.8150 - mse: 177.5430 - val_loss: 195.2831 - val_mae: 13.5295 - val_mse: 195.2831 - lr: 0.0100\n\nEpoch 107: LearningRateScheduler setting learning rate to 0.01.\nEpoch 107/200\n8/8 [==============================] - 0s 4ms/step - loss: 175.8784 - mae: 12.7500 - mse: 175.8784 - val_loss: 193.6405 - val_mae: 13.4675 - val_mse: 193.6405 - lr: 0.0100\n\nEpoch 108: LearningRateScheduler setting learning rate to 0.01.\nEpoch 108/200\n8/8 [==============================] - 0s 8ms/step - loss: 174.2794 - mae: 12.6865 - mse: 174.2794 - val_loss: 192.0310 - val_mae: 13.4061 - val_mse: 192.0310 - lr: 0.0100\n\nEpoch 109: LearningRateScheduler setting learning rate to 0.01.\nEpoch 109/200\n8/8 [==============================] - 0s 4ms/step - loss: 172.6335 - mae: 12.6226 - mse: 172.6335 - val_loss: 190.2165 - val_mae: 13.3402 - val_mse: 190.2165 - lr: 0.0100\n\nEpoch 110: LearningRateScheduler setting learning rate to 0.01.\nEpoch 110/200\n8/8 [==============================] - 0s 8ms/step - loss: 171.0256 - mae: 12.5596 - mse: 171.0256 - val_loss: 188.5142 - val_mae: 13.2765 - val_mse: 188.5142 - lr: 0.0100\n\nEpoch 111: LearningRateScheduler setting learning rate to 0.01.\nEpoch 111/200\n8/8 [==============================] - 0s 5ms/step - loss: 169.4484 - mae: 12.4968 - mse: 169.4484 - val_loss: 186.8249 - val_mae: 13.2130 - val_mse: 186.8249 - lr: 0.0100\n\nEpoch 112: LearningRateScheduler setting learning rate to 0.01.\nEpoch 112/200\n8/8 [==============================] - 0s 9ms/step - loss: 167.8403 - mae: 12.4332 - mse: 167.8403 - val_loss: 185.2698 - val_mae: 13.1529 - val_mse: 185.2698 - lr: 0.0100\n\nEpoch 113: LearningRateScheduler setting learning rate to 0.01.\nEpoch 113/200\n8/8 [==============================] - 0s 6ms/step - loss: 166.2610 - mae: 12.3695 - mse: 166.2610 - val_loss: 183.5484 - val_mae: 13.0886 - val_mse: 183.5484 - lr: 0.0100\n\nEpoch 114: LearningRateScheduler setting learning rate to 0.01.\nEpoch 114/200\n8/8 [==============================] - 0s 4ms/step - loss: 164.7172 - mae: 12.3076 - mse: 164.7172 - val_loss: 181.8954 - val_mae: 13.0258 - val_mse: 181.8954 - lr: 0.0100\n\nEpoch 115: LearningRateScheduler setting learning rate to 0.01.\nEpoch 115/200\n8/8 [==============================] - 0s 6ms/step - loss: 163.1859 - mae: 12.2463 - mse: 163.1859 - val_loss: 180.1346 - val_mae: 12.9604 - val_mse: 180.1346 - lr: 0.0100\n\nEpoch 116: LearningRateScheduler setting learning rate to 0.01.\nEpoch 116/200\n8/8 [==============================] - 0s 8ms/step - loss: 161.6616 - mae: 12.1850 - mse: 161.6616 - val_loss: 178.4605 - val_mae: 12.8965 - val_mse: 178.4605 - lr: 0.0100\n\nEpoch 117: LearningRateScheduler setting learning rate to 0.01.\nEpoch 117/200\n8/8 [==============================] - 0s 7ms/step - loss: 160.0813 - mae: 12.1213 - mse: 160.0813 - val_loss: 176.8483 - val_mae: 12.8344 - val_mse: 176.8483 - lr: 0.0100\n\nEpoch 118: LearningRateScheduler setting learning rate to 0.01.\nEpoch 118/200\n8/8 [==============================] - 0s 6ms/step - loss: 158.6242 - mae: 12.0600 - mse: 158.6242 - val_loss: 175.2271 - val_mae: 12.7719 - val_mse: 175.2271 - lr: 0.0100\n\nEpoch 119: LearningRateScheduler setting learning rate to 0.01.\nEpoch 119/200\n8/8 [==============================] - 0s 4ms/step - loss: 157.0806 - mae: 11.9972 - mse: 157.0806 - val_loss: 173.6233 - val_mae: 12.7095 - val_mse: 173.6233 - lr: 0.0100\n\nEpoch 120: LearningRateScheduler setting learning rate to 0.01.\nEpoch 120/200\n8/8 [==============================] - 0s 8ms/step - loss: 155.5872 - mae: 11.9356 - mse: 155.5872 - val_loss: 172.0441 - val_mae: 12.6477 - val_mse: 172.0441 - lr: 0.0100\n\nEpoch 121: LearningRateScheduler setting learning rate to 0.01.\nEpoch 121/200\n8/8 [==============================] - 0s 6ms/step - loss: 154.1120 - mae: 11.8745 - mse: 154.1120 - val_loss: 170.5282 - val_mae: 12.5871 - val_mse: 170.5282 - lr: 0.0100\n\nEpoch 122: LearningRateScheduler setting learning rate to 0.01.\nEpoch 122/200\n8/8 [==============================] - 0s 8ms/step - loss: 152.6541 - mae: 11.8135 - mse: 152.6541 - val_loss: 169.0015 - val_mae: 12.5263 - val_mse: 169.0015 - lr: 0.0100\n\nEpoch 123: LearningRateScheduler setting learning rate to 0.01.\nEpoch 123/200\n8/8 [==============================] - 0s 9ms/step - loss: 151.2070 - mae: 11.7527 - mse: 151.2070 - val_loss: 167.4840 - val_mae: 12.4655 - val_mse: 167.4840 - lr: 0.0100\n\nEpoch 124: LearningRateScheduler setting learning rate to 0.01.\nEpoch 124/200\n8/8 [==============================] - 0s 4ms/step - loss: 149.8301 - mae: 11.6941 - mse: 149.8301 - val_loss: 166.1075 - val_mae: 12.4077 - val_mse: 166.1075 - lr: 0.0100\n\nEpoch 125: LearningRateScheduler setting learning rate to 0.01.\nEpoch 125/200\n8/8 [==============================] - 0s 4ms/step - loss: 148.3189 - mae: 11.6307 - mse: 148.3189 - val_loss: 164.4735 - val_mae: 12.3440 - val_mse: 164.4735 - lr: 0.0100\n\nEpoch 126: LearningRateScheduler setting learning rate to 0.01.\nEpoch 126/200\n8/8 [==============================] - 0s 8ms/step - loss: 146.9173 - mae: 11.5708 - mse: 146.9173 - val_loss: 162.9649 - val_mae: 12.2828 - val_mse: 162.9649 - lr: 0.0100\n\nEpoch 127: LearningRateScheduler setting learning rate to 0.01.\nEpoch 127/200\n8/8 [==============================] - 0s 4ms/step - loss: 145.4833 - mae: 11.5100 - mse: 145.4833 - val_loss: 161.4147 - val_mae: 12.2210 - val_mse: 161.4147 - lr: 0.0100\n\nEpoch 128: LearningRateScheduler setting learning rate to 0.01.\nEpoch 128/200\n8/8 [==============================] - 0s 10ms/step - loss: 144.0963 - mae: 11.4506 - mse: 144.0963 - val_loss: 159.8979 - val_mae: 12.1595 - val_mse: 159.8979 - lr: 0.0100\n\nEpoch 129: LearningRateScheduler setting learning rate to 0.01.\nEpoch 129/200\n8/8 [==============================] - 0s 4ms/step - loss: 142.7255 - mae: 11.3906 - mse: 142.7255 - val_loss: 158.4413 - val_mae: 12.0996 - val_mse: 158.4413 - lr: 0.0100\n\nEpoch 130: LearningRateScheduler setting learning rate to 0.01.\nEpoch 130/200\n8/8 [==============================] - 0s 4ms/step - loss: 141.3364 - mae: 11.3318 - mse: 141.3364 - val_loss: 156.9513 - val_mae: 12.0385 - val_mse: 156.9513 - lr: 0.0100\n\nEpoch 131: LearningRateScheduler setting learning rate to 0.01.\nEpoch 131/200\n8/8 [==============================] - 0s 5ms/step - loss: 139.9799 - mae: 11.2721 - mse: 139.9799 - val_loss: 155.4882 - val_mae: 11.9782 - val_mse: 155.4882 - lr: 0.0100\n\nEpoch 132: LearningRateScheduler setting learning rate to 0.01.\nEpoch 132/200\n8/8 [==============================] - 0s 5ms/step - loss: 138.5984 - mae: 11.2120 - mse: 138.5984 - val_loss: 154.0364 - val_mae: 11.9180 - val_mse: 154.0364 - lr: 0.0100\n\nEpoch 133: LearningRateScheduler setting learning rate to 0.01.\nEpoch 133/200\n8/8 [==============================] - 0s 6ms/step - loss: 137.2477 - mae: 11.1523 - mse: 137.2477 - val_loss: 152.6602 - val_mae: 11.8595 - val_mse: 152.6602 - lr: 0.0100\n\nEpoch 134: LearningRateScheduler setting learning rate to 0.01.\nEpoch 134/200\n8/8 [==============================] - 0s 4ms/step - loss: 135.9243 - mae: 11.0938 - mse: 135.9243 - val_loss: 151.2719 - val_mae: 11.8006 - val_mse: 151.2719 - lr: 0.0100\n\nEpoch 135: LearningRateScheduler setting learning rate to 0.01.\nEpoch 135/200\n8/8 [==============================] - 0s 6ms/step - loss: 134.6180 - mae: 11.0344 - mse: 134.6180 - val_loss: 149.7773 - val_mae: 11.7387 - val_mse: 149.7773 - lr: 0.0100\n\nEpoch 136: LearningRateScheduler setting learning rate to 0.01.\nEpoch 136/200\n8/8 [==============================] - 0s 5ms/step - loss: 133.3332 - mae: 10.9786 - mse: 133.3332 - val_loss: 148.2934 - val_mae: 11.6774 - val_mse: 148.2934 - lr: 0.0100\n\nEpoch 137: LearningRateScheduler setting learning rate to 0.01.\nEpoch 137/200\n8/8 [==============================] - 0s 8ms/step - loss: 131.9950 - mae: 10.9181 - mse: 131.9950 - val_loss: 146.9026 - val_mae: 11.6179 - val_mse: 146.9026 - lr: 0.0100\n\nEpoch 138: LearningRateScheduler setting learning rate to 0.01.\nEpoch 138/200\n8/8 [==============================] - 0s 5ms/step - loss: 130.7139 - mae: 10.8604 - mse: 130.7139 - val_loss: 145.5886 - val_mae: 11.5602 - val_mse: 145.5886 - lr: 0.0100\n\nEpoch 139: LearningRateScheduler setting learning rate to 0.01.\nEpoch 139/200\n8/8 [==============================] - 0s 7ms/step - loss: 129.4027 - mae: 10.8007 - mse: 129.4027 - val_loss: 144.2332 - val_mae: 11.5015 - val_mse: 144.2332 - lr: 0.0100\n\nEpoch 140: LearningRateScheduler setting learning rate to 0.01.\nEpoch 140/200\n8/8 [==============================] - 0s 4ms/step - loss: 128.1482 - mae: 10.7431 - mse: 128.1482 - val_loss: 142.8506 - val_mae: 11.4422 - val_mse: 142.8506 - lr: 0.0100\n\nEpoch 141: LearningRateScheduler setting learning rate to 0.01.\nEpoch 141/200\n8/8 [==============================] - 0s 9ms/step - loss: 126.8983 - mae: 10.6849 - mse: 126.8983 - val_loss: 141.6004 - val_mae: 11.3856 - val_mse: 141.6004 - lr: 0.0100\n\nEpoch 142: LearningRateScheduler setting learning rate to 0.01.\nEpoch 142/200\n8/8 [==============================] - 0s 4ms/step - loss: 125.6490 - mae: 10.6263 - mse: 125.6490 - val_loss: 140.3364 - val_mae: 11.3292 - val_mse: 140.3364 - lr: 0.0100\n\nEpoch 143: LearningRateScheduler setting learning rate to 0.01.\nEpoch 143/200\n8/8 [==============================] - 0s 8ms/step - loss: 124.4374 - mae: 10.5693 - mse: 124.4374 - val_loss: 139.0836 - val_mae: 11.2725 - val_mse: 139.0836 - lr: 0.0100\n\nEpoch 144: LearningRateScheduler setting learning rate to 0.01.\nEpoch 144/200\n8/8 [==============================] - 0s 4ms/step - loss: 123.1577 - mae: 10.5100 - mse: 123.1577 - val_loss: 137.6913 - val_mae: 11.2125 - val_mse: 137.6913 - lr: 0.0100\n\nEpoch 145: LearningRateScheduler setting learning rate to 0.01.\nEpoch 145/200\n8/8 [==============================] - 0s 9ms/step - loss: 121.9743 - mae: 10.4544 - mse: 121.9743 - val_loss: 136.2596 - val_mae: 11.1514 - val_mse: 136.2596 - lr: 0.0100\n\nEpoch 146: LearningRateScheduler setting learning rate to 0.01.\nEpoch 146/200\n8/8 [==============================] - 0s 4ms/step - loss: 120.7187 - mae: 10.3958 - mse: 120.7187 - val_loss: 135.0254 - val_mae: 11.0949 - val_mse: 135.0254 - lr: 0.0100\n\nEpoch 147: LearningRateScheduler setting learning rate to 0.01.\nEpoch 147/200\n8/8 [==============================] - 0s 6ms/step - loss: 119.5161 - mae: 10.3377 - mse: 119.5161 - val_loss: 133.7414 - val_mae: 11.0371 - val_mse: 133.7414 - lr: 0.0100\n\nEpoch 148: LearningRateScheduler setting learning rate to 0.01.\nEpoch 148/200\n8/8 [==============================] - 0s 6ms/step - loss: 118.3284 - mae: 10.2819 - mse: 118.3284 - val_loss: 132.4063 - val_mae: 10.9780 - val_mse: 132.4063 - lr: 0.0100\n\nEpoch 149: LearningRateScheduler setting learning rate to 0.01.\nEpoch 149/200\n8/8 [==============================] - 0s 8ms/step - loss: 117.1797 - mae: 10.2270 - mse: 117.1797 - val_loss: 131.1173 - val_mae: 10.9201 - val_mse: 131.1173 - lr: 0.0100\n\nEpoch 150: LearningRateScheduler setting learning rate to 0.01.\nEpoch 150/200\n8/8 [==============================] - 0s 4ms/step - loss: 115.9835 - mae: 10.1684 - mse: 115.9835 - val_loss: 129.9031 - val_mae: 10.8636 - val_mse: 129.9031 - lr: 0.0100\n\nEpoch 151: LearningRateScheduler setting learning rate to 0.01.\nEpoch 151/200\n8/8 [==============================] - 0s 8ms/step - loss: 114.8048 - mae: 10.1121 - mse: 114.8048 - val_loss: 128.6843 - val_mae: 10.8073 - val_mse: 128.6843 - lr: 0.0100\n\nEpoch 152: LearningRateScheduler setting learning rate to 0.01.\nEpoch 152/200\n8/8 [==============================] - 0s 4ms/step - loss: 113.6788 - mae: 10.0555 - mse: 113.6788 - val_loss: 127.5350 - val_mae: 10.7523 - val_mse: 127.5350 - lr: 0.0100\n\nEpoch 153: LearningRateScheduler setting learning rate to 0.01.\nEpoch 153/200\n8/8 [==============================] - 0s 8ms/step - loss: 112.5362 - mae: 9.9989 - mse: 112.5362 - val_loss: 126.2217 - val_mae: 10.6930 - val_mse: 126.2217 - lr: 0.0100\n\nEpoch 154: LearningRateScheduler setting learning rate to 0.01.\nEpoch 154/200\n8/8 [==============================] - 0s 8ms/step - loss: 111.4001 - mae: 9.9429 - mse: 111.4001 - val_loss: 125.0445 - val_mae: 10.6375 - val_mse: 125.0445 - lr: 0.0100\n\nEpoch 155: LearningRateScheduler setting learning rate to 0.01.\nEpoch 155/200\n8/8 [==============================] - 0s 4ms/step - loss: 110.2619 - mae: 9.8867 - mse: 110.2619 - val_loss: 123.8217 - val_mae: 10.5806 - val_mse: 123.8217 - lr: 0.0100\n\nEpoch 156: LearningRateScheduler setting learning rate to 0.01.\nEpoch 156/200\n8/8 [==============================] - 0s 6ms/step - loss: 109.1727 - mae: 9.8315 - mse: 109.1727 - val_loss: 122.5753 - val_mae: 10.5227 - val_mse: 122.5753 - lr: 0.0100\n\nEpoch 157: LearningRateScheduler setting learning rate to 0.01.\nEpoch 157/200\n8/8 [==============================] - 0s 4ms/step - loss: 108.0638 - mae: 9.7764 - mse: 108.0638 - val_loss: 121.4131 - val_mae: 10.4673 - val_mse: 121.4131 - lr: 0.0100\n\nEpoch 158: LearningRateScheduler setting learning rate to 0.01.\nEpoch 158/200\n8/8 [==============================] - 0s 4ms/step - loss: 106.9381 - mae: 9.7201 - mse: 106.9381 - val_loss: 120.2431 - val_mae: 10.4115 - val_mse: 120.2431 - lr: 0.0100\n\nEpoch 159: LearningRateScheduler setting learning rate to 0.01.\nEpoch 159/200\n8/8 [==============================] - 0s 8ms/step - loss: 105.8789 - mae: 9.6655 - mse: 105.8789 - val_loss: 119.0993 - val_mae: 10.3560 - val_mse: 119.0993 - lr: 0.0100\n\nEpoch 160: LearningRateScheduler setting learning rate to 0.01.\nEpoch 160/200\n8/8 [==============================] - 0s 4ms/step - loss: 104.8040 - mae: 9.6099 - mse: 104.8040 - val_loss: 118.0017 - val_mae: 10.3017 - val_mse: 118.0017 - lr: 0.0100\n\nEpoch 161: LearningRateScheduler setting learning rate to 0.01.\nEpoch 161/200\n8/8 [==============================] - 0s 4ms/step - loss: 103.7300 - mae: 9.5545 - mse: 103.7300 - val_loss: 116.7785 - val_mae: 10.2441 - val_mse: 116.7785 - lr: 0.0100\n\nEpoch 162: LearningRateScheduler setting learning rate to 0.01.\nEpoch 162/200\n8/8 [==============================] - 0s 5ms/step - loss: 102.6942 - mae: 9.5010 - mse: 102.6942 - val_loss: 115.5248 - val_mae: 10.1854 - val_mse: 115.5248 - lr: 0.0100\n\nEpoch 163: LearningRateScheduler setting learning rate to 0.01.\nEpoch 163/200\n8/8 [==============================] - 0s 4ms/step - loss: 101.6350 - mae: 9.4462 - mse: 101.6350 - val_loss: 114.4430 - val_mae: 10.1317 - val_mse: 114.4430 - lr: 0.0100\n\nEpoch 164: LearningRateScheduler setting learning rate to 0.01.\nEpoch 164/200\n8/8 [==============================] - 0s 6ms/step - loss: 100.6196 - mae: 9.3927 - mse: 100.6196 - val_loss: 113.3884 - val_mae: 10.0781 - val_mse: 113.3884 - lr: 0.0100\n\nEpoch 165: LearningRateScheduler setting learning rate to 0.01.\nEpoch 165/200\n8/8 [==============================] - 0s 6ms/step - loss: 99.5716 - mae: 9.3386 - mse: 99.5716 - val_loss: 112.1806 - val_mae: 10.0206 - val_mse: 112.1806 - lr: 0.0100\n\nEpoch 166: LearningRateScheduler setting learning rate to 0.01.\nEpoch 166/200\n8/8 [==============================] - 0s 4ms/step - loss: 98.5620 - mae: 9.2853 - mse: 98.5620 - val_loss: 111.0424 - val_mae: 9.9649 - val_mse: 111.0424 - lr: 0.0100\n\nEpoch 167: LearningRateScheduler setting learning rate to 0.01.\nEpoch 167/200\n8/8 [==============================] - 0s 6ms/step - loss: 97.5199 - mae: 9.2305 - mse: 97.5199 - val_loss: 109.9399 - val_mae: 9.9098 - val_mse: 109.9399 - lr: 0.0100\n\nEpoch 168: LearningRateScheduler setting learning rate to 0.01.\nEpoch 168/200\n8/8 [==============================] - 0s 6ms/step - loss: 96.5443 - mae: 9.1781 - mse: 96.5443 - val_loss: 108.7947 - val_mae: 9.8537 - val_mse: 108.7947 - lr: 0.0100\n\nEpoch 169: LearningRateScheduler setting learning rate to 0.01.\nEpoch 169/200\n8/8 [==============================] - 0s 6ms/step - loss: 95.5334 - mae: 9.1247 - mse: 95.5334 - val_loss: 107.7411 - val_mae: 9.7999 - val_mse: 107.7411 - lr: 0.0100\n\nEpoch 170: LearningRateScheduler setting learning rate to 0.01.\nEpoch 170/200\n8/8 [==============================] - 0s 7ms/step - loss: 94.5599 - mae: 9.0713 - mse: 94.5599 - val_loss: 106.7465 - val_mae: 9.7476 - val_mse: 106.7465 - lr: 0.0100\n\nEpoch 171: LearningRateScheduler setting learning rate to 0.01.\nEpoch 171/200\n8/8 [==============================] - 0s 5ms/step - loss: 93.5586 - mae: 9.0171 - mse: 93.5586 - val_loss: 105.7015 - val_mae: 9.6940 - val_mse: 105.7015 - lr: 0.0100\n\nEpoch 172: LearningRateScheduler setting learning rate to 0.01.\nEpoch 172/200\n8/8 [==============================] - 0s 9ms/step - loss: 92.5750 - mae: 8.9642 - mse: 92.5750 - val_loss: 104.6274 - val_mae: 9.6397 - val_mse: 104.6274 - lr: 0.0100\n\nEpoch 173: LearningRateScheduler setting learning rate to 0.01.\nEpoch 173/200\n8/8 [==============================] - 0s 4ms/step - loss: 91.6443 - mae: 8.9131 - mse: 91.6443 - val_loss: 103.5181 - val_mae: 9.5842 - val_mse: 103.5181 - lr: 0.0100\n\nEpoch 174: LearningRateScheduler setting learning rate to 0.01.\nEpoch 174/200\n8/8 [==============================] - 0s 9ms/step - loss: 90.6913 - mae: 8.8615 - mse: 90.6913 - val_loss: 102.4625 - val_mae: 9.5301 - val_mse: 102.4625 - lr: 0.0100\n\nEpoch 175: LearningRateScheduler setting learning rate to 0.01.\nEpoch 175/200\n8/8 [==============================] - 0s 4ms/step - loss: 89.7408 - mae: 8.8086 - mse: 89.7408 - val_loss: 101.4726 - val_mae: 9.4774 - val_mse: 101.4726 - lr: 0.0100\n\nEpoch 176: LearningRateScheduler setting learning rate to 0.01.\nEpoch 176/200\n8/8 [==============================] - 0s 6ms/step - loss: 88.8105 - mae: 8.7561 - mse: 88.8105 - val_loss: 100.4530 - val_mae: 9.4237 - val_mse: 100.4530 - lr: 0.0100\n\nEpoch 177: LearningRateScheduler setting learning rate to 0.01.\nEpoch 177/200\n8/8 [==============================] - 0s 4ms/step - loss: 87.8935 - mae: 8.7042 - mse: 87.8935 - val_loss: 99.5041 - val_mae: 9.3720 - val_mse: 99.5041 - lr: 0.0100\n\nEpoch 178: LearningRateScheduler setting learning rate to 0.01.\nEpoch 178/200\n8/8 [==============================] - 0s 9ms/step - loss: 86.9575 - mae: 8.6514 - mse: 86.9575 - val_loss: 98.4878 - val_mae: 9.3185 - val_mse: 98.4878 - lr: 0.0100\n\nEpoch 179: LearningRateScheduler setting learning rate to 0.01.\nEpoch 179/200\n8/8 [==============================] - 0s 8ms/step - loss: 86.0522 - mae: 8.6009 - mse: 86.0522 - val_loss: 97.4979 - val_mae: 9.2657 - val_mse: 97.4979 - lr: 0.0100\n\nEpoch 180: LearningRateScheduler setting learning rate to 0.01.\nEpoch 180/200\n8/8 [==============================] - 0s 6ms/step - loss: 85.1599 - mae: 8.5502 - mse: 85.1599 - val_loss: 96.4987 - val_mae: 9.2125 - val_mse: 96.4987 - lr: 0.0100\n\nEpoch 181: LearningRateScheduler setting learning rate to 0.01.\nEpoch 181/200\n8/8 [==============================] - 0s 4ms/step - loss: 84.2766 - mae: 8.5001 - mse: 84.2766 - val_loss: 95.4773 - val_mae: 9.1584 - val_mse: 95.4773 - lr: 0.0100\n\nEpoch 182: LearningRateScheduler setting learning rate to 0.01.\nEpoch 182/200\n8/8 [==============================] - 0s 8ms/step - loss: 83.3686 - mae: 8.4480 - mse: 83.3686 - val_loss: 94.4988 - val_mae: 9.1056 - val_mse: 94.4988 - lr: 0.0100\n\nEpoch 183: LearningRateScheduler setting learning rate to 0.01.\nEpoch 183/200\n8/8 [==============================] - 0s 5ms/step - loss: 82.5042 - mae: 8.3977 - mse: 82.5042 - val_loss: 93.5729 - val_mae: 9.0542 - val_mse: 93.5729 - lr: 0.0100\n\nEpoch 184: LearningRateScheduler setting learning rate to 0.01.\nEpoch 184/200\n8/8 [==============================] - 0s 7ms/step - loss: 81.6645 - mae: 8.3484 - mse: 81.6645 - val_loss: 92.5801 - val_mae: 9.0011 - val_mse: 92.5801 - lr: 0.0100\n\nEpoch 185: LearningRateScheduler setting learning rate to 0.01.\nEpoch 185/200\n8/8 [==============================] - 0s 7ms/step - loss: 80.7705 - mae: 8.2974 - mse: 80.7705 - val_loss: 91.6460 - val_mae: 8.9493 - val_mse: 91.6460 - lr: 0.0100\n\nEpoch 186: LearningRateScheduler setting learning rate to 0.01.\nEpoch 186/200\n8/8 [==============================] - 0s 4ms/step - loss: 79.9255 - mae: 8.2473 - mse: 79.9255 - val_loss: 90.6949 - val_mae: 8.8969 - val_mse: 90.6949 - lr: 0.0100\n\nEpoch 187: LearningRateScheduler setting learning rate to 0.01.\nEpoch 187/200\n8/8 [==============================] - 0s 5ms/step - loss: 79.1043 - mae: 8.1984 - mse: 79.1043 - val_loss: 89.7967 - val_mae: 8.8463 - val_mse: 89.7967 - lr: 0.0100\n\nEpoch 188: LearningRateScheduler setting learning rate to 0.01.\nEpoch 188/200\n8/8 [==============================] - 0s 7ms/step - loss: 78.2436 - mae: 8.1478 - mse: 78.2436 - val_loss: 88.8952 - val_mae: 8.7951 - val_mse: 88.8952 - lr: 0.0100\n\nEpoch 189: LearningRateScheduler setting learning rate to 0.01.\nEpoch 189/200\n8/8 [==============================] - 0s 4ms/step - loss: 77.4343 - mae: 8.0987 - mse: 77.4343 - val_loss: 87.9436 - val_mae: 8.7424 - val_mse: 87.9436 - lr: 0.0100\n\nEpoch 190: LearningRateScheduler setting learning rate to 0.01.\nEpoch 190/200\n8/8 [==============================] - 0s 9ms/step - loss: 76.5948 - mae: 8.0479 - mse: 76.5948 - val_loss: 87.0530 - val_mae: 8.6915 - val_mse: 87.0530 - lr: 0.0100\n\nEpoch 191: LearningRateScheduler setting learning rate to 0.01.\nEpoch 191/200\n8/8 [==============================] - 0s 4ms/step - loss: 75.7868 - mae: 7.9989 - mse: 75.7868 - val_loss: 86.2159 - val_mae: 8.6423 - val_mse: 86.2159 - lr: 0.0100\n\nEpoch 192: LearningRateScheduler setting learning rate to 0.01.\nEpoch 192/200\n8/8 [==============================] - 0s 8ms/step - loss: 74.9966 - mae: 7.9488 - mse: 74.9966 - val_loss: 85.2732 - val_mae: 8.5894 - val_mse: 85.2732 - lr: 0.0100\n\nEpoch 193: LearningRateScheduler setting learning rate to 0.01.\nEpoch 193/200\n8/8 [==============================] - 0s 5ms/step - loss: 74.1869 - mae: 7.9000 - mse: 74.1869 - val_loss: 84.4380 - val_mae: 8.5398 - val_mse: 84.4380 - lr: 0.0100\n\nEpoch 194: LearningRateScheduler setting learning rate to 0.01.\nEpoch 194/200\n8/8 [==============================] - 0s 9ms/step - loss: 73.3927 - mae: 7.8507 - mse: 73.3927 - val_loss: 83.5521 - val_mae: 8.4889 - val_mse: 83.5521 - lr: 0.0100\n\nEpoch 195: LearningRateScheduler setting learning rate to 0.01.\nEpoch 195/200\n8/8 [==============================] - 0s 5ms/step - loss: 72.6021 - mae: 7.8016 - mse: 72.6021 - val_loss: 82.7206 - val_mae: 8.4393 - val_mse: 82.7206 - lr: 0.0100\n\nEpoch 196: LearningRateScheduler setting learning rate to 0.01.\nEpoch 196/200\n8/8 [==============================] - 0s 6ms/step - loss: 71.8296 - mae: 7.7528 - mse: 71.8296 - val_loss: 81.8456 - val_mae: 8.3887 - val_mse: 81.8456 - lr: 0.0100\n\nEpoch 197: LearningRateScheduler setting learning rate to 0.01.\nEpoch 197/200\n8/8 [==============================] - 0s 6ms/step - loss: 71.0907 - mae: 7.7056 - mse: 71.0907 - val_loss: 81.0777 - val_mae: 8.3408 - val_mse: 81.0777 - lr: 0.0100\n\nEpoch 198: LearningRateScheduler setting learning rate to 0.01.\nEpoch 198/200\n8/8 [==============================] - 0s 8ms/step - loss: 70.3495 - mae: 7.6585 - mse: 70.3495 - val_loss: 80.1270 - val_mae: 8.2877 - val_mse: 80.1270 - lr: 0.0100\n\nEpoch 199: LearningRateScheduler setting learning rate to 0.01.\nEpoch 199/200\n8/8 [==============================] - 0s 9ms/step - loss: 69.5663 - mae: 7.6091 - mse: 69.5663 - val_loss: 79.3024 - val_mae: 8.2378 - val_mse: 79.3024 - lr: 0.0100\n\nEpoch 200: LearningRateScheduler setting learning rate to 0.01.\nEpoch 200/200\n8/8 [==============================] - 0s 7ms/step - loss: 68.8060 - mae: 7.5602 - mse: 68.8060 - val_loss: 78.4901 - val_mae: 8.1885 - val_mse: 78.4901 - lr: 0.0100\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Custom Callback N.1\n\nLet's write a simple custom callback that logs the loss and metrics values after every batch, epoch, etc.","metadata":{"cell_id":"b78216242819454dba61dfcd21892cb9","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"class CustomLogger(keras.callbacks.Callback):\n    def on_train_begin(self, logs=None):\n        print(\"Starting training; log content: {}\".format(logs))\n\n    def on_train_end(self, logs=None):\n        print(\"Stop training; log content: {}\".format(logs))\n\n    def on_epoch_end(self, epoch, logs=None):\n        print(\"End epoch {} of training; log content: {}\".format(epoch, logs))\n\n    def on_train_batch_end(self, batch, logs=None):\n        print(\"...Training: end of batch {}; log content: {}\".format(batch, logs))\n","metadata":{"cell_id":"7ce275c361494caa87bbc061b187e106","source_hash":"f6673c52","execution_start":1667314330890,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":15},{"cell_type":"code","source":"log_callback = CustomLogger()\n\nmodel = get_model(train_features)\nhistory = model.fit(train_features, train_labels, epochs=200,\n                                verbose=0, #verbose=0 to avoid mixing our prints and the default ones of keras\n                                validation_split = 0.2,\n                                callbacks=[log_callback]\n)","metadata":{"cell_id":"22f33605d3c74211ab5d5f2af850b1ae","source_hash":"3f383a80","execution_start":1667314331681,"execution_millis":8873,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"...Training: end of batch 1; log content: {'loss': 11.813257217407227, 'mae': 2.599332809448242, 'mse': 11.813257217407227}\n...Training: end of batch 2; log content: {'loss': 10.3507719039917, 'mae': 2.438214063644409, 'mse': 10.3507719039917}\n...Training: end of batch 3; log content: {'loss': 11.39717960357666, 'mae': 2.5712409019470215, 'mse': 11.39717960357666}\n...Training: end of batch 4; log content: {'loss': 11.581306457519531, 'mae': 2.6131412982940674, 'mse': 11.581306457519531}\n...Training: end of batch 5; log content: {'loss': 11.448307037353516, 'mae': 2.5711774826049805, 'mse': 11.448307037353516}\n...Training: end of batch 6; log content: {'loss': 11.949694633483887, 'mae': 2.6010830402374268, 'mse': 11.949694633483887}\n...Training: end of batch 7; log content: {'loss': 11.411012649536133, 'mae': 2.556143045425415, 'mse': 11.411012649536133}\nEnd epoch 89 of training; log content: {'loss': 11.411012649536133, 'mae': 2.556143045425415, 'mse': 11.411012649536133, 'val_loss': 10.344679832458496, 'val_mae': 2.4609670639038086, 'val_mse': 10.344679832458496}\n...Training: end of batch 0; log content: {'loss': 5.829084873199463, 'mae': 2.0018808841705322, 'mse': 5.829084873199463}\n...Training: end of batch 1; log content: {'loss': 6.802611351013184, 'mae': 2.105628490447998, 'mse': 6.802611351013184}\n...Training: end of batch 2; log content: {'loss': 8.704754829406738, 'mae': 2.2246947288513184, 'mse': 8.704754829406738}\n...Training: end of batch 3; log content: {'loss': 10.365391731262207, 'mae': 2.4421544075012207, 'mse': 10.365391731262207}\n...Training: end of batch 4; log content: {'loss': 11.081934928894043, 'mae': 2.4695053100585938, 'mse': 11.081934928894043}\n...Training: end of batch 5; log content: {'loss': 12.220044136047363, 'mae': 2.633671522140503, 'mse': 12.220044136047363}\n...Training: end of batch 6; log content: {'loss': 11.990180015563965, 'mae': 2.631500482559204, 'mse': 11.990180015563965}\n...Training: end of batch 7; log content: {'loss': 11.458626747131348, 'mae': 2.558864116668701, 'mse': 11.458626747131348}\nEnd epoch 90 of training; log content: {'loss': 11.458626747131348, 'mae': 2.558864116668701, 'mse': 11.458626747131348, 'val_loss': 10.246285438537598, 'val_mae': 2.45176100730896, 'val_mse': 10.246285438537598}\n...Training: end of batch 0; log content: {'loss': 16.241432189941406, 'mae': 3.005105495452881, 'mse': 16.241432189941406}\n...Training: end of batch 1; log content: {'loss': 13.165115356445312, 'mae': 2.7687771320343018, 'mse': 13.165115356445312}\n...Training: end of batch 2; log content: {'loss': 12.013453483581543, 'mae': 2.620734930038452, 'mse': 12.013453483581543}\n...Training: end of batch 3; log content: {'loss': 10.804191589355469, 'mae': 2.4502480030059814, 'mse': 10.804191589355469}\n...Training: end of batch 4; log content: {'loss': 10.765295028686523, 'mae': 2.4889469146728516, 'mse': 10.765295028686523}\n...Training: end of batch 5; log content: {'loss': 10.707056999206543, 'mae': 2.457224130630493, 'mse': 10.707056999206543}\n...Training: end of batch 6; log content: {'loss': 11.32691478729248, 'mae': 2.536184549331665, 'mse': 11.32691478729248}\n...Training: end of batch 7; log content: {'loss': 11.384806632995605, 'mae': 2.557830572128296, 'mse': 11.384806632995605}\nEnd epoch 91 of training; log content: {'loss': 11.384806632995605, 'mae': 2.557830572128296, 'mse': 11.384806632995605, 'val_loss': 10.188315391540527, 'val_mae': 2.446516752243042, 'val_mse': 10.188315391540527}\n...Training: end of batch 0; log content: {'loss': 13.92277717590332, 'mae': 2.653743028640747, 'mse': 13.92277717590332}\n...Training: end of batch 1; log content: {'loss': 12.394721031188965, 'mae': 2.4602420330047607, 'mse': 12.394721031188965}\n...Training: end of batch 2; log content: {'loss': 13.121808052062988, 'mae': 2.724729537963867, 'mse': 13.121808052062988}\n...Training: end of batch 3; log content: {'loss': 12.109052658081055, 'mae': 2.591684103012085, 'mse': 12.109052658081055}\n...Training: end of batch 4; log content: {'loss': 12.035592079162598, 'mae': 2.5712554454803467, 'mse': 12.035592079162598}\n...Training: end of batch 5; log content: {'loss': 12.22702407836914, 'mae': 2.6616933345794678, 'mse': 12.22702407836914}\n...Training: end of batch 6; log content: {'loss': 11.272540092468262, 'mae': 2.5532593727111816, 'mse': 11.272540092468262}\n...Training: end of batch 7; log content: {'loss': 11.3869047164917, 'mae': 2.559117317199707, 'mse': 11.3869047164917}\nEnd epoch 92 of training; log content: {'loss': 11.3869047164917, 'mae': 2.559117317199707, 'mse': 11.3869047164917, 'val_loss': 10.198301315307617, 'val_mae': 2.4566447734832764, 'val_mse': 10.198301315307617}\n...Training: end of batch 0; log content: {'loss': 15.847311019897461, 'mae': 2.7498397827148438, 'mse': 15.847311019897461}\n...Training: end of batch 1; log content: {'loss': 13.25065803527832, 'mae': 2.6358084678649902, 'mse': 13.25065803527832}\n...Training: end of batch 2; log content: {'loss': 11.58331298828125, 'mae': 2.4668776988983154, 'mse': 11.58331298828125}\n...Training: end of batch 3; log content: {'loss': 11.725945472717285, 'mae': 2.5434956550598145, 'mse': 11.725945472717285}\n...Training: end of batch 4; log content: {'loss': 10.962457656860352, 'mae': 2.498292922973633, 'mse': 10.962457656860352}\n...Training: end of batch 5; log content: {'loss': 10.732813835144043, 'mae': 2.4678051471710205, 'mse': 10.732813835144043}\n...Training: end of batch 6; log content: {'loss': 11.008326530456543, 'mae': 2.511136293411255, 'mse': 11.008326530456543}\n...Training: end of batch 7; log content: {'loss': 11.365345001220703, 'mae': 2.5478265285491943, 'mse': 11.365345001220703}\nEnd epoch 93 of training; log content: {'loss': 11.365345001220703, 'mae': 2.5478265285491943, 'mse': 11.365345001220703, 'val_loss': 10.230493545532227, 'val_mae': 2.46327805519104, 'val_mse': 10.230493545532227}\n...Training: end of batch 0; log content: {'loss': 7.86958646774292, 'mae': 2.0629782676696777, 'mse': 7.86958646774292}\n...Training: end of batch 1; log content: {'loss': 8.507298469543457, 'mae': 2.176407814025879, 'mse': 8.507298469543457}\n...Training: end of batch 2; log content: {'loss': 12.87744140625, 'mae': 2.6341822147369385, 'mse': 12.87744140625}\n...Training: end of batch 3; log content: {'loss': 11.961030960083008, 'mae': 2.534693717956543, 'mse': 11.961030960083008}\n...Training: end of batch 4; log content: {'loss': 12.602258682250977, 'mae': 2.654106855392456, 'mse': 12.602258682250977}\n...Training: end of batch 5; log content: {'loss': 12.362595558166504, 'mae': 2.629887819290161, 'mse': 12.362595558166504}\n...Training: end of batch 6; log content: {'loss': 11.382172584533691, 'mae': 2.524932622909546, 'mse': 11.382172584533691}\n...Training: end of batch 7; log content: {'loss': 11.39663314819336, 'mae': 2.5548832416534424, 'mse': 11.39663314819336}\nEnd epoch 94 of training; log content: {'loss': 11.39663314819336, 'mae': 2.5548832416534424, 'mse': 11.39663314819336, 'val_loss': 10.213971138000488, 'val_mae': 2.4479281902313232, 'val_mse': 10.213971138000488}\n...Training: end of batch 0; log content: {'loss': 16.555191040039062, 'mae': 3.0319271087646484, 'mse': 16.555191040039062}\n...Training: end of batch 1; log content: {'loss': 13.454151153564453, 'mae': 2.811021089553833, 'mse': 13.454151153564453}\n...Training: end of batch 2; log content: {'loss': 13.592170715332031, 'mae': 2.7667179107666016, 'mse': 13.592170715332031}\n...Training: end of batch 3; log content: {'loss': 13.703097343444824, 'mae': 2.8239028453826904, 'mse': 13.703097343444824}\n...Training: end of batch 4; log content: {'loss': 12.810052871704102, 'mae': 2.712245225906372, 'mse': 12.810052871704102}\n...Training: end of batch 5; log content: {'loss': 12.385601997375488, 'mae': 2.6567232608795166, 'mse': 12.385601997375488}\n...Training: end of batch 6; log content: {'loss': 11.973715782165527, 'mae': 2.628694772720337, 'mse': 11.973715782165527}\n...Training: end of batch 7; log content: {'loss': 11.393451690673828, 'mae': 2.5646812915802, 'mse': 11.393451690673828}\nEnd epoch 95 of training; log content: {'loss': 11.393451690673828, 'mae': 2.5646812915802, 'mse': 11.393451690673828, 'val_loss': 10.166999816894531, 'val_mae': 2.4394142627716064, 'val_mse': 10.166999816894531}\n...Training: end of batch 0; log content: {'loss': 14.159130096435547, 'mae': 2.9177870750427246, 'mse': 14.159130096435547}\n...Training: end of batch 1; log content: {'loss': 13.016680717468262, 'mae': 2.8555140495300293, 'mse': 13.016680717468262}\n...Training: end of batch 2; log content: {'loss': 11.278149604797363, 'mae': 2.659771203994751, 'mse': 11.278149604797363}\n...Training: end of batch 3; log content: {'loss': 11.236860275268555, 'mae': 2.673867702484131, 'mse': 11.236860275268555}\n...Training: end of batch 4; log content: {'loss': 10.890917778015137, 'mae': 2.592132568359375, 'mse': 10.890917778015137}\n...Training: end of batch 5; log content: {'loss': 10.332097053527832, 'mae': 2.510981798171997, 'mse': 10.332097053527832}\n...Training: end of batch 6; log content: {'loss': 11.920334815979004, 'mae': 2.627258777618408, 'mse': 11.920334815979004}\n...Training: end of batch 7; log content: {'loss': 11.388103485107422, 'mae': 2.5525834560394287, 'mse': 11.388103485107422}\nEnd epoch 96 of training; log content: {'loss': 11.388103485107422, 'mae': 2.5525834560394287, 'mse': 11.388103485107422, 'val_loss': 10.220792770385742, 'val_mae': 2.455967903137207, 'val_mse': 10.220792770385742}\n...Training: end of batch 0; log content: {'loss': 6.819429397583008, 'mae': 1.8313984870910645, 'mse': 6.819429397583008}\n...Training: end of batch 1; log content: {'loss': 13.331622123718262, 'mae': 2.5665884017944336, 'mse': 13.331622123718262}\n...Training: end of batch 2; log content: {'loss': 13.080862998962402, 'mae': 2.630805015563965, 'mse': 13.080862998962402}\n...Training: end of batch 3; log content: {'loss': 12.084477424621582, 'mae': 2.605501890182495, 'mse': 12.084477424621582}\n...Training: end of batch 4; log content: {'loss': 11.433442115783691, 'mae': 2.5433437824249268, 'mse': 11.433442115783691}\n...Training: end of batch 5; log content: {'loss': 11.708394050598145, 'mae': 2.5679919719696045, 'mse': 11.708394050598145}\n...Training: end of batch 6; log content: {'loss': 11.41690731048584, 'mae': 2.5671043395996094, 'mse': 11.41690731048584}\n...Training: end of batch 7; log content: {'loss': 11.357760429382324, 'mae': 2.5529229640960693, 'mse': 11.357760429382324}\nEnd epoch 97 of training; log content: {'loss': 11.357760429382324, 'mae': 2.5529229640960693, 'mse': 11.357760429382324, 'val_loss': 10.18663501739502, 'val_mae': 2.453791618347168, 'val_mse': 10.18663501739502}\n...Training: end of batch 0; log content: {'loss': 13.241065979003906, 'mae': 3.0519328117370605, 'mse': 13.241065979003906}\n...Training: end of batch 1; log content: {'loss': 17.885330200195312, 'mae': 3.171473503112793, 'mse': 17.885330200195312}\n...Training: end of batch 2; log content: {'loss': 16.272220611572266, 'mae': 3.02204966545105, 'mse': 16.272220611572266}\n...Training: end of batch 3; log content: {'loss': 14.91535758972168, 'mae': 2.9308252334594727, 'mse': 14.91535758972168}\n...Training: end of batch 4; log content: {'loss': 13.245346069335938, 'mae': 2.7632274627685547, 'mse': 13.245346069335938}\n...Training: end of batch 5; log content: {'loss': 12.215858459472656, 'mae': 2.654099464416504, 'mse': 12.215858459472656}\n...Training: end of batch 6; log content: {'loss': 11.376599311828613, 'mae': 2.560316562652588, 'mse': 11.376599311828613}\n...Training: end of batch 7; log content: {'loss': 11.41595458984375, 'mae': 2.5786848068237305, 'mse': 11.41595458984375}\nEnd epoch 98 of training; log content: {'loss': 11.41595458984375, 'mae': 2.5786848068237305, 'mse': 11.41595458984375, 'val_loss': 10.155782699584961, 'val_mae': 2.4460413455963135, 'val_mse': 10.155782699584961}\n...Training: end of batch 0; log content: {'loss': 13.72900390625, 'mae': 2.9408178329467773, 'mse': 13.72900390625}\n...Training: end of batch 1; log content: {'loss': 12.976531028747559, 'mae': 2.783895492553711, 'mse': 12.976531028747559}\n...Training: end of batch 2; log content: {'loss': 12.216254234313965, 'mae': 2.801898241043091, 'mse': 12.216254234313965}\n...Training: end of batch 3; log content: {'loss': 12.125456809997559, 'mae': 2.753929853439331, 'mse': 12.125456809997559}\n...Training: end of batch 4; log content: {'loss': 11.365187644958496, 'mae': 2.634265661239624, 'mse': 11.365187644958496}\n...Training: end of batch 5; log content: {'loss': 11.081971168518066, 'mae': 2.6059350967407227, 'mse': 11.081971168518066}\n...Training: end of batch 6; log content: {'loss': 10.75739574432373, 'mae': 2.565044641494751, 'mse': 10.75739574432373}\n...Training: end of batch 7; log content: {'loss': 11.40570068359375, 'mae': 2.5738911628723145, 'mse': 11.40570068359375}\nEnd epoch 99 of training; log content: {'loss': 11.40570068359375, 'mae': 2.5738911628723145, 'mse': 11.40570068359375, 'val_loss': 10.292757034301758, 'val_mae': 2.461467981338501, 'val_mse': 10.292757034301758}\n...Training: end of batch 0; log content: {'loss': 11.345582962036133, 'mae': 2.52435040473938, 'mse': 11.345582962036133}\n...Training: end of batch 1; log content: {'loss': 10.048413276672363, 'mae': 2.5062575340270996, 'mse': 10.048413276672363}\n...Training: end of batch 2; log content: {'loss': 10.598326683044434, 'mae': 2.56622576713562, 'mse': 10.598326683044434}\n...Training: end of batch 3; log content: {'loss': 9.85715103149414, 'mae': 2.421630859375, 'mse': 9.85715103149414}\n...Training: end of batch 4; log content: {'loss': 10.416696548461914, 'mae': 2.4913227558135986, 'mse': 10.416696548461914}\n...Training: end of batch 5; log content: {'loss': 11.528990745544434, 'mae': 2.5462584495544434, 'mse': 11.528990745544434}\n...Training: end of batch 6; log content: {'loss': 11.315673828125, 'mae': 2.5341250896453857, 'mse': 11.315673828125}\n...Training: end of batch 7; log content: {'loss': 11.362156867980957, 'mae': 2.5655860900878906, 'mse': 11.362156867980957}\nEnd epoch 100 of training; log content: {'loss': 11.362156867980957, 'mae': 2.5655860900878906, 'mse': 11.362156867980957, 'val_loss': 10.19926643371582, 'val_mae': 2.4462928771972656, 'val_mse': 10.19926643371582}\n...Training: end of batch 0; log content: {'loss': 11.046215057373047, 'mae': 2.4504661560058594, 'mse': 11.046215057373047}\n...Training: end of batch 1; log content: {'loss': 9.93951416015625, 'mae': 2.385214328765869, 'mse': 9.93951416015625}\n...Training: end of batch 2; log content: {'loss': 9.370701789855957, 'mae': 2.3279876708984375, 'mse': 9.370701789855957}\n...Training: end of batch 3; log content: {'loss': 9.58503532409668, 'mae': 2.3734216690063477, 'mse': 9.58503532409668}\n...Training: end of batch 4; log content: {'loss': 10.310552597045898, 'mae': 2.4627561569213867, 'mse': 10.310552597045898}\n...Training: end of batch 5; log content: {'loss': 10.932827949523926, 'mae': 2.5284388065338135, 'mse': 10.932827949523926}\n...Training: end of batch 6; log content: {'loss': 10.707536697387695, 'mae': 2.5036025047302246, 'mse': 10.707536697387695}\n...Training: end of batch 7; log content: {'loss': 11.393692016601562, 'mae': 2.5651533603668213, 'mse': 11.393692016601562}\nEnd epoch 101 of training; log content: {'loss': 11.393692016601562, 'mae': 2.5651533603668213, 'mse': 11.393692016601562, 'val_loss': 10.06765365600586, 'val_mae': 2.4416754245758057, 'val_mse': 10.06765365600586}\n...Training: end of batch 0; log content: {'loss': 14.945655822753906, 'mae': 2.7875657081604004, 'mse': 14.945655822753906}\n...Training: end of batch 1; log content: {'loss': 11.301509857177734, 'mae': 2.4369282722473145, 'mse': 11.301509857177734}\n...Training: end of batch 2; log content: {'loss': 11.692002296447754, 'mae': 2.508788585662842, 'mse': 11.692002296447754}\n...Training: end of batch 3; log content: {'loss': 11.467473983764648, 'mae': 2.5444979667663574, 'mse': 11.467473983764648}\n...Training: end of batch 4; log content: {'loss': 12.040891647338867, 'mae': 2.619939088821411, 'mse': 12.040891647338867}\n...Training: end of batch 5; log content: {'loss': 11.828431129455566, 'mae': 2.6247398853302, 'mse': 11.828431129455566}\n...Training: end of batch 6; log content: {'loss': 11.535201072692871, 'mae': 2.58219313621521, 'mse': 11.535201072692871}\n...Training: end of batch 7; log content: {'loss': 11.336495399475098, 'mae': 2.5534706115722656, 'mse': 11.336495399475098}\nEnd epoch 102 of training; log content: {'loss': 11.336495399475098, 'mae': 2.5534706115722656, 'mse': 11.336495399475098, 'val_loss': 10.107710838317871, 'val_mae': 2.444115161895752, 'val_mse': 10.107710838317871}\n...Training: end of batch 0; log content: {'loss': 12.994144439697266, 'mae': 2.731433868408203, 'mse': 12.994144439697266}\n...Training: end of batch 1; log content: {'loss': 13.110098838806152, 'mae': 2.8525946140289307, 'mse': 13.110098838806152}\n...Training: end of batch 2; log content: {'loss': 14.584793090820312, 'mae': 2.9027976989746094, 'mse': 14.584793090820312}\n...Training: end of batch 3; log content: {'loss': 14.006566047668457, 'mae': 2.89473295211792, 'mse': 14.006566047668457}\n...Training: end of batch 4; log content: {'loss': 13.608736991882324, 'mae': 2.832014799118042, 'mse': 13.608736991882324}\n...Training: end of batch 5; log content: {'loss': 12.69550609588623, 'mae': 2.7348625659942627, 'mse': 12.69550609588623}\n...Training: end of batch 6; log content: {'loss': 11.928667068481445, 'mae': 2.6244466304779053, 'mse': 11.928667068481445}\n...Training: end of batch 7; log content: {'loss': 11.334880828857422, 'mae': 2.5566976070404053, 'mse': 11.334880828857422}\nEnd epoch 103 of training; log content: {'loss': 11.334880828857422, 'mae': 2.5566976070404053, 'mse': 11.334880828857422, 'val_loss': 10.159212112426758, 'val_mae': 2.445801258087158, 'val_mse': 10.159212112426758}\n...Training: end of batch 0; log content: {'loss': 7.096700668334961, 'mae': 2.1138792037963867, 'mse': 7.096700668334961}\n...Training: end of batch 1; log content: {'loss': 8.044078826904297, 'mae': 2.1967170238494873, 'mse': 8.044078826904297}\n...Training: end of batch 2; log content: {'loss': 9.042790412902832, 'mae': 2.2942330837249756, 'mse': 9.042790412902832}\n...Training: end of batch 3; log content: {'loss': 11.36355209350586, 'mae': 2.5193753242492676, 'mse': 11.36355209350586}\n...Training: end of batch 4; log content: {'loss': 10.623075485229492, 'mae': 2.485900402069092, 'mse': 10.623075485229492}\n...Training: end of batch 5; log content: {'loss': 10.547759056091309, 'mae': 2.4882161617279053, 'mse': 10.547759056091309}\n...Training: end of batch 6; log content: {'loss': 11.591227531433105, 'mae': 2.546416997909546, 'mse': 11.591227531433105}\n...Training: end of batch 7; log content: {'loss': 11.383397102355957, 'mae': 2.558767557144165, 'mse': 11.383397102355957}\nEnd epoch 104 of training; log content: {'loss': 11.383397102355957, 'mae': 2.558767557144165, 'mse': 11.383397102355957, 'val_loss': 10.18454360961914, 'val_mae': 2.4565346240997314, 'val_mse': 10.18454360961914}\n...Training: end of batch 0; log content: {'loss': 9.57574462890625, 'mae': 2.540285587310791, 'mse': 9.57574462890625}\n...Training: end of batch 1; log content: {'loss': 16.967304229736328, 'mae': 3.2366034984588623, 'mse': 16.967304229736328}\n...Training: end of batch 2; log content: {'loss': 16.2000675201416, 'mae': 3.1664321422576904, 'mse': 16.2000675201416}\n...Training: end of batch 3; log content: {'loss': 15.567098617553711, 'mae': 3.0527210235595703, 'mse': 15.567098617553711}\n...Training: end of batch 4; log content: {'loss': 13.958781242370605, 'mae': 2.843507766723633, 'mse': 13.958781242370605}\n...Training: end of batch 5; log content: {'loss': 13.13111400604248, 'mae': 2.7603061199188232, 'mse': 13.13111400604248}\n...Training: end of batch 6; log content: {'loss': 12.14671802520752, 'mae': 2.6673147678375244, 'mse': 12.14671802520752}\n...Training: end of batch 7; log content: {'loss': 11.375665664672852, 'mae': 2.562389612197876, 'mse': 11.375665664672852}\nEnd epoch 105 of training; log content: {'loss': 11.375665664672852, 'mae': 2.562389612197876, 'mse': 11.375665664672852, 'val_loss': 10.137248992919922, 'val_mae': 2.4348762035369873, 'val_mse': 10.137248992919922}\n...Training: end of batch 0; log content: {'loss': 18.568706512451172, 'mae': 3.5566320419311523, 'mse': 18.568706512451172}\n...Training: end of batch 1; log content: {'loss': 13.185453414916992, 'mae': 2.8327174186706543, 'mse': 13.185453414916992}\n...Training: end of batch 2; log content: {'loss': 10.57186222076416, 'mae': 2.4537622928619385, 'mse': 10.57186222076416}\n...Training: end of batch 3; log content: {'loss': 10.807485580444336, 'mae': 2.462599754333496, 'mse': 10.807485580444336}\n...Training: end of batch 4; log content: {'loss': 10.701016426086426, 'mae': 2.4355363845825195, 'mse': 10.701016426086426}\n...Training: end of batch 5; log content: {'loss': 10.44873332977295, 'mae': 2.464489221572876, 'mse': 10.44873332977295}\n...Training: end of batch 6; log content: {'loss': 11.219731330871582, 'mae': 2.5270462036132812, 'mse': 11.219731330871582}\n...Training: end of batch 7; log content: {'loss': 11.346853256225586, 'mae': 2.5675787925720215, 'mse': 11.346853256225586}\nEnd epoch 106 of training; log content: {'loss': 11.346853256225586, 'mae': 2.5675787925720215, 'mse': 11.346853256225586, 'val_loss': 10.190861701965332, 'val_mae': 2.4486422538757324, 'val_mse': 10.190861701965332}\n...Training: end of batch 0; log content: {'loss': 17.173715591430664, 'mae': 3.0746257305145264, 'mse': 17.173715591430664}\n...Training: end of batch 1; log content: {'loss': 12.136297225952148, 'mae': 2.5712924003601074, 'mse': 12.136297225952148}\n...Training: end of batch 2; log content: {'loss': 12.75803279876709, 'mae': 2.7037353515625, 'mse': 12.75803279876709}\n...Training: end of batch 3; log content: {'loss': 13.162001609802246, 'mae': 2.7713136672973633, 'mse': 13.162001609802246}\n...Training: end of batch 4; log content: {'loss': 13.282995223999023, 'mae': 2.8355369567871094, 'mse': 13.282995223999023}\n...Training: end of batch 5; log content: {'loss': 11.909980773925781, 'mae': 2.6170942783355713, 'mse': 11.909980773925781}\n...Training: end of batch 6; log content: {'loss': 11.664441108703613, 'mae': 2.587697744369507, 'mse': 11.664441108703613}\n...Training: end of batch 7; log content: {'loss': 11.395360946655273, 'mae': 2.567716360092163, 'mse': 11.395360946655273}\nEnd epoch 107 of training; log content: {'loss': 11.395360946655273, 'mae': 2.567716360092163, 'mse': 11.395360946655273, 'val_loss': 10.199459075927734, 'val_mae': 2.4451956748962402, 'val_mse': 10.199459075927734}\n...Training: end of batch 0; log content: {'loss': 9.911324501037598, 'mae': 2.5462660789489746, 'mse': 9.911324501037598}\n...Training: end of batch 1; log content: {'loss': 11.360404968261719, 'mae': 2.3851845264434814, 'mse': 11.360404968261719}\n...Training: end of batch 2; log content: {'loss': 10.029194831848145, 'mae': 2.331136465072632, 'mse': 10.029194831848145}\n...Training: end of batch 3; log content: {'loss': 9.613546371459961, 'mae': 2.322589874267578, 'mse': 9.613546371459961}\n...Training: end of batch 4; log content: {'loss': 9.50141716003418, 'mae': 2.367271900177002, 'mse': 9.50141716003418}\n...Training: end of batch 5; log content: {'loss': 9.276690483093262, 'mae': 2.3390395641326904, 'mse': 9.276690483093262}\n...Training: end of batch 6; log content: {'loss': 10.362058639526367, 'mae': 2.478896379470825, 'mse': 10.362058639526367}\n...Training: end of batch 7; log content: {'loss': 11.347766876220703, 'mae': 2.5563318729400635, 'mse': 11.347766876220703}\nEnd epoch 108 of training; log content: {'loss': 11.347766876220703, 'mae': 2.5563318729400635, 'mse': 11.347766876220703, 'val_loss': 10.141092300415039, 'val_mae': 2.45527720451355, 'val_mse': 10.141092300415039}\n...Training: end of batch 0; log content: {'loss': 12.248978614807129, 'mae': 2.7644169330596924, 'mse': 12.248978614807129}\n...Training: end of batch 1; log content: {'loss': 13.67352294921875, 'mae': 2.8431992530822754, 'mse': 13.67352294921875}\n...Training: end of batch 2; log content: {'loss': 12.772205352783203, 'mae': 2.797437906265259, 'mse': 12.772205352783203}\n...Training: end of batch 3; log content: {'loss': 12.072539329528809, 'mae': 2.688532590866089, 'mse': 12.072539329528809}\n...Training: end of batch 4; log content: {'loss': 12.24400520324707, 'mae': 2.72802734375, 'mse': 12.24400520324707}\n...Training: end of batch 5; log content: {'loss': 10.882797241210938, 'mae': 2.515502452850342, 'mse': 10.882797241210938}\n...Training: end of batch 6; log content: {'loss': 11.989109992980957, 'mae': 2.6216213703155518, 'mse': 11.989109992980957}\n...Training: end of batch 7; log content: {'loss': 11.401918411254883, 'mae': 2.568349599838257, 'mse': 11.401918411254883}\nEnd epoch 109 of training; log content: {'loss': 11.401918411254883, 'mae': 2.568349599838257, 'mse': 11.401918411254883, 'val_loss': 10.063939094543457, 'val_mae': 2.4385151863098145, 'val_mse': 10.063939094543457}\n...Training: end of batch 0; log content: {'loss': 11.63911247253418, 'mae': 2.673753261566162, 'mse': 11.63911247253418}\n...Training: end of batch 1; log content: {'loss': 12.868400573730469, 'mae': 2.8160834312438965, 'mse': 12.868400573730469}\n...Training: end of batch 2; log content: {'loss': 11.609291076660156, 'mae': 2.6764450073242188, 'mse': 11.609291076660156}\n...Training: end of batch 3; log content: {'loss': 10.826730728149414, 'mae': 2.635899543762207, 'mse': 10.826730728149414}\n...Training: end of batch 4; log content: {'loss': 10.907060623168945, 'mae': 2.572192668914795, 'mse': 10.907060623168945}\n...Training: end of batch 5; log content: {'loss': 11.011322021484375, 'mae': 2.5431606769561768, 'mse': 11.011322021484375}\n...Training: end of batch 6; log content: {'loss': 11.508238792419434, 'mae': 2.580548048019409, 'mse': 11.508238792419434}\n...Training: end of batch 7; log content: {'loss': 11.339780807495117, 'mae': 2.566694736480713, 'mse': 11.339780807495117}\nEnd epoch 110 of training; log content: {'loss': 11.339780807495117, 'mae': 2.566694736480713, 'mse': 11.339780807495117, 'val_loss': 10.128945350646973, 'val_mae': 2.4535233974456787, 'val_mse': 10.128945350646973}\n...Training: end of batch 0; log content: {'loss': 10.722748756408691, 'mae': 2.612588405609131, 'mse': 10.722748756408691}\n...Training: end of batch 1; log content: {'loss': 9.8619966506958, 'mae': 2.4036037921905518, 'mse': 9.8619966506958}\n...Training: end of batch 2; log content: {'loss': 11.566081047058105, 'mae': 2.578601121902466, 'mse': 11.566081047058105}\n...Training: end of batch 3; log content: {'loss': 11.725831985473633, 'mae': 2.563373565673828, 'mse': 11.725831985473633}\n...Training: end of batch 4; log content: {'loss': 11.711898803710938, 'mae': 2.577920913696289, 'mse': 11.711898803710938}\n...Training: end of batch 5; log content: {'loss': 11.787997245788574, 'mae': 2.5864622592926025, 'mse': 11.787997245788574}\n...Training: end of batch 6; log content: {'loss': 11.5535306930542, 'mae': 2.597086191177368, 'mse': 11.5535306930542}\n...Training: end of batch 7; log content: {'loss': 11.329583168029785, 'mae': 2.562748908996582, 'mse': 11.329583168029785}\nEnd epoch 111 of training; log content: {'loss': 11.329583168029785, 'mae': 2.562748908996582, 'mse': 11.329583168029785, 'val_loss': 10.065966606140137, 'val_mae': 2.4485740661621094, 'val_mse': 10.065966606140137}\n...Training: end of batch 0; log content: {'loss': 10.816021919250488, 'mae': 2.3748903274536133, 'mse': 10.816021919250488}\n...Training: end of batch 1; log content: {'loss': 11.490732192993164, 'mae': 2.5366151332855225, 'mse': 11.490732192993164}\n...Training: end of batch 2; log content: {'loss': 10.527337074279785, 'mae': 2.4867305755615234, 'mse': 10.527337074279785}\n...Training: end of batch 3; log content: {'loss': 9.943225860595703, 'mae': 2.4309921264648438, 'mse': 9.943225860595703}\n...Training: end of batch 4; log content: {'loss': 10.271941184997559, 'mae': 2.4472270011901855, 'mse': 10.271941184997559}\n...Training: end of batch 5; log content: {'loss': 11.361923217773438, 'mae': 2.5404675006866455, 'mse': 11.361923217773438}\n...Training: end of batch 6; log content: {'loss': 11.261702537536621, 'mae': 2.541978597640991, 'mse': 11.261702537536621}\n...Training: end of batch 7; log content: {'loss': 11.352581977844238, 'mae': 2.5629217624664307, 'mse': 11.352581977844238}\nEnd epoch 112 of training; log content: {'loss': 11.352581977844238, 'mae': 2.5629217624664307, 'mse': 11.352581977844238, 'val_loss': 10.030189514160156, 'val_mae': 2.4407899379730225, 'val_mse': 10.030189514160156}\n...Training: end of batch 0; log content: {'loss': 10.311429977416992, 'mae': 2.46486759185791, 'mse': 10.311429977416992}\n...Training: end of batch 1; log content: {'loss': 11.045645713806152, 'mae': 2.485382556915283, 'mse': 11.045645713806152}\n...Training: end of batch 2; log content: {'loss': 10.913002014160156, 'mae': 2.4754045009613037, 'mse': 10.913002014160156}\n...Training: end of batch 3; log content: {'loss': 11.964274406433105, 'mae': 2.662024736404419, 'mse': 11.964274406433105}\n...Training: end of batch 4; log content: {'loss': 11.58013916015625, 'mae': 2.6209468841552734, 'mse': 11.58013916015625}\n...Training: end of batch 5; log content: {'loss': 10.888439178466797, 'mae': 2.550243854522705, 'mse': 10.888439178466797}\n...Training: end of batch 6; log content: {'loss': 11.357991218566895, 'mae': 2.5633552074432373, 'mse': 11.357991218566895}\n...Training: end of batch 7; log content: {'loss': 11.366365432739258, 'mae': 2.5671777725219727, 'mse': 11.366365432739258}\nEnd epoch 113 of training; log content: {'loss': 11.366365432739258, 'mae': 2.5671777725219727, 'mse': 11.366365432739258, 'val_loss': 10.161966323852539, 'val_mae': 2.443615674972534, 'val_mse': 10.161966323852539}\n...Training: end of batch 0; log content: {'loss': 12.887624740600586, 'mae': 2.7632288932800293, 'mse': 12.887624740600586}\n...Training: end of batch 1; log content: {'loss': 12.684883117675781, 'mae': 2.7905995845794678, 'mse': 12.684883117675781}\n...Training: end of batch 2; log content: {'loss': 11.113251686096191, 'mae': 2.6382453441619873, 'mse': 11.113251686096191}\n...Training: end of batch 3; log content: {'loss': 12.013099670410156, 'mae': 2.6832025051116943, 'mse': 12.013099670410156}\n...Training: end of batch 4; log content: {'loss': 10.581595420837402, 'mae': 2.505032777786255, 'mse': 10.581595420837402}\n...Training: end of batch 5; log content: {'loss': 10.537657737731934, 'mae': 2.4730770587921143, 'mse': 10.537657737731934}\n...Training: end of batch 6; log content: {'loss': 11.34605598449707, 'mae': 2.556469678878784, 'mse': 11.34605598449707}\n...Training: end of batch 7; log content: {'loss': 11.335036277770996, 'mae': 2.5508415699005127, 'mse': 11.335036277770996}\nEnd epoch 114 of training; log content: {'loss': 11.335036277770996, 'mae': 2.5508415699005127, 'mse': 11.335036277770996, 'val_loss': 10.266487121582031, 'val_mae': 2.461594820022583, 'val_mse': 10.266487121582031}\n...Training: end of batch 0; log content: {'loss': 16.92310333251953, 'mae': 2.940187692642212, 'mse': 16.92310333251953}\n...Training: end of batch 1; log content: {'loss': 14.840778350830078, 'mae': 2.790004253387451, 'mse': 14.840778350830078}\n...Training: end of batch 2; log content: {'loss': 12.10074520111084, 'mae': 2.562220811843872, 'mse': 12.10074520111084}\n...Training: end of batch 3; log content: {'loss': 11.24549388885498, 'mae': 2.496549367904663, 'mse': 11.24549388885498}\n...Training: end of batch 4; log content: {'loss': 10.749765396118164, 'mae': 2.433109760284424, 'mse': 10.749765396118164}\n...Training: end of batch 5; log content: {'loss': 11.457565307617188, 'mae': 2.5352084636688232, 'mse': 11.457565307617188}\n...Training: end of batch 6; log content: {'loss': 10.892690658569336, 'mae': 2.484180212020874, 'mse': 10.892690658569336}\n...Training: end of batch 7; log content: {'loss': 11.342615127563477, 'mae': 2.5586953163146973, 'mse': 11.342615127563477}\nEnd epoch 115 of training; log content: {'loss': 11.342615127563477, 'mae': 2.5586953163146973, 'mse': 11.342615127563477, 'val_loss': 10.149032592773438, 'val_mae': 2.4406931400299072, 'val_mse': 10.149032592773438}\n...Training: end of batch 0; log content: {'loss': 12.487238883972168, 'mae': 2.61177921295166, 'mse': 12.487238883972168}\n...Training: end of batch 1; log content: {'loss': 10.258237838745117, 'mae': 2.469453811645508, 'mse': 10.258237838745117}\n...Training: end of batch 2; log content: {'loss': 11.839550018310547, 'mae': 2.5486135482788086, 'mse': 11.839550018310547}\n...Training: end of batch 3; log content: {'loss': 10.825559616088867, 'mae': 2.483798027038574, 'mse': 10.825559616088867}\n...Training: end of batch 4; log content: {'loss': 11.415227890014648, 'mae': 2.578641414642334, 'mse': 11.415227890014648}\n...Training: end of batch 5; log content: {'loss': 10.977076530456543, 'mae': 2.539721727371216, 'mse': 10.977076530456543}\n...Training: end of batch 6; log content: {'loss': 11.022682189941406, 'mae': 2.5484519004821777, 'mse': 11.022682189941406}\n...Training: end of batch 7; log content: {'loss': 11.31644344329834, 'mae': 2.5689384937286377, 'mse': 11.31644344329834}\nEnd epoch 116 of training; log content: {'loss': 11.31644344329834, 'mae': 2.5689384937286377, 'mse': 11.31644344329834, 'val_loss': 10.051390647888184, 'val_mae': 2.4366252422332764, 'val_mse': 10.051390647888184}\n...Training: end of batch 0; log content: {'loss': 8.462357521057129, 'mae': 2.468533515930176, 'mse': 8.462357521057129}\n...Training: end of batch 1; log content: {'loss': 10.103921890258789, 'mae': 2.549877882003784, 'mse': 10.103921890258789}\n...Training: end of batch 2; log content: {'loss': 10.626591682434082, 'mae': 2.560453176498413, 'mse': 10.626591682434082}\n...Training: end of batch 3; log content: {'loss': 10.939714431762695, 'mae': 2.6307692527770996, 'mse': 10.939714431762695}\n...Training: end of batch 4; log content: {'loss': 10.690420150756836, 'mae': 2.6077072620391846, 'mse': 10.690420150756836}\n...Training: end of batch 5; log content: {'loss': 10.958895683288574, 'mae': 2.5581867694854736, 'mse': 10.958895683288574}\n...Training: end of batch 6; log content: {'loss': 11.693222999572754, 'mae': 2.620962619781494, 'mse': 11.693222999572754}\n...Training: end of batch 7; log content: {'loss': 11.389456748962402, 'mae': 2.56180477142334, 'mse': 11.389456748962402}\nEnd epoch 117 of training; log content: {'loss': 11.389456748962402, 'mae': 2.56180477142334, 'mse': 11.389456748962402, 'val_loss': 10.113228797912598, 'val_mae': 2.445990562438965, 'val_mse': 10.113228797912598}\n...Training: end of batch 0; log content: {'loss': 10.670793533325195, 'mae': 2.4124670028686523, 'mse': 10.670793533325195}\n...Training: end of batch 1; log content: {'loss': 8.258075714111328, 'mae': 2.121842622756958, 'mse': 8.258075714111328}\n...Training: end of batch 2; log content: {'loss': 9.194830894470215, 'mae': 2.254835844039917, 'mse': 9.194830894470215}\n...Training: end of batch 3; log content: {'loss': 9.312331199645996, 'mae': 2.3443093299865723, 'mse': 9.312331199645996}\n...Training: end of batch 4; log content: {'loss': 10.411592483520508, 'mae': 2.433753490447998, 'mse': 10.411592483520508}\n...Training: end of batch 5; log content: {'loss': 11.141230583190918, 'mae': 2.5319530963897705, 'mse': 11.141230583190918}\n...Training: end of batch 6; log content: {'loss': 11.217623710632324, 'mae': 2.5779669284820557, 'mse': 11.217623710632324}\n...Training: end of batch 7; log content: {'loss': 11.315875053405762, 'mae': 2.5614335536956787, 'mse': 11.315875053405762}\nEnd epoch 118 of training; log content: {'loss': 11.315875053405762, 'mae': 2.5614335536956787, 'mse': 11.315875053405762, 'val_loss': 10.025854110717773, 'val_mae': 2.4382386207580566, 'val_mse': 10.025854110717773}\n...Training: end of batch 0; log content: {'loss': 17.60824203491211, 'mae': 3.1928482055664062, 'mse': 17.60824203491211}\n...Training: end of batch 1; log content: {'loss': 15.879323959350586, 'mae': 3.120002031326294, 'mse': 15.879323959350586}\n...Training: end of batch 2; log content: {'loss': 13.865313529968262, 'mae': 2.8263347148895264, 'mse': 13.865313529968262}\n...Training: end of batch 3; log content: {'loss': 12.655439376831055, 'mae': 2.727313995361328, 'mse': 12.655439376831055}\n...Training: end of batch 4; log content: {'loss': 12.352540969848633, 'mae': 2.6755993366241455, 'mse': 12.352540969848633}\n...Training: end of batch 5; log content: {'loss': 12.202621459960938, 'mae': 2.6636393070220947, 'mse': 12.202621459960938}\n...Training: end of batch 6; log content: {'loss': 12.125935554504395, 'mae': 2.6665537357330322, 'mse': 12.125935554504395}\n...Training: end of batch 7; log content: {'loss': 11.36888599395752, 'mae': 2.5858771800994873, 'mse': 11.36888599395752}\nEnd epoch 119 of training; log content: {'loss': 11.36888599395752, 'mae': 2.5858771800994873, 'mse': 11.36888599395752, 'val_loss': 10.105013847351074, 'val_mae': 2.4386227130889893, 'val_mse': 10.105013847351074}\n...Training: end of batch 0; log content: {'loss': 15.970417022705078, 'mae': 3.004084587097168, 'mse': 15.970417022705078}\n...Training: end of batch 1; log content: {'loss': 12.72418212890625, 'mae': 2.7116780281066895, 'mse': 12.72418212890625}\n...Training: end of batch 2; log content: {'loss': 12.646227836608887, 'mae': 2.6188125610351562, 'mse': 12.646227836608887}\n...Training: end of batch 3; log content: {'loss': 12.024724006652832, 'mae': 2.5820064544677734, 'mse': 12.024724006652832}\n...Training: end of batch 4; log content: {'loss': 11.659585952758789, 'mae': 2.558410882949829, 'mse': 11.659585952758789}\n...Training: end of batch 5; log content: {'loss': 10.92702579498291, 'mae': 2.5009748935699463, 'mse': 10.92702579498291}\n...Training: end of batch 6; log content: {'loss': 11.337538719177246, 'mae': 2.565383195877075, 'mse': 11.337538719177246}\n...Training: end of batch 7; log content: {'loss': 11.323643684387207, 'mae': 2.5853099822998047, 'mse': 11.323643684387207}\nEnd epoch 120 of training; log content: {'loss': 11.323643684387207, 'mae': 2.5853099822998047, 'mse': 11.323643684387207, 'val_loss': 10.090808868408203, 'val_mae': 2.4491446018218994, 'val_mse': 10.090808868408203}\n...Training: end of batch 0; log content: {'loss': 10.159692764282227, 'mae': 2.4426071643829346, 'mse': 10.159692764282227}\n...Training: end of batch 1; log content: {'loss': 10.117843627929688, 'mae': 2.3803045749664307, 'mse': 10.117843627929688}\n...Training: end of batch 2; log content: {'loss': 9.653098106384277, 'mae': 2.3550455570220947, 'mse': 9.653098106384277}\n...Training: end of batch 3; log content: {'loss': 9.861540794372559, 'mae': 2.413588047027588, 'mse': 9.861540794372559}\n...Training: end of batch 4; log content: {'loss': 10.231907844543457, 'mae': 2.431567430496216, 'mse': 10.231907844543457}\n...Training: end of batch 5; log content: {'loss': 10.584567070007324, 'mae': 2.525499105453491, 'mse': 10.584567070007324}\n...Training: end of batch 6; log content: {'loss': 11.548535346984863, 'mae': 2.600558042526245, 'mse': 11.548535346984863}\n...Training: end of batch 7; log content: {'loss': 11.368136405944824, 'mae': 2.5574076175689697, 'mse': 11.368136405944824}\nEnd epoch 121 of training; log content: {'loss': 11.368136405944824, 'mae': 2.5574076175689697, 'mse': 11.368136405944824, 'val_loss': 10.064424514770508, 'val_mae': 2.460143566131592, 'val_mse': 10.064424514770508}\n...Training: end of batch 0; log content: {'loss': 7.860367774963379, 'mae': 2.2783796787261963, 'mse': 7.860367774963379}\n...Training: end of batch 1; log content: {'loss': 12.700479507446289, 'mae': 2.710211753845215, 'mse': 12.700479507446289}\n...Training: end of batch 2; log content: {'loss': 13.156425476074219, 'mae': 2.6847426891326904, 'mse': 13.156425476074219}\n...Training: end of batch 3; log content: {'loss': 12.032958984375, 'mae': 2.55470609664917, 'mse': 12.032958984375}\n...Training: end of batch 4; log content: {'loss': 11.347993850708008, 'mae': 2.5446083545684814, 'mse': 11.347993850708008}\n...Training: end of batch 5; log content: {'loss': 11.309271812438965, 'mae': 2.543637990951538, 'mse': 11.309271812438965}\n...Training: end of batch 6; log content: {'loss': 11.703370094299316, 'mae': 2.6023306846618652, 'mse': 11.703370094299316}\n...Training: end of batch 7; log content: {'loss': 11.367530822753906, 'mae': 2.5637524127960205, 'mse': 11.367530822753906}\nEnd epoch 122 of training; log content: {'loss': 11.367530822753906, 'mae': 2.5637524127960205, 'mse': 11.367530822753906, 'val_loss': 10.028434753417969, 'val_mae': 2.438382625579834, 'val_mse': 10.028434753417969}\n...Training: end of batch 0; log content: {'loss': 12.893896102905273, 'mae': 2.6804275512695312, 'mse': 12.893896102905273}\n...Training: end of batch 1; log content: {'loss': 14.354835510253906, 'mae': 2.8410544395446777, 'mse': 14.354835510253906}\n...Training: end of batch 2; log content: {'loss': 13.760967254638672, 'mae': 2.7720441818237305, 'mse': 13.760967254638672}\n...Training: end of batch 3; log content: {'loss': 12.36868953704834, 'mae': 2.5701184272766113, 'mse': 12.36868953704834}\n...Training: end of batch 4; log content: {'loss': 12.145258903503418, 'mae': 2.5941402912139893, 'mse': 12.145258903503418}\n...Training: end of batch 5; log content: {'loss': 12.092415809631348, 'mae': 2.612166166305542, 'mse': 12.092415809631348}\n...Training: end of batch 6; log content: {'loss': 11.609956741333008, 'mae': 2.5677123069763184, 'mse': 11.609956741333008}\n...Training: end of batch 7; log content: {'loss': 11.309370994567871, 'mae': 2.574700355529785, 'mse': 11.309370994567871}\nEnd epoch 123 of training; log content: {'loss': 11.309370994567871, 'mae': 2.574700355529785, 'mse': 11.309370994567871, 'val_loss': 10.054133415222168, 'val_mae': 2.442110061645508, 'val_mse': 10.054133415222168}\n...Training: end of batch 0; log content: {'loss': 13.402488708496094, 'mae': 2.8809444904327393, 'mse': 13.402488708496094}\n...Training: end of batch 1; log content: {'loss': 11.056097984313965, 'mae': 2.6765637397766113, 'mse': 11.056097984313965}\n...Training: end of batch 2; log content: {'loss': 12.406428337097168, 'mae': 2.7597835063934326, 'mse': 12.406428337097168}\n...Training: end of batch 3; log content: {'loss': 11.133488655090332, 'mae': 2.5960283279418945, 'mse': 11.133488655090332}\n...Training: end of batch 4; log content: {'loss': 10.781370162963867, 'mae': 2.5641238689422607, 'mse': 10.781370162963867}\n...Training: end of batch 5; log content: {'loss': 11.26767635345459, 'mae': 2.6029212474823, 'mse': 11.26767635345459}\n...Training: end of batch 6; log content: {'loss': 11.852262496948242, 'mae': 2.6197259426116943, 'mse': 11.852262496948242}\n...Training: end of batch 7; log content: {'loss': 11.414470672607422, 'mae': 2.5733888149261475, 'mse': 11.414470672607422}\nEnd epoch 124 of training; log content: {'loss': 11.414470672607422, 'mae': 2.5733888149261475, 'mse': 11.414470672607422, 'val_loss': 10.205962181091309, 'val_mae': 2.4700777530670166, 'val_mse': 10.205962181091309}\n...Training: end of batch 0; log content: {'loss': 10.852331161499023, 'mae': 2.4368104934692383, 'mse': 10.852331161499023}\n...Training: end of batch 1; log content: {'loss': 9.21403694152832, 'mae': 2.2800467014312744, 'mse': 9.21403694152832}\n...Training: end of batch 2; log content: {'loss': 12.41673755645752, 'mae': 2.5942955017089844, 'mse': 12.41673755645752}\n...Training: end of batch 3; log content: {'loss': 11.578153610229492, 'mae': 2.5493900775909424, 'mse': 11.578153610229492}\n...Training: end of batch 4; log content: {'loss': 11.193511962890625, 'mae': 2.5401761531829834, 'mse': 11.193511962890625}\n...Training: end of batch 5; log content: {'loss': 10.54517650604248, 'mae': 2.467900276184082, 'mse': 10.54517650604248}\n...Training: end of batch 6; log content: {'loss': 11.025288581848145, 'mae': 2.539191484451294, 'mse': 11.025288581848145}\n...Training: end of batch 7; log content: {'loss': 11.378558158874512, 'mae': 2.5780911445617676, 'mse': 11.378558158874512}\nEnd epoch 125 of training; log content: {'loss': 11.378558158874512, 'mae': 2.5780911445617676, 'mse': 11.378558158874512, 'val_loss': 10.030217170715332, 'val_mae': 2.426147937774658, 'val_mse': 10.030217170715332}\n...Training: end of batch 0; log content: {'loss': 6.3175764083862305, 'mae': 1.7500545978546143, 'mse': 6.3175764083862305}\n...Training: end of batch 1; log content: {'loss': 9.313102722167969, 'mae': 2.1455764770507812, 'mse': 9.313102722167969}\n...Training: end of batch 2; log content: {'loss': 9.917468070983887, 'mae': 2.3486106395721436, 'mse': 9.917468070983887}\n...Training: end of batch 3; log content: {'loss': 11.91136360168457, 'mae': 2.6443021297454834, 'mse': 11.91136360168457}\n...Training: end of batch 4; log content: {'loss': 12.376359939575195, 'mae': 2.6657519340515137, 'mse': 12.376359939575195}\n...Training: end of batch 5; log content: {'loss': 11.948593139648438, 'mae': 2.629345178604126, 'mse': 11.948593139648438}\n...Training: end of batch 6; log content: {'loss': 11.32215404510498, 'mae': 2.5948073863983154, 'mse': 11.32215404510498}\n...Training: end of batch 7; log content: {'loss': 11.322861671447754, 'mae': 2.580953598022461, 'mse': 11.322861671447754}\nEnd epoch 126 of training; log content: {'loss': 11.322861671447754, 'mae': 2.580953598022461, 'mse': 11.322861671447754, 'val_loss': 10.075884819030762, 'val_mae': 2.429324150085449, 'val_mse': 10.075884819030762}\n...Training: end of batch 0; log content: {'loss': 5.813950538635254, 'mae': 1.9854707717895508, 'mse': 5.813950538635254}\n...Training: end of batch 1; log content: {'loss': 8.248743057250977, 'mae': 2.2824764251708984, 'mse': 8.248743057250977}\n...Training: end of batch 2; log content: {'loss': 10.134671211242676, 'mae': 2.5322654247283936, 'mse': 10.134671211242676}\n...Training: end of batch 3; log content: {'loss': 9.742849349975586, 'mae': 2.469050168991089, 'mse': 9.742849349975586}\n...Training: end of batch 4; log content: {'loss': 9.994958877563477, 'mae': 2.514772891998291, 'mse': 9.994958877563477}\n...Training: end of batch 5; log content: {'loss': 10.171319961547852, 'mae': 2.515645742416382, 'mse': 10.171319961547852}\n...Training: end of batch 6; log content: {'loss': 10.419649124145508, 'mae': 2.5075504779815674, 'mse': 10.419649124145508}\n...Training: end of batch 7; log content: {'loss': 11.36893367767334, 'mae': 2.5669326782226562, 'mse': 11.36893367767334}\nEnd epoch 127 of training; log content: {'loss': 11.36893367767334, 'mae': 2.5669326782226562, 'mse': 11.36893367767334, 'val_loss': 10.226198196411133, 'val_mae': 2.4604666233062744, 'val_mse': 10.226198196411133}\n...Training: end of batch 0; log content: {'loss': 8.776519775390625, 'mae': 2.223623037338257, 'mse': 8.776519775390625}\n...Training: end of batch 1; log content: {'loss': 14.954029083251953, 'mae': 2.7756848335266113, 'mse': 14.954029083251953}\n...Training: end of batch 2; log content: {'loss': 13.43384075164795, 'mae': 2.712991714477539, 'mse': 13.43384075164795}\n...Training: end of batch 3; log content: {'loss': 13.129619598388672, 'mae': 2.707472324371338, 'mse': 13.129619598388672}\n...Training: end of batch 4; log content: {'loss': 11.987743377685547, 'mae': 2.6038613319396973, 'mse': 11.987743377685547}\n...Training: end of batch 5; log content: {'loss': 11.451634407043457, 'mae': 2.5394022464752197, 'mse': 11.451634407043457}\n...Training: end of batch 6; log content: {'loss': 11.356865882873535, 'mae': 2.577139377593994, 'mse': 11.356865882873535}\n...Training: end of batch 7; log content: {'loss': 11.431013107299805, 'mae': 2.57915997505188, 'mse': 11.431013107299805}\nEnd epoch 128 of training; log content: {'loss': 11.431013107299805, 'mae': 2.57915997505188, 'mse': 11.431013107299805, 'val_loss': 10.030213356018066, 'val_mae': 2.4220354557037354, 'val_mse': 10.030213356018066}\n...Training: end of batch 0; log content: {'loss': 16.997364044189453, 'mae': 3.166621685028076, 'mse': 16.997364044189453}\n...Training: end of batch 1; log content: {'loss': 15.708738327026367, 'mae': 3.056942939758301, 'mse': 15.708738327026367}\n...Training: end of batch 2; log content: {'loss': 13.273295402526855, 'mae': 2.8241589069366455, 'mse': 13.273295402526855}\n...Training: end of batch 3; log content: {'loss': 13.364494323730469, 'mae': 2.7997000217437744, 'mse': 13.364494323730469}\n...Training: end of batch 4; log content: {'loss': 12.407495498657227, 'mae': 2.6958365440368652, 'mse': 12.407495498657227}\n...Training: end of batch 5; log content: {'loss': 12.917052268981934, 'mae': 2.8004977703094482, 'mse': 12.917052268981934}\n...Training: end of batch 6; log content: {'loss': 11.619307518005371, 'mae': 2.627531051635742, 'mse': 11.619307518005371}\n...Training: end of batch 7; log content: {'loss': 11.340263366699219, 'mae': 2.5788116455078125, 'mse': 11.340263366699219}\nEnd epoch 129 of training; log content: {'loss': 11.340263366699219, 'mae': 2.5788116455078125, 'mse': 11.340263366699219, 'val_loss': 10.125633239746094, 'val_mae': 2.446533679962158, 'val_mse': 10.125633239746094}\n...Training: end of batch 0; log content: {'loss': 9.408805847167969, 'mae': 2.364389419555664, 'mse': 9.408805847167969}\n...Training: end of batch 1; log content: {'loss': 11.658122062683105, 'mae': 2.370037317276001, 'mse': 11.658122062683105}\n...Training: end of batch 2; log content: {'loss': 11.107978820800781, 'mae': 2.4593489170074463, 'mse': 11.107978820800781}\n...Training: end of batch 3; log content: {'loss': 10.769989013671875, 'mae': 2.476820468902588, 'mse': 10.769989013671875}\n...Training: end of batch 4; log content: {'loss': 10.909991264343262, 'mae': 2.5060925483703613, 'mse': 10.909991264343262}\n...Training: end of batch 5; log content: {'loss': 11.543730735778809, 'mae': 2.568260431289673, 'mse': 11.543730735778809}\n...Training: end of batch 6; log content: {'loss': 11.603970527648926, 'mae': 2.560605764389038, 'mse': 11.603970527648926}\n...Training: end of batch 7; log content: {'loss': 11.35561466217041, 'mae': 2.5590438842773438, 'mse': 11.35561466217041}\nEnd epoch 130 of training; log content: {'loss': 11.35561466217041, 'mae': 2.5590438842773438, 'mse': 11.35561466217041, 'val_loss': 10.172910690307617, 'val_mae': 2.467150926589966, 'val_mse': 10.172910690307617}\n...Training: end of batch 0; log content: {'loss': 7.602845668792725, 'mae': 1.915493369102478, 'mse': 7.602845668792725}\n...Training: end of batch 1; log content: {'loss': 11.269556999206543, 'mae': 2.285346508026123, 'mse': 11.269556999206543}\n...Training: end of batch 2; log content: {'loss': 11.970463752746582, 'mae': 2.4686591625213623, 'mse': 11.970463752746582}\n...Training: end of batch 3; log content: {'loss': 11.964925765991211, 'mae': 2.544386386871338, 'mse': 11.964925765991211}\n...Training: end of batch 4; log content: {'loss': 11.592353820800781, 'mae': 2.5736331939697266, 'mse': 11.592353820800781}\n...Training: end of batch 5; log content: {'loss': 10.659711837768555, 'mae': 2.4960687160491943, 'mse': 10.659711837768555}\n...Training: end of batch 6; log content: {'loss': 11.275390625, 'mae': 2.575960636138916, 'mse': 11.275390625}\n...Training: end of batch 7; log content: {'loss': 11.35758113861084, 'mae': 2.5720973014831543, 'mse': 11.35758113861084}\nEnd epoch 131 of training; log content: {'loss': 11.35758113861084, 'mae': 2.5720973014831543, 'mse': 11.35758113861084, 'val_loss': 9.942754745483398, 'val_mae': 2.4320459365844727, 'val_mse': 9.942754745483398}\n...Training: end of batch 0; log content: {'loss': 10.424494743347168, 'mae': 2.713984489440918, 'mse': 10.424494743347168}\n...Training: end of batch 1; log content: {'loss': 13.689212799072266, 'mae': 2.9024791717529297, 'mse': 13.689212799072266}\n...Training: end of batch 2; log content: {'loss': 10.72595500946045, 'mae': 2.5240824222564697, 'mse': 10.72595500946045}\n...Training: end of batch 3; log content: {'loss': 10.931321144104004, 'mae': 2.5787336826324463, 'mse': 10.931321144104004}\n...Training: end of batch 4; log content: {'loss': 11.393755912780762, 'mae': 2.5783326625823975, 'mse': 11.393755912780762}\n...Training: end of batch 5; log content: {'loss': 11.216392517089844, 'mae': 2.5598092079162598, 'mse': 11.216392517089844}\n...Training: end of batch 6; log content: {'loss': 11.038925170898438, 'mae': 2.544376850128174, 'mse': 11.038925170898438}\n...Training: end of batch 7; log content: {'loss': 11.321704864501953, 'mae': 2.572533130645752, 'mse': 11.321704864501953}\nEnd epoch 132 of training; log content: {'loss': 11.321704864501953, 'mae': 2.572533130645752, 'mse': 11.321704864501953, 'val_loss': 10.033677101135254, 'val_mae': 2.4340693950653076, 'val_mse': 10.033677101135254}\n...Training: end of batch 0; log content: {'loss': 19.547039031982422, 'mae': 3.3583736419677734, 'mse': 19.547039031982422}\n...Training: end of batch 1; log content: {'loss': 16.1804141998291, 'mae': 3.043116331100464, 'mse': 16.1804141998291}\n...Training: end of batch 2; log content: {'loss': 13.816901206970215, 'mae': 2.751337766647339, 'mse': 13.816901206970215}\n...Training: end of batch 3; log content: {'loss': 13.02562427520752, 'mae': 2.759488582611084, 'mse': 13.02562427520752}\n...Training: end of batch 4; log content: {'loss': 11.591890335083008, 'mae': 2.6173133850097656, 'mse': 11.591890335083008}\n...Training: end of batch 5; log content: {'loss': 11.108857154846191, 'mae': 2.5671541690826416, 'mse': 11.108857154846191}\n...Training: end of batch 6; log content: {'loss': 11.334202766418457, 'mae': 2.5794827938079834, 'mse': 11.334202766418457}\n...Training: end of batch 7; log content: {'loss': 11.354838371276855, 'mae': 2.573512554168701, 'mse': 11.354838371276855}\nEnd epoch 133 of training; log content: {'loss': 11.354838371276855, 'mae': 2.573512554168701, 'mse': 11.354838371276855, 'val_loss': 10.09153938293457, 'val_mae': 2.454864263534546, 'val_mse': 10.09153938293457}\n...Training: end of batch 0; log content: {'loss': 12.851930618286133, 'mae': 3.1533992290496826, 'mse': 12.851930618286133}\n...Training: end of batch 1; log content: {'loss': 13.670479774475098, 'mae': 3.024937629699707, 'mse': 13.670479774475098}\n...Training: end of batch 2; log content: {'loss': 12.34747314453125, 'mae': 2.8100852966308594, 'mse': 12.34747314453125}\n...Training: end of batch 3; log content: {'loss': 11.927105903625488, 'mae': 2.753842353820801, 'mse': 11.927105903625488}\n...Training: end of batch 4; log content: {'loss': 11.770317077636719, 'mae': 2.667556047439575, 'mse': 11.770317077636719}\n...Training: end of batch 5; log content: {'loss': 11.872570991516113, 'mae': 2.6495602130889893, 'mse': 11.872570991516113}\n...Training: end of batch 6; log content: {'loss': 11.842630386352539, 'mae': 2.636749744415283, 'mse': 11.842630386352539}\n...Training: end of batch 7; log content: {'loss': 11.329846382141113, 'mae': 2.57635760307312, 'mse': 11.329846382141113}\nEnd epoch 134 of training; log content: {'loss': 11.329846382141113, 'mae': 2.57635760307312, 'mse': 11.329846382141113, 'val_loss': 10.070368766784668, 'val_mae': 2.429527759552002, 'val_mse': 10.070368766784668}\n...Training: end of batch 0; log content: {'loss': 11.9814453125, 'mae': 2.6064863204956055, 'mse': 11.9814453125}\n...Training: end of batch 1; log content: {'loss': 10.808670043945312, 'mae': 2.517425775527954, 'mse': 10.808670043945312}\n...Training: end of batch 2; log content: {'loss': 11.722808837890625, 'mae': 2.532026529312134, 'mse': 11.722808837890625}\n...Training: end of batch 3; log content: {'loss': 12.571292877197266, 'mae': 2.62467098236084, 'mse': 12.571292877197266}\n...Training: end of batch 4; log content: {'loss': 11.24207878112793, 'mae': 2.507540702819824, 'mse': 11.24207878112793}\n...Training: end of batch 5; log content: {'loss': 11.3794527053833, 'mae': 2.570814371109009, 'mse': 11.3794527053833}\n...Training: end of batch 6; log content: {'loss': 11.45751953125, 'mae': 2.564037561416626, 'mse': 11.45751953125}\n...Training: end of batch 7; log content: {'loss': 11.380191802978516, 'mae': 2.5854721069335938, 'mse': 11.380191802978516}\nEnd epoch 135 of training; log content: {'loss': 11.380191802978516, 'mae': 2.5854721069335938, 'mse': 11.380191802978516, 'val_loss': 10.049179077148438, 'val_mae': 2.4459097385406494, 'val_mse': 10.049179077148438}\n...Training: end of batch 0; log content: {'loss': 9.460691452026367, 'mae': 2.458106517791748, 'mse': 9.460691452026367}\n...Training: end of batch 1; log content: {'loss': 11.999382019042969, 'mae': 2.7215356826782227, 'mse': 11.999382019042969}\n...Training: end of batch 2; log content: {'loss': 11.99065113067627, 'mae': 2.7370786666870117, 'mse': 11.99065113067627}\n...Training: end of batch 3; log content: {'loss': 11.337703704833984, 'mae': 2.64432430267334, 'mse': 11.337703704833984}\n...Training: end of batch 4; log content: {'loss': 11.194050788879395, 'mae': 2.61690354347229, 'mse': 11.194050788879395}\n...Training: end of batch 5; log content: {'loss': 11.8567533493042, 'mae': 2.6416139602661133, 'mse': 11.8567533493042}\n...Training: end of batch 6; log content: {'loss': 11.804435729980469, 'mae': 2.6469578742980957, 'mse': 11.804435729980469}\n...Training: end of batch 7; log content: {'loss': 11.329867362976074, 'mae': 2.5765695571899414, 'mse': 11.329867362976074}\nEnd epoch 136 of training; log content: {'loss': 11.329867362976074, 'mae': 2.5765695571899414, 'mse': 11.329867362976074, 'val_loss': 10.04690170288086, 'val_mae': 2.4317431449890137, 'val_mse': 10.04690170288086}\n...Training: end of batch 0; log content: {'loss': 17.215343475341797, 'mae': 3.125255584716797, 'mse': 17.215343475341797}\n...Training: end of batch 1; log content: {'loss': 13.456075668334961, 'mae': 2.8176519870758057, 'mse': 13.456075668334961}\n...Training: end of batch 2; log content: {'loss': 12.78260326385498, 'mae': 2.863144636154175, 'mse': 12.78260326385498}\n...Training: end of batch 3; log content: {'loss': 12.358519554138184, 'mae': 2.783815622329712, 'mse': 12.358519554138184}\n...Training: end of batch 4; log content: {'loss': 11.68976879119873, 'mae': 2.641801357269287, 'mse': 11.68976879119873}\n...Training: end of batch 5; log content: {'loss': 11.325736045837402, 'mae': 2.603703498840332, 'mse': 11.325736045837402}\n...Training: end of batch 6; log content: {'loss': 11.720845222473145, 'mae': 2.6126556396484375, 'mse': 11.720845222473145}\n...Training: end of batch 7; log content: {'loss': 11.289457321166992, 'mae': 2.566558599472046, 'mse': 11.289457321166992}\nEnd epoch 137 of training; log content: {'loss': 11.289457321166992, 'mae': 2.566558599472046, 'mse': 11.289457321166992, 'val_loss': 10.12779426574707, 'val_mae': 2.454298496246338, 'val_mse': 10.12779426574707}\n...Training: end of batch 0; log content: {'loss': 12.977187156677246, 'mae': 2.465148448944092, 'mse': 12.977187156677246}\n...Training: end of batch 1; log content: {'loss': 10.584165573120117, 'mae': 2.294104814529419, 'mse': 10.584165573120117}\n...Training: end of batch 2; log content: {'loss': 11.28506088256836, 'mae': 2.424348831176758, 'mse': 11.28506088256836}\n...Training: end of batch 3; log content: {'loss': 10.8793363571167, 'mae': 2.4560718536376953, 'mse': 10.8793363571167}\n...Training: end of batch 4; log content: {'loss': 11.369569778442383, 'mae': 2.5649023056030273, 'mse': 11.369569778442383}\n...Training: end of batch 5; log content: {'loss': 11.489323616027832, 'mae': 2.5484459400177, 'mse': 11.489323616027832}\n...Training: end of batch 6; log content: {'loss': 11.77053165435791, 'mae': 2.5807244777679443, 'mse': 11.77053165435791}\n...Training: end of batch 7; log content: {'loss': 11.371917724609375, 'mae': 2.5571446418762207, 'mse': 11.371917724609375}\nEnd epoch 138 of training; log content: {'loss': 11.371917724609375, 'mae': 2.5571446418762207, 'mse': 11.371917724609375, 'val_loss': 10.162534713745117, 'val_mae': 2.453404426574707, 'val_mse': 10.162534713745117}\n...Training: end of batch 0; log content: {'loss': 8.924694061279297, 'mae': 2.2842140197753906, 'mse': 8.924694061279297}\n...Training: end of batch 1; log content: {'loss': 11.800077438354492, 'mae': 2.617988348007202, 'mse': 11.800077438354492}\n...Training: end of batch 2; log content: {'loss': 11.848526954650879, 'mae': 2.6544559001922607, 'mse': 11.848526954650879}\n...Training: end of batch 3; log content: {'loss': 11.862391471862793, 'mae': 2.6767020225524902, 'mse': 11.862391471862793}\n...Training: end of batch 4; log content: {'loss': 11.928194999694824, 'mae': 2.650968313217163, 'mse': 11.928194999694824}\n...Training: end of batch 5; log content: {'loss': 11.758978843688965, 'mae': 2.6533868312835693, 'mse': 11.758978843688965}\n...Training: end of batch 6; log content: {'loss': 11.853297233581543, 'mae': 2.6281228065490723, 'mse': 11.853297233581543}\n...Training: end of batch 7; log content: {'loss': 11.30815315246582, 'mae': 2.5668270587921143, 'mse': 11.30815315246582}\nEnd epoch 139 of training; log content: {'loss': 11.30815315246582, 'mae': 2.5668270587921143, 'mse': 11.30815315246582, 'val_loss': 10.023886680603027, 'val_mae': 2.4333105087280273, 'val_mse': 10.023886680603027}\n...Training: end of batch 0; log content: {'loss': 12.341388702392578, 'mae': 2.83256196975708, 'mse': 12.341388702392578}\n...Training: end of batch 1; log content: {'loss': 11.558289527893066, 'mae': 2.8025219440460205, 'mse': 11.558289527893066}\n...Training: end of batch 2; log content: {'loss': 11.813041687011719, 'mae': 2.715381622314453, 'mse': 11.813041687011719}\n...Training: end of batch 3; log content: {'loss': 9.680230140686035, 'mae': 2.4269845485687256, 'mse': 9.680230140686035}\n...Training: end of batch 4; log content: {'loss': 11.57666301727295, 'mae': 2.652161121368408, 'mse': 11.57666301727295}\n...Training: end of batch 5; log content: {'loss': 10.729535102844238, 'mae': 2.578699827194214, 'mse': 10.729535102844238}\n...Training: end of batch 6; log content: {'loss': 11.89441967010498, 'mae': 2.652430772781372, 'mse': 11.89441967010498}\n...Training: end of batch 7; log content: {'loss': 11.31330680847168, 'mae': 2.5784544944763184, 'mse': 11.31330680847168}\nEnd epoch 140 of training; log content: {'loss': 11.31330680847168, 'mae': 2.5784544944763184, 'mse': 11.31330680847168, 'val_loss': 9.950023651123047, 'val_mae': 2.428093433380127, 'val_mse': 9.950023651123047}\n...Training: end of batch 0; log content: {'loss': 6.960434913635254, 'mae': 2.0684878826141357, 'mse': 6.960434913635254}\n...Training: end of batch 1; log content: {'loss': 14.575250625610352, 'mae': 2.7674560546875, 'mse': 14.575250625610352}\n...Training: end of batch 2; log content: {'loss': 13.43471908569336, 'mae': 2.731065511703491, 'mse': 13.43471908569336}\n...Training: end of batch 3; log content: {'loss': 11.966882705688477, 'mae': 2.604365348815918, 'mse': 11.966882705688477}\n...Training: end of batch 4; log content: {'loss': 11.151922225952148, 'mae': 2.5316576957702637, 'mse': 11.151922225952148}\n...Training: end of batch 5; log content: {'loss': 11.64688491821289, 'mae': 2.5656003952026367, 'mse': 11.64688491821289}\n...Training: end of batch 6; log content: {'loss': 11.580933570861816, 'mae': 2.5987393856048584, 'mse': 11.580933570861816}\n...Training: end of batch 7; log content: {'loss': 11.351933479309082, 'mae': 2.578848361968994, 'mse': 11.351933479309082}\nEnd epoch 141 of training; log content: {'loss': 11.351933479309082, 'mae': 2.578848361968994, 'mse': 11.351933479309082, 'val_loss': 9.914196014404297, 'val_mae': 2.4355366230010986, 'val_mse': 9.914196014404297}\n...Training: end of batch 0; log content: {'loss': 11.527713775634766, 'mae': 2.5599169731140137, 'mse': 11.527713775634766}\n...Training: end of batch 1; log content: {'loss': 11.500252723693848, 'mae': 2.737138271331787, 'mse': 11.500252723693848}\n...Training: end of batch 2; log content: {'loss': 13.769986152648926, 'mae': 2.90094256401062, 'mse': 13.769986152648926}\n...Training: end of batch 3; log content: {'loss': 11.664816856384277, 'mae': 2.6574771404266357, 'mse': 11.664816856384277}\n...Training: end of batch 4; log content: {'loss': 11.782052993774414, 'mae': 2.6572325229644775, 'mse': 11.782052993774414}\n...Training: end of batch 5; log content: {'loss': 12.450889587402344, 'mae': 2.725733518600464, 'mse': 12.450889587402344}\n...Training: end of batch 6; log content: {'loss': 11.632197380065918, 'mae': 2.6109044551849365, 'mse': 11.632197380065918}\n...Training: end of batch 7; log content: {'loss': 11.36037540435791, 'mae': 2.566540479660034, 'mse': 11.36037540435791}\nEnd epoch 142 of training; log content: {'loss': 11.36037540435791, 'mae': 2.566540479660034, 'mse': 11.36037540435791, 'val_loss': 10.037179946899414, 'val_mae': 2.453382968902588, 'val_mse': 10.037179946899414}\n...Training: end of batch 0; log content: {'loss': 8.176532745361328, 'mae': 2.1532633304595947, 'mse': 8.176532745361328}\n...Training: end of batch 1; log content: {'loss': 9.644366264343262, 'mae': 2.4071154594421387, 'mse': 9.644366264343262}\n...Training: end of batch 2; log content: {'loss': 9.37878131866455, 'mae': 2.350140333175659, 'mse': 9.37878131866455}\n...Training: end of batch 3; log content: {'loss': 9.55605697631836, 'mae': 2.3659555912017822, 'mse': 9.55605697631836}\n...Training: end of batch 4; log content: {'loss': 9.848376274108887, 'mae': 2.4477450847625732, 'mse': 9.848376274108887}\n...Training: end of batch 5; log content: {'loss': 10.222512245178223, 'mae': 2.475438356399536, 'mse': 10.222512245178223}\n...Training: end of batch 6; log content: {'loss': 10.726761817932129, 'mae': 2.508449077606201, 'mse': 10.726761817932129}\n...Training: end of batch 7; log content: {'loss': 11.321444511413574, 'mae': 2.5809412002563477, 'mse': 11.321444511413574}\nEnd epoch 143 of training; log content: {'loss': 11.321444511413574, 'mae': 2.5809412002563477, 'mse': 11.321444511413574, 'val_loss': 10.053174018859863, 'val_mae': 2.4289472103118896, 'val_mse': 10.053174018859863}\n...Training: end of batch 0; log content: {'loss': 9.179443359375, 'mae': 2.4243721961975098, 'mse': 9.179443359375}\n...Training: end of batch 1; log content: {'loss': 12.53128433227539, 'mae': 2.6301956176757812, 'mse': 12.53128433227539}\n...Training: end of batch 2; log content: {'loss': 11.317461967468262, 'mae': 2.5811803340911865, 'mse': 11.317461967468262}\n...Training: end of batch 3; log content: {'loss': 10.588129043579102, 'mae': 2.4548897743225098, 'mse': 10.588129043579102}\n...Training: end of batch 4; log content: {'loss': 11.428622245788574, 'mae': 2.619302272796631, 'mse': 11.428622245788574}\n...Training: end of batch 5; log content: {'loss': 10.470677375793457, 'mae': 2.479546070098877, 'mse': 10.470677375793457}\n...Training: end of batch 6; log content: {'loss': 10.705293655395508, 'mae': 2.515641927719116, 'mse': 10.705293655395508}\n...Training: end of batch 7; log content: {'loss': 11.301777839660645, 'mae': 2.573464870452881, 'mse': 11.301777839660645}\nEnd epoch 144 of training; log content: {'loss': 11.301777839660645, 'mae': 2.573464870452881, 'mse': 11.301777839660645, 'val_loss': 10.152849197387695, 'val_mae': 2.444319486618042, 'val_mse': 10.152849197387695}\n...Training: end of batch 0; log content: {'loss': 11.152420997619629, 'mae': 2.5831985473632812, 'mse': 11.152420997619629}\n...Training: end of batch 1; log content: {'loss': 10.905998229980469, 'mae': 2.564633846282959, 'mse': 10.905998229980469}\n...Training: end of batch 2; log content: {'loss': 11.006810188293457, 'mae': 2.589531898498535, 'mse': 11.006810188293457}\n...Training: end of batch 3; log content: {'loss': 12.027002334594727, 'mae': 2.6371889114379883, 'mse': 12.027002334594727}\n...Training: end of batch 4; log content: {'loss': 12.676959991455078, 'mae': 2.692019462585449, 'mse': 12.676959991455078}\n...Training: end of batch 5; log content: {'loss': 12.468995094299316, 'mae': 2.7086517810821533, 'mse': 12.468995094299316}\n...Training: end of batch 6; log content: {'loss': 12.199692726135254, 'mae': 2.6896069049835205, 'mse': 12.199692726135254}\n...Training: end of batch 7; log content: {'loss': 11.330620765686035, 'mae': 2.569432020187378, 'mse': 11.330620765686035}\nEnd epoch 145 of training; log content: {'loss': 11.330620765686035, 'mae': 2.569432020187378, 'mse': 11.330620765686035, 'val_loss': 10.024188041687012, 'val_mae': 2.4396939277648926, 'val_mse': 10.024188041687012}\n...Training: end of batch 0; log content: {'loss': 8.282402038574219, 'mae': 2.335721015930176, 'mse': 8.282402038574219}\n...Training: end of batch 1; log content: {'loss': 9.676512718200684, 'mae': 2.5819058418273926, 'mse': 9.676512718200684}\n...Training: end of batch 2; log content: {'loss': 8.487093925476074, 'mae': 2.4151298999786377, 'mse': 8.487093925476074}\n...Training: end of batch 3; log content: {'loss': 9.075098037719727, 'mae': 2.4181571006774902, 'mse': 9.075098037719727}\n...Training: end of batch 4; log content: {'loss': 9.752128601074219, 'mae': 2.532078266143799, 'mse': 9.752128601074219}\n...Training: end of batch 5; log content: {'loss': 10.550369262695312, 'mae': 2.5681850910186768, 'mse': 10.550369262695312}\n...Training: end of batch 6; log content: {'loss': 11.000360488891602, 'mae': 2.558363437652588, 'mse': 11.000360488891602}\n...Training: end of batch 7; log content: {'loss': 11.332771301269531, 'mae': 2.5576741695404053, 'mse': 11.332771301269531}\nEnd epoch 146 of training; log content: {'loss': 11.332771301269531, 'mae': 2.5576741695404053, 'mse': 11.332771301269531, 'val_loss': 10.102289199829102, 'val_mae': 2.45359468460083, 'val_mse': 10.102289199829102}\n...Training: end of batch 0; log content: {'loss': 10.212082862854004, 'mae': 2.534824848175049, 'mse': 10.212082862854004}\n...Training: end of batch 1; log content: {'loss': 11.610771179199219, 'mae': 2.6842923164367676, 'mse': 11.610771179199219}\n...Training: end of batch 2; log content: {'loss': 12.787826538085938, 'mae': 2.8457958698272705, 'mse': 12.787826538085938}\n...Training: end of batch 3; log content: {'loss': 12.04362678527832, 'mae': 2.743421792984009, 'mse': 12.04362678527832}\n...Training: end of batch 4; log content: {'loss': 11.606881141662598, 'mae': 2.576768398284912, 'mse': 11.606881141662598}\n...Training: end of batch 5; log content: {'loss': 11.454360008239746, 'mae': 2.584803342819214, 'mse': 11.454360008239746}\n...Training: end of batch 6; log content: {'loss': 11.448585510253906, 'mae': 2.5830039978027344, 'mse': 11.448585510253906}\n...Training: end of batch 7; log content: {'loss': 11.419304847717285, 'mae': 2.5939762592315674, 'mse': 11.419304847717285}\nEnd epoch 147 of training; log content: {'loss': 11.419304847717285, 'mae': 2.5939762592315674, 'mse': 11.419304847717285, 'val_loss': 9.987462997436523, 'val_mae': 2.4076294898986816, 'val_mse': 9.987462997436523}\n...Training: end of batch 0; log content: {'loss': 10.245166778564453, 'mae': 2.3694543838500977, 'mse': 10.245166778564453}\n...Training: end of batch 1; log content: {'loss': 10.698506355285645, 'mae': 2.5101449489593506, 'mse': 10.698506355285645}\n...Training: end of batch 2; log content: {'loss': 10.646073341369629, 'mae': 2.543609380722046, 'mse': 10.646073341369629}\n...Training: end of batch 3; log content: {'loss': 11.19074821472168, 'mae': 2.5806658267974854, 'mse': 11.19074821472168}\n...Training: end of batch 4; log content: {'loss': 11.296846389770508, 'mae': 2.6271936893463135, 'mse': 11.296846389770508}\n...Training: end of batch 5; log content: {'loss': 11.16364574432373, 'mae': 2.587214231491089, 'mse': 11.16364574432373}\n...Training: end of batch 6; log content: {'loss': 11.812891006469727, 'mae': 2.634507656097412, 'mse': 11.812891006469727}\n...Training: end of batch 7; log content: {'loss': 11.312503814697266, 'mae': 2.5835933685302734, 'mse': 11.312503814697266}\nEnd epoch 148 of training; log content: {'loss': 11.312503814697266, 'mae': 2.5835933685302734, 'mse': 11.312503814697266, 'val_loss': 10.048905372619629, 'val_mae': 2.450103282928467, 'val_mse': 10.048905372619629}\n...Training: end of batch 0; log content: {'loss': 13.223909378051758, 'mae': 2.727057695388794, 'mse': 13.223909378051758}\n...Training: end of batch 1; log content: {'loss': 11.189218521118164, 'mae': 2.508436679840088, 'mse': 11.189218521118164}\n...Training: end of batch 2; log content: {'loss': 12.011824607849121, 'mae': 2.592651605606079, 'mse': 12.011824607849121}\n...Training: end of batch 3; log content: {'loss': 12.33792781829834, 'mae': 2.6916403770446777, 'mse': 12.33792781829834}\n...Training: end of batch 4; log content: {'loss': 11.380596160888672, 'mae': 2.5824246406555176, 'mse': 11.380596160888672}\n...Training: end of batch 5; log content: {'loss': 11.820252418518066, 'mae': 2.5924065113067627, 'mse': 11.820252418518066}\n...Training: end of batch 6; log content: {'loss': 11.790757179260254, 'mae': 2.609846830368042, 'mse': 11.790757179260254}\n...Training: end of batch 7; log content: {'loss': 11.320796012878418, 'mae': 2.570554256439209, 'mse': 11.320796012878418}\nEnd epoch 149 of training; log content: {'loss': 11.320796012878418, 'mae': 2.570554256439209, 'mse': 11.320796012878418, 'val_loss': 10.0235013961792, 'val_mae': 2.4484896659851074, 'val_mse': 10.0235013961792}\n...Training: end of batch 0; log content: {'loss': 10.947317123413086, 'mae': 2.5651988983154297, 'mse': 10.947317123413086}\n...Training: end of batch 1; log content: {'loss': 11.776918411254883, 'mae': 2.6473376750946045, 'mse': 11.776918411254883}\n...Training: end of batch 2; log content: {'loss': 10.99020767211914, 'mae': 2.5927796363830566, 'mse': 10.99020767211914}\n...Training: end of batch 3; log content: {'loss': 13.003621101379395, 'mae': 2.7144672870635986, 'mse': 13.003621101379395}\n...Training: end of batch 4; log content: {'loss': 12.341104507446289, 'mae': 2.6693005561828613, 'mse': 12.341104507446289}\n...Training: end of batch 5; log content: {'loss': 11.788861274719238, 'mae': 2.6178994178771973, 'mse': 11.788861274719238}\n...Training: end of batch 6; log content: {'loss': 11.290295600891113, 'mae': 2.5645694732666016, 'mse': 11.290295600891113}\n...Training: end of batch 7; log content: {'loss': 11.307823181152344, 'mae': 2.563534736633301, 'mse': 11.307823181152344}\nEnd epoch 150 of training; log content: {'loss': 11.307823181152344, 'mae': 2.563534736633301, 'mse': 11.307823181152344, 'val_loss': 10.09244155883789, 'val_mae': 2.448258638381958, 'val_mse': 10.09244155883789}\n...Training: end of batch 0; log content: {'loss': 14.20418930053711, 'mae': 2.7908687591552734, 'mse': 14.20418930053711}\n...Training: end of batch 1; log content: {'loss': 12.544477462768555, 'mae': 2.7002828121185303, 'mse': 12.544477462768555}\n...Training: end of batch 2; log content: {'loss': 12.0695219039917, 'mae': 2.5934865474700928, 'mse': 12.0695219039917}\n...Training: end of batch 3; log content: {'loss': 11.411565780639648, 'mae': 2.5789427757263184, 'mse': 11.411565780639648}\n...Training: end of batch 4; log content: {'loss': 12.28801155090332, 'mae': 2.699380874633789, 'mse': 12.28801155090332}\n...Training: end of batch 5; log content: {'loss': 11.630061149597168, 'mae': 2.603083848953247, 'mse': 11.630061149597168}\n...Training: end of batch 6; log content: {'loss': 11.258118629455566, 'mae': 2.5543761253356934, 'mse': 11.258118629455566}\n...Training: end of batch 7; log content: {'loss': 11.3102388381958, 'mae': 2.564666271209717, 'mse': 11.3102388381958}\nEnd epoch 151 of training; log content: {'loss': 11.3102388381958, 'mae': 2.564666271209717, 'mse': 11.3102388381958, 'val_loss': 10.063933372497559, 'val_mae': 2.444643020629883, 'val_mse': 10.063933372497559}\n...Training: end of batch 0; log content: {'loss': 12.12077522277832, 'mae': 2.5682530403137207, 'mse': 12.12077522277832}\n...Training: end of batch 1; log content: {'loss': 10.860236167907715, 'mae': 2.489567279815674, 'mse': 10.860236167907715}\n...Training: end of batch 2; log content: {'loss': 11.456958770751953, 'mae': 2.587033987045288, 'mse': 11.456958770751953}\n...Training: end of batch 3; log content: {'loss': 10.859254837036133, 'mae': 2.5716543197631836, 'mse': 10.859254837036133}\n...Training: end of batch 4; log content: {'loss': 10.925753593444824, 'mae': 2.5882201194763184, 'mse': 10.925753593444824}\n...Training: end of batch 5; log content: {'loss': 11.248428344726562, 'mae': 2.6138670444488525, 'mse': 11.248428344726562}\n...Training: end of batch 6; log content: {'loss': 11.867903709411621, 'mae': 2.634381055831909, 'mse': 11.867903709411621}\n...Training: end of batch 7; log content: {'loss': 11.284608840942383, 'mae': 2.5780696868896484, 'mse': 11.284608840942383}\nEnd epoch 152 of training; log content: {'loss': 11.284608840942383, 'mae': 2.5780696868896484, 'mse': 11.284608840942383, 'val_loss': 9.986686706542969, 'val_mae': 2.420668363571167, 'val_mse': 9.986686706542969}\n...Training: end of batch 0; log content: {'loss': 12.592412948608398, 'mae': 2.661048650741577, 'mse': 12.592412948608398}\n...Training: end of batch 1; log content: {'loss': 10.311767578125, 'mae': 2.4073915481567383, 'mse': 10.311767578125}\n...Training: end of batch 2; log content: {'loss': 10.279213905334473, 'mae': 2.452632188796997, 'mse': 10.279213905334473}\n...Training: end of batch 3; log content: {'loss': 10.081628799438477, 'mae': 2.474828004837036, 'mse': 10.081628799438477}\n...Training: end of batch 4; log content: {'loss': 10.051630020141602, 'mae': 2.4885988235473633, 'mse': 10.051630020141602}\n...Training: end of batch 5; log content: {'loss': 10.532309532165527, 'mae': 2.5251615047454834, 'mse': 10.532309532165527}\n...Training: end of batch 6; log content: {'loss': 10.393463134765625, 'mae': 2.515692949295044, 'mse': 10.393463134765625}\n...Training: end of batch 7; log content: {'loss': 11.366023063659668, 'mae': 2.590003252029419, 'mse': 11.366023063659668}\nEnd epoch 153 of training; log content: {'loss': 11.366023063659668, 'mae': 2.590003252029419, 'mse': 11.366023063659668, 'val_loss': 9.950362205505371, 'val_mae': 2.434380531311035, 'val_mse': 9.950362205505371}\n...Training: end of batch 0; log content: {'loss': 12.078678131103516, 'mae': 2.5583720207214355, 'mse': 12.078678131103516}\n...Training: end of batch 1; log content: {'loss': 10.634740829467773, 'mae': 2.439736843109131, 'mse': 10.634740829467773}\n...Training: end of batch 2; log content: {'loss': 10.165716171264648, 'mae': 2.4750826358795166, 'mse': 10.165716171264648}\n...Training: end of batch 3; log content: {'loss': 10.81256103515625, 'mae': 2.4328622817993164, 'mse': 10.81256103515625}\n...Training: end of batch 4; log content: {'loss': 10.713401794433594, 'mae': 2.458549976348877, 'mse': 10.713401794433594}\n...Training: end of batch 5; log content: {'loss': 11.06714916229248, 'mae': 2.5012094974517822, 'mse': 11.06714916229248}\n...Training: end of batch 6; log content: {'loss': 10.93144702911377, 'mae': 2.5035669803619385, 'mse': 10.93144702911377}\n...Training: end of batch 7; log content: {'loss': 11.295204162597656, 'mae': 2.5618600845336914, 'mse': 11.295204162597656}\nEnd epoch 154 of training; log content: {'loss': 11.295204162597656, 'mae': 2.5618600845336914, 'mse': 11.295204162597656, 'val_loss': 10.063236236572266, 'val_mae': 2.448072671890259, 'val_mse': 10.063236236572266}\n...Training: end of batch 0; log content: {'loss': 12.958969116210938, 'mae': 2.6543049812316895, 'mse': 12.958969116210938}\n...Training: end of batch 1; log content: {'loss': 11.1339111328125, 'mae': 2.5089030265808105, 'mse': 11.1339111328125}\n...Training: end of batch 2; log content: {'loss': 10.843070983886719, 'mae': 2.565185308456421, 'mse': 10.843070983886719}\n...Training: end of batch 3; log content: {'loss': 12.304523468017578, 'mae': 2.65413761138916, 'mse': 12.304523468017578}\n...Training: end of batch 4; log content: {'loss': 12.123024940490723, 'mae': 2.6328532695770264, 'mse': 12.123024940490723}\n...Training: end of batch 5; log content: {'loss': 11.781940460205078, 'mae': 2.602247714996338, 'mse': 11.781940460205078}\n...Training: end of batch 6; log content: {'loss': 10.978659629821777, 'mae': 2.508864164352417, 'mse': 10.978659629821777}\n...Training: end of batch 7; log content: {'loss': 11.306915283203125, 'mae': 2.566584587097168, 'mse': 11.306915283203125}\nEnd epoch 155 of training; log content: {'loss': 11.306915283203125, 'mae': 2.566584587097168, 'mse': 11.306915283203125, 'val_loss': 10.012517929077148, 'val_mae': 2.4389657974243164, 'val_mse': 10.012517929077148}\n...Training: end of batch 0; log content: {'loss': 9.169777870178223, 'mae': 2.558445453643799, 'mse': 9.169777870178223}\n...Training: end of batch 1; log content: {'loss': 11.482081413269043, 'mae': 2.5572190284729004, 'mse': 11.482081413269043}\n...Training: end of batch 2; log content: {'loss': 10.659531593322754, 'mae': 2.4846761226654053, 'mse': 10.659531593322754}\n...Training: end of batch 3; log content: {'loss': 11.247203826904297, 'mae': 2.590956926345825, 'mse': 11.247203826904297}\n...Training: end of batch 4; log content: {'loss': 10.551206588745117, 'mae': 2.505786895751953, 'mse': 10.551206588745117}\n...Training: end of batch 5; log content: {'loss': 10.855975151062012, 'mae': 2.504728078842163, 'mse': 10.855975151062012}\n...Training: end of batch 6; log content: {'loss': 10.917447090148926, 'mae': 2.5123450756073, 'mse': 10.917447090148926}\n...Training: end of batch 7; log content: {'loss': 11.327824592590332, 'mae': 2.5700252056121826, 'mse': 11.327824592590332}\nEnd epoch 156 of training; log content: {'loss': 11.327824592590332, 'mae': 2.5700252056121826, 'mse': 11.327824592590332, 'val_loss': 10.04328441619873, 'val_mae': 2.429570436477661, 'val_mse': 10.04328441619873}\n...Training: end of batch 0; log content: {'loss': 11.658632278442383, 'mae': 2.5187666416168213, 'mse': 11.658632278442383}\n...Training: end of batch 1; log content: {'loss': 13.731672286987305, 'mae': 2.7240662574768066, 'mse': 13.731672286987305}\n...Training: end of batch 2; log content: {'loss': 12.085453033447266, 'mae': 2.6339852809906006, 'mse': 12.085453033447266}\n...Training: end of batch 3; log content: {'loss': 11.1966552734375, 'mae': 2.530426502227783, 'mse': 11.1966552734375}\n...Training: end of batch 4; log content: {'loss': 11.744779586791992, 'mae': 2.6179099082946777, 'mse': 11.744779586791992}\n...Training: end of batch 5; log content: {'loss': 11.4691743850708, 'mae': 2.58829402923584, 'mse': 11.4691743850708}\n...Training: end of batch 6; log content: {'loss': 11.26035213470459, 'mae': 2.5599637031555176, 'mse': 11.26035213470459}\n...Training: end of batch 7; log content: {'loss': 11.303580284118652, 'mae': 2.5769150257110596, 'mse': 11.303580284118652}\nEnd epoch 157 of training; log content: {'loss': 11.303580284118652, 'mae': 2.5769150257110596, 'mse': 11.303580284118652, 'val_loss': 10.046910285949707, 'val_mae': 2.4391391277313232, 'val_mse': 10.046910285949707}\n...Training: end of batch 0; log content: {'loss': 10.337141036987305, 'mae': 2.5758533477783203, 'mse': 10.337141036987305}\n...Training: end of batch 1; log content: {'loss': 12.638181686401367, 'mae': 2.757336378097534, 'mse': 12.638181686401367}\n...Training: end of batch 2; log content: {'loss': 11.206549644470215, 'mae': 2.539665699005127, 'mse': 11.206549644470215}\n...Training: end of batch 3; log content: {'loss': 10.75206184387207, 'mae': 2.541327953338623, 'mse': 10.75206184387207}\n...Training: end of batch 4; log content: {'loss': 11.460306167602539, 'mae': 2.60792875289917, 'mse': 11.460306167602539}\n...Training: end of batch 5; log content: {'loss': 11.525650024414062, 'mae': 2.633310556411743, 'mse': 11.525650024414062}\n...Training: end of batch 6; log content: {'loss': 11.220565795898438, 'mae': 2.6001110076904297, 'mse': 11.220565795898438}\n...Training: end of batch 7; log content: {'loss': 11.325765609741211, 'mae': 2.5893001556396484, 'mse': 11.325765609741211}\nEnd epoch 158 of training; log content: {'loss': 11.325765609741211, 'mae': 2.5893001556396484, 'mse': 11.325765609741211, 'val_loss': 9.927496910095215, 'val_mae': 2.424581289291382, 'val_mse': 9.927496910095215}\n...Training: end of batch 0; log content: {'loss': 14.266338348388672, 'mae': 2.9251136779785156, 'mse': 14.266338348388672}\n...Training: end of batch 1; log content: {'loss': 12.151606559753418, 'mae': 2.6846864223480225, 'mse': 12.151606559753418}\n...Training: end of batch 2; log content: {'loss': 11.62855052947998, 'mae': 2.634645700454712, 'mse': 11.62855052947998}\n...Training: end of batch 3; log content: {'loss': 11.075836181640625, 'mae': 2.606121301651001, 'mse': 11.075836181640625}\n...Training: end of batch 4; log content: {'loss': 11.354057312011719, 'mae': 2.615291118621826, 'mse': 11.354057312011719}\n...Training: end of batch 5; log content: {'loss': 11.46678638458252, 'mae': 2.5798463821411133, 'mse': 11.46678638458252}\n...Training: end of batch 6; log content: {'loss': 11.287138938903809, 'mae': 2.5654332637786865, 'mse': 11.287138938903809}\n...Training: end of batch 7; log content: {'loss': 11.275787353515625, 'mae': 2.577847957611084, 'mse': 11.275787353515625}\nEnd epoch 159 of training; log content: {'loss': 11.275787353515625, 'mae': 2.577847957611084, 'mse': 11.275787353515625, 'val_loss': 10.058048248291016, 'val_mae': 2.4455714225769043, 'val_mse': 10.058048248291016}\n...Training: end of batch 0; log content: {'loss': 14.113748550415039, 'mae': 2.9758455753326416, 'mse': 14.113748550415039}\n...Training: end of batch 1; log content: {'loss': 11.891839027404785, 'mae': 2.748469829559326, 'mse': 11.891839027404785}\n...Training: end of batch 2; log content: {'loss': 10.125994682312012, 'mae': 2.5322015285491943, 'mse': 10.125994682312012}\n...Training: end of batch 3; log content: {'loss': 12.24921989440918, 'mae': 2.7256453037261963, 'mse': 12.24921989440918}\n...Training: end of batch 4; log content: {'loss': 11.306402206420898, 'mae': 2.611081838607788, 'mse': 11.306402206420898}\n...Training: end of batch 5; log content: {'loss': 11.03675365447998, 'mae': 2.570934772491455, 'mse': 11.03675365447998}\n...Training: end of batch 6; log content: {'loss': 11.320600509643555, 'mae': 2.588505268096924, 'mse': 11.320600509643555}\n...Training: end of batch 7; log content: {'loss': 11.329011917114258, 'mae': 2.567622184753418, 'mse': 11.329011917114258}\nEnd epoch 160 of training; log content: {'loss': 11.329011917114258, 'mae': 2.567622184753418, 'mse': 11.329011917114258, 'val_loss': 10.046219825744629, 'val_mae': 2.449763774871826, 'val_mse': 10.046219825744629}\n...Training: end of batch 0; log content: {'loss': 5.3477463722229, 'mae': 1.8976324796676636, 'mse': 5.3477463722229}\n...Training: end of batch 1; log content: {'loss': 9.919045448303223, 'mae': 2.5113070011138916, 'mse': 9.919045448303223}\n...Training: end of batch 2; log content: {'loss': 12.241604804992676, 'mae': 2.7827394008636475, 'mse': 12.241604804992676}\n...Training: end of batch 3; log content: {'loss': 11.686628341674805, 'mae': 2.761530637741089, 'mse': 11.686628341674805}\n...Training: end of batch 4; log content: {'loss': 11.093545913696289, 'mae': 2.6297736167907715, 'mse': 11.093545913696289}\n...Training: end of batch 5; log content: {'loss': 12.079963684082031, 'mae': 2.671684980392456, 'mse': 12.079963684082031}\n...Training: end of batch 6; log content: {'loss': 12.036951065063477, 'mae': 2.6480460166931152, 'mse': 12.036951065063477}\n...Training: end of batch 7; log content: {'loss': 11.389592170715332, 'mae': 2.5667757987976074, 'mse': 11.389592170715332}\nEnd epoch 161 of training; log content: {'loss': 11.389592170715332, 'mae': 2.5667757987976074, 'mse': 11.389592170715332, 'val_loss': 10.031045913696289, 'val_mae': 2.445096254348755, 'val_mse': 10.031045913696289}\n...Training: end of batch 0; log content: {'loss': 13.01502513885498, 'mae': 2.70552396774292, 'mse': 13.01502513885498}\n...Training: end of batch 1; log content: {'loss': 10.696920394897461, 'mae': 2.483933687210083, 'mse': 10.696920394897461}\n...Training: end of batch 2; log content: {'loss': 10.511505126953125, 'mae': 2.471790313720703, 'mse': 10.511505126953125}\n...Training: end of batch 3; log content: {'loss': 10.473011016845703, 'mae': 2.5182290077209473, 'mse': 10.473011016845703}\n...Training: end of batch 4; log content: {'loss': 11.097246170043945, 'mae': 2.6505813598632812, 'mse': 11.097246170043945}\n...Training: end of batch 5; log content: {'loss': 11.14240550994873, 'mae': 2.623769521713257, 'mse': 11.14240550994873}\n...Training: end of batch 6; log content: {'loss': 11.252286911010742, 'mae': 2.592399835586548, 'mse': 11.252286911010742}\n...Training: end of batch 7; log content: {'loss': 11.32534122467041, 'mae': 2.5898289680480957, 'mse': 11.32534122467041}\nEnd epoch 162 of training; log content: {'loss': 11.32534122467041, 'mae': 2.5898289680480957, 'mse': 11.32534122467041, 'val_loss': 10.05302906036377, 'val_mae': 2.425473690032959, 'val_mse': 10.05302906036377}\n...Training: end of batch 0; log content: {'loss': 12.819713592529297, 'mae': 2.843618392944336, 'mse': 12.819713592529297}\n...Training: end of batch 1; log content: {'loss': 10.067550659179688, 'mae': 2.5381667613983154, 'mse': 10.067550659179688}\n...Training: end of batch 2; log content: {'loss': 10.315592765808105, 'mae': 2.517132520675659, 'mse': 10.315592765808105}\n...Training: end of batch 3; log content: {'loss': 9.933967590332031, 'mae': 2.4885523319244385, 'mse': 9.933967590332031}\n...Training: end of batch 4; log content: {'loss': 9.52558708190918, 'mae': 2.4236090183258057, 'mse': 9.52558708190918}\n...Training: end of batch 5; log content: {'loss': 9.870318412780762, 'mae': 2.4629576206207275, 'mse': 9.870318412780762}\n...Training: end of batch 6; log content: {'loss': 11.243288040161133, 'mae': 2.5994603633880615, 'mse': 11.243288040161133}\n...Training: end of batch 7; log content: {'loss': 11.35977554321289, 'mae': 2.5958077907562256, 'mse': 11.35977554321289}\nEnd epoch 163 of training; log content: {'loss': 11.35977554321289, 'mae': 2.5958077907562256, 'mse': 11.35977554321289, 'val_loss': 10.05042839050293, 'val_mae': 2.4375836849212646, 'val_mse': 10.05042839050293}\n...Training: end of batch 0; log content: {'loss': 11.24317741394043, 'mae': 2.566727876663208, 'mse': 11.24317741394043}\n...Training: end of batch 1; log content: {'loss': 12.578588485717773, 'mae': 2.7492716312408447, 'mse': 12.578588485717773}\n...Training: end of batch 2; log content: {'loss': 11.825900077819824, 'mae': 2.678769111633301, 'mse': 11.825900077819824}\n...Training: end of batch 3; log content: {'loss': 11.970405578613281, 'mae': 2.6464033126831055, 'mse': 11.970405578613281}\n...Training: end of batch 4; log content: {'loss': 11.286348342895508, 'mae': 2.603668689727783, 'mse': 11.286348342895508}\n...Training: end of batch 5; log content: {'loss': 11.547660827636719, 'mae': 2.624391555786133, 'mse': 11.547660827636719}\n...Training: end of batch 6; log content: {'loss': 11.149969100952148, 'mae': 2.570871114730835, 'mse': 11.149969100952148}\n...Training: end of batch 7; log content: {'loss': 11.42561149597168, 'mae': 2.571946382522583, 'mse': 11.42561149597168}\nEnd epoch 164 of training; log content: {'loss': 11.42561149597168, 'mae': 2.571946382522583, 'mse': 11.42561149597168, 'val_loss': 10.24879264831543, 'val_mae': 2.466451406478882, 'val_mse': 10.24879264831543}\n...Training: end of batch 0; log content: {'loss': 7.23439884185791, 'mae': 2.174576759338379, 'mse': 7.23439884185791}\n...Training: end of batch 1; log content: {'loss': 8.459851264953613, 'mae': 2.367825746536255, 'mse': 8.459851264953613}\n...Training: end of batch 2; log content: {'loss': 11.20661449432373, 'mae': 2.6366236209869385, 'mse': 11.20661449432373}\n...Training: end of batch 3; log content: {'loss': 10.356637954711914, 'mae': 2.48519229888916, 'mse': 10.356637954711914}\n...Training: end of batch 4; log content: {'loss': 11.145776748657227, 'mae': 2.5083394050598145, 'mse': 11.145776748657227}\n...Training: end of batch 5; log content: {'loss': 11.132882118225098, 'mae': 2.4972455501556396, 'mse': 11.132882118225098}\n...Training: end of batch 6; log content: {'loss': 10.885643005371094, 'mae': 2.5030674934387207, 'mse': 10.885643005371094}\n...Training: end of batch 7; log content: {'loss': 11.380668640136719, 'mae': 2.584359884262085, 'mse': 11.380668640136719}\nEnd epoch 165 of training; log content: {'loss': 11.380668640136719, 'mae': 2.584359884262085, 'mse': 11.380668640136719, 'val_loss': 9.928936004638672, 'val_mae': 2.415357828140259, 'val_mse': 9.928936004638672}\n...Training: end of batch 0; log content: {'loss': 12.007474899291992, 'mae': 2.307908058166504, 'mse': 12.007474899291992}\n...Training: end of batch 1; log content: {'loss': 9.21394157409668, 'mae': 2.129556179046631, 'mse': 9.21394157409668}\n...Training: end of batch 2; log content: {'loss': 12.097458839416504, 'mae': 2.499420166015625, 'mse': 12.097458839416504}\n...Training: end of batch 3; log content: {'loss': 11.704439163208008, 'mae': 2.5092062950134277, 'mse': 11.704439163208008}\n...Training: end of batch 4; log content: {'loss': 11.365730285644531, 'mae': 2.5573673248291016, 'mse': 11.365730285644531}\n...Training: end of batch 5; log content: {'loss': 12.083869934082031, 'mae': 2.636009931564331, 'mse': 12.083869934082031}\n...Training: end of batch 6; log content: {'loss': 11.675432205200195, 'mae': 2.6140975952148438, 'mse': 11.675432205200195}\n...Training: end of batch 7; log content: {'loss': 11.49229621887207, 'mae': 2.614187717437744, 'mse': 11.49229621887207}\nEnd epoch 166 of training; log content: {'loss': 11.49229621887207, 'mae': 2.614187717437744, 'mse': 11.49229621887207, 'val_loss': 9.923484802246094, 'val_mae': 2.418365478515625, 'val_mse': 9.923484802246094}\n...Training: end of batch 0; log content: {'loss': 8.62745475769043, 'mae': 2.307145595550537, 'mse': 8.62745475769043}\n...Training: end of batch 1; log content: {'loss': 12.768155097961426, 'mae': 2.7212986946105957, 'mse': 12.768155097961426}\n...Training: end of batch 2; log content: {'loss': 11.470601081848145, 'mae': 2.614349603652954, 'mse': 11.470601081848145}\n...Training: end of batch 3; log content: {'loss': 10.616251945495605, 'mae': 2.531930446624756, 'mse': 10.616251945495605}\n...Training: end of batch 4; log content: {'loss': 10.944439888000488, 'mae': 2.5590131282806396, 'mse': 10.944439888000488}\n...Training: end of batch 5; log content: {'loss': 11.976113319396973, 'mae': 2.614276885986328, 'mse': 11.976113319396973}\n...Training: end of batch 6; log content: {'loss': 11.843881607055664, 'mae': 2.6071324348449707, 'mse': 11.843881607055664}\n...Training: end of batch 7; log content: {'loss': 11.360050201416016, 'mae': 2.565579891204834, 'mse': 11.360050201416016}\nEnd epoch 167 of training; log content: {'loss': 11.360050201416016, 'mae': 2.565579891204834, 'mse': 11.360050201416016, 'val_loss': 10.158032417297363, 'val_mae': 2.465334415435791, 'val_mse': 10.158032417297363}\n...Training: end of batch 0; log content: {'loss': 11.029644012451172, 'mae': 2.6810550689697266, 'mse': 11.029644012451172}\n...Training: end of batch 1; log content: {'loss': 12.379919052124023, 'mae': 2.739025592803955, 'mse': 12.379919052124023}\n...Training: end of batch 2; log content: {'loss': 12.521868705749512, 'mae': 2.682286262512207, 'mse': 12.521868705749512}\n...Training: end of batch 3; log content: {'loss': 10.696907043457031, 'mae': 2.467900276184082, 'mse': 10.696907043457031}\n...Training: end of batch 4; log content: {'loss': 11.187153816223145, 'mae': 2.5579912662506104, 'mse': 11.187153816223145}\n...Training: end of batch 5; log content: {'loss': 11.249611854553223, 'mae': 2.576580286026001, 'mse': 11.249611854553223}\n...Training: end of batch 6; log content: {'loss': 11.408699989318848, 'mae': 2.5545268058776855, 'mse': 11.408699989318848}\n...Training: end of batch 7; log content: {'loss': 11.36212158203125, 'mae': 2.568709135055542, 'mse': 11.36212158203125}\nEnd epoch 168 of training; log content: {'loss': 11.36212158203125, 'mae': 2.568709135055542, 'mse': 11.36212158203125, 'val_loss': 10.082247734069824, 'val_mae': 2.45181941986084, 'val_mse': 10.082247734069824}\n...Training: end of batch 0; log content: {'loss': 13.359233856201172, 'mae': 2.871556282043457, 'mse': 13.359233856201172}\n...Training: end of batch 1; log content: {'loss': 11.93899154663086, 'mae': 2.733582019805908, 'mse': 11.93899154663086}\n...Training: end of batch 2; log content: {'loss': 12.758861541748047, 'mae': 2.804267168045044, 'mse': 12.758861541748047}\n...Training: end of batch 3; log content: {'loss': 11.530381202697754, 'mae': 2.6961374282836914, 'mse': 11.530381202697754}\n...Training: end of batch 4; log content: {'loss': 11.507315635681152, 'mae': 2.6588239669799805, 'mse': 11.507315635681152}\n...Training: end of batch 5; log content: {'loss': 10.844863891601562, 'mae': 2.599395990371704, 'mse': 10.844863891601562}\n...Training: end of batch 6; log content: {'loss': 11.093720436096191, 'mae': 2.5757083892822266, 'mse': 11.093720436096191}\n...Training: end of batch 7; log content: {'loss': 11.334068298339844, 'mae': 2.596450090408325, 'mse': 11.334068298339844}\nEnd epoch 169 of training; log content: {'loss': 11.334068298339844, 'mae': 2.596450090408325, 'mse': 11.334068298339844, 'val_loss': 9.987481117248535, 'val_mae': 2.4195759296417236, 'val_mse': 9.987481117248535}\n...Training: end of batch 0; log content: {'loss': 13.216771125793457, 'mae': 2.955604314804077, 'mse': 13.216771125793457}\n...Training: end of batch 1; log content: {'loss': 15.013406753540039, 'mae': 2.918088912963867, 'mse': 15.013406753540039}\n...Training: end of batch 2; log content: {'loss': 13.384696960449219, 'mae': 2.7915992736816406, 'mse': 13.384696960449219}\n...Training: end of batch 3; log content: {'loss': 12.474403381347656, 'mae': 2.727879047393799, 'mse': 12.474403381347656}\n...Training: end of batch 4; log content: {'loss': 11.047887802124023, 'mae': 2.5349278450012207, 'mse': 11.047887802124023}\n...Training: end of batch 5; log content: {'loss': 11.316798210144043, 'mae': 2.5755984783172607, 'mse': 11.316798210144043}\n...Training: end of batch 6; log content: {'loss': 11.695144653320312, 'mae': 2.6271307468414307, 'mse': 11.695144653320312}\n...Training: end of batch 7; log content: {'loss': 11.355034828186035, 'mae': 2.5846571922302246, 'mse': 11.355034828186035}\nEnd epoch 170 of training; log content: {'loss': 11.355034828186035, 'mae': 2.5846571922302246, 'mse': 11.355034828186035, 'val_loss': 9.978569984436035, 'val_mae': 2.449655294418335, 'val_mse': 9.978569984436035}\n...Training: end of batch 0; log content: {'loss': 9.703359603881836, 'mae': 2.45219087600708, 'mse': 9.703359603881836}\n...Training: end of batch 1; log content: {'loss': 9.744560241699219, 'mae': 2.450237274169922, 'mse': 9.744560241699219}\n...Training: end of batch 2; log content: {'loss': 12.278422355651855, 'mae': 2.7308590412139893, 'mse': 12.278422355651855}\n...Training: end of batch 3; log content: {'loss': 11.625653266906738, 'mae': 2.6500589847564697, 'mse': 11.625653266906738}\n...Training: end of batch 4; log content: {'loss': 10.472822189331055, 'mae': 2.5307095050811768, 'mse': 10.472822189331055}\n...Training: end of batch 5; log content: {'loss': 10.50302791595459, 'mae': 2.5029280185699463, 'mse': 10.50302791595459}\n...Training: end of batch 6; log content: {'loss': 11.094401359558105, 'mae': 2.536938190460205, 'mse': 11.094401359558105}\n...Training: end of batch 7; log content: {'loss': 11.303030967712402, 'mae': 2.5568437576293945, 'mse': 11.303030967712402}\nEnd epoch 171 of training; log content: {'loss': 11.303030967712402, 'mae': 2.5568437576293945, 'mse': 11.303030967712402, 'val_loss': 10.044892311096191, 'val_mae': 2.4386327266693115, 'val_mse': 10.044892311096191}\n...Training: end of batch 0; log content: {'loss': 9.123199462890625, 'mae': 2.4732682704925537, 'mse': 9.123199462890625}\n...Training: end of batch 1; log content: {'loss': 11.747102737426758, 'mae': 2.6607978343963623, 'mse': 11.747102737426758}\n...Training: end of batch 2; log content: {'loss': 10.99548625946045, 'mae': 2.6207032203674316, 'mse': 10.99548625946045}\n...Training: end of batch 3; log content: {'loss': 10.913640975952148, 'mae': 2.589031934738159, 'mse': 10.913640975952148}\n...Training: end of batch 4; log content: {'loss': 11.351850509643555, 'mae': 2.6255593299865723, 'mse': 11.351850509643555}\n...Training: end of batch 5; log content: {'loss': 11.838841438293457, 'mae': 2.6271848678588867, 'mse': 11.838841438293457}\n...Training: end of batch 6; log content: {'loss': 11.681718826293945, 'mae': 2.616279125213623, 'mse': 11.681718826293945}\n...Training: end of batch 7; log content: {'loss': 11.31669807434082, 'mae': 2.5649502277374268, 'mse': 11.31669807434082}\nEnd epoch 172 of training; log content: {'loss': 11.31669807434082, 'mae': 2.5649502277374268, 'mse': 11.31669807434082, 'val_loss': 10.085758209228516, 'val_mae': 2.439116954803467, 'val_mse': 10.085758209228516}\n...Training: end of batch 0; log content: {'loss': 12.69717025756836, 'mae': 2.9093446731567383, 'mse': 12.69717025756836}\n...Training: end of batch 1; log content: {'loss': 10.430994987487793, 'mae': 2.5613253116607666, 'mse': 10.430994987487793}\n...Training: end of batch 2; log content: {'loss': 10.179488182067871, 'mae': 2.5349323749542236, 'mse': 10.179488182067871}\n...Training: end of batch 3; log content: {'loss': 10.12882137298584, 'mae': 2.5122017860412598, 'mse': 10.12882137298584}\n...Training: end of batch 4; log content: {'loss': 11.888006210327148, 'mae': 2.6139299869537354, 'mse': 11.888006210327148}\n...Training: end of batch 5; log content: {'loss': 11.258800506591797, 'mae': 2.554121971130371, 'mse': 11.258800506591797}\n...Training: end of batch 6; log content: {'loss': 11.715492248535156, 'mae': 2.616793155670166, 'mse': 11.715492248535156}\n...Training: end of batch 7; log content: {'loss': 11.312769889831543, 'mae': 2.575939178466797, 'mse': 11.312769889831543}\nEnd epoch 173 of training; log content: {'loss': 11.312769889831543, 'mae': 2.575939178466797, 'mse': 11.312769889831543, 'val_loss': 9.953975677490234, 'val_mae': 2.4134318828582764, 'val_mse': 9.953975677490234}\n...Training: end of batch 0; log content: {'loss': 11.136772155761719, 'mae': 2.425450563430786, 'mse': 11.136772155761719}\n...Training: end of batch 1; log content: {'loss': 12.366096496582031, 'mae': 2.705493927001953, 'mse': 12.366096496582031}\n...Training: end of batch 2; log content: {'loss': 12.739238739013672, 'mae': 2.6864821910858154, 'mse': 12.739238739013672}\n...Training: end of batch 3; log content: {'loss': 13.70020866394043, 'mae': 2.7576403617858887, 'mse': 13.70020866394043}\n...Training: end of batch 4; log content: {'loss': 12.854257583618164, 'mae': 2.704357624053955, 'mse': 12.854257583618164}\n...Training: end of batch 5; log content: {'loss': 12.463282585144043, 'mae': 2.6779205799102783, 'mse': 12.463282585144043}\n...Training: end of batch 6; log content: {'loss': 11.974160194396973, 'mae': 2.635570526123047, 'mse': 11.974160194396973}\n...Training: end of batch 7; log content: {'loss': 11.379234313964844, 'mae': 2.5932810306549072, 'mse': 11.379234313964844}\nEnd epoch 174 of training; log content: {'loss': 11.379234313964844, 'mae': 2.5932810306549072, 'mse': 11.379234313964844, 'val_loss': 9.993764877319336, 'val_mae': 2.4473440647125244, 'val_mse': 9.993764877319336}\n...Training: end of batch 0; log content: {'loss': 16.27212905883789, 'mae': 2.7731680870056152, 'mse': 16.27212905883789}\n...Training: end of batch 1; log content: {'loss': 12.91700553894043, 'mae': 2.6200201511383057, 'mse': 12.91700553894043}\n...Training: end of batch 2; log content: {'loss': 11.95559024810791, 'mae': 2.587344169616699, 'mse': 11.95559024810791}\n...Training: end of batch 3; log content: {'loss': 11.693862915039062, 'mae': 2.615771770477295, 'mse': 11.693862915039062}\n...Training: end of batch 4; log content: {'loss': 11.244161605834961, 'mae': 2.537874221801758, 'mse': 11.244161605834961}\n...Training: end of batch 5; log content: {'loss': 11.380888938903809, 'mae': 2.583176612854004, 'mse': 11.380888938903809}\n...Training: end of batch 6; log content: {'loss': 11.23381519317627, 'mae': 2.5708656311035156, 'mse': 11.23381519317627}\n...Training: end of batch 7; log content: {'loss': 11.27780532836914, 'mae': 2.5743765830993652, 'mse': 11.27780532836914}\nEnd epoch 175 of training; log content: {'loss': 11.27780532836914, 'mae': 2.5743765830993652, 'mse': 11.27780532836914, 'val_loss': 9.964136123657227, 'val_mae': 2.4386041164398193, 'val_mse': 9.964136123657227}\n...Training: end of batch 0; log content: {'loss': 13.65677261352539, 'mae': 2.9129815101623535, 'mse': 13.65677261352539}\n...Training: end of batch 1; log content: {'loss': 12.034591674804688, 'mae': 2.7488677501678467, 'mse': 12.034591674804688}\n...Training: end of batch 2; log content: {'loss': 11.750720024108887, 'mae': 2.6758382320404053, 'mse': 11.750720024108887}\n...Training: end of batch 3; log content: {'loss': 12.583878517150879, 'mae': 2.7339367866516113, 'mse': 12.583878517150879}\n...Training: end of batch 4; log content: {'loss': 12.01774787902832, 'mae': 2.6931841373443604, 'mse': 12.01774787902832}\n...Training: end of batch 5; log content: {'loss': 11.006099700927734, 'mae': 2.5692882537841797, 'mse': 11.006099700927734}\n...Training: end of batch 6; log content: {'loss': 10.994826316833496, 'mae': 2.5685057640075684, 'mse': 10.994826316833496}\n...Training: end of batch 7; log content: {'loss': 11.373122215270996, 'mae': 2.601541757583618, 'mse': 11.373122215270996}\nEnd epoch 176 of training; log content: {'loss': 11.373122215270996, 'mae': 2.601541757583618, 'mse': 11.373122215270996, 'val_loss': 9.90715217590332, 'val_mae': 2.4221138954162598, 'val_mse': 9.90715217590332}\n...Training: end of batch 0; log content: {'loss': 11.032552719116211, 'mae': 2.5026073455810547, 'mse': 11.032552719116211}\n...Training: end of batch 1; log content: {'loss': 12.00615119934082, 'mae': 2.6434988975524902, 'mse': 12.00615119934082}\n...Training: end of batch 2; log content: {'loss': 11.0613431930542, 'mae': 2.62845778465271, 'mse': 11.0613431930542}\n...Training: end of batch 3; log content: {'loss': 11.741140365600586, 'mae': 2.683201313018799, 'mse': 11.741140365600586}\n...Training: end of batch 4; log content: {'loss': 11.901728630065918, 'mae': 2.708287000656128, 'mse': 11.901728630065918}\n...Training: end of batch 5; log content: {'loss': 12.20263385772705, 'mae': 2.677962064743042, 'mse': 12.20263385772705}\n...Training: end of batch 6; log content: {'loss': 11.705015182495117, 'mae': 2.635309934616089, 'mse': 11.705015182495117}\n...Training: end of batch 7; log content: {'loss': 11.282707214355469, 'mae': 2.5816335678100586, 'mse': 11.282707214355469}\nEnd epoch 177 of training; log content: {'loss': 11.282707214355469, 'mae': 2.5816335678100586, 'mse': 11.282707214355469, 'val_loss': 10.028436660766602, 'val_mae': 2.43953537940979, 'val_mse': 10.028436660766602}\n...Training: end of batch 0; log content: {'loss': 10.62320327758789, 'mae': 2.624807834625244, 'mse': 10.62320327758789}\n...Training: end of batch 1; log content: {'loss': 9.660175323486328, 'mae': 2.5279927253723145, 'mse': 9.660175323486328}\n...Training: end of batch 2; log content: {'loss': 11.665352821350098, 'mae': 2.6862566471099854, 'mse': 11.665352821350098}\n...Training: end of batch 3; log content: {'loss': 11.250024795532227, 'mae': 2.5958783626556396, 'mse': 11.250024795532227}\n...Training: end of batch 4; log content: {'loss': 11.954780578613281, 'mae': 2.6225101947784424, 'mse': 11.954780578613281}\n...Training: end of batch 5; log content: {'loss': 11.233318328857422, 'mae': 2.567955255508423, 'mse': 11.233318328857422}\n...Training: end of batch 6; log content: {'loss': 10.824880599975586, 'mae': 2.5359911918640137, 'mse': 10.824880599975586}\n...Training: end of batch 7; log content: {'loss': 11.531343460083008, 'mae': 2.5784454345703125, 'mse': 11.531343460083008}\nEnd epoch 178 of training; log content: {'loss': 11.531343460083008, 'mae': 2.5784454345703125, 'mse': 11.531343460083008, 'val_loss': 10.370213508605957, 'val_mae': 2.4777987003326416, 'val_mse': 10.370213508605957}\n...Training: end of batch 0; log content: {'loss': 10.944154739379883, 'mae': 2.4908242225646973, 'mse': 10.944154739379883}\n...Training: end of batch 1; log content: {'loss': 11.977303504943848, 'mae': 2.5972352027893066, 'mse': 11.977303504943848}\n...Training: end of batch 2; log content: {'loss': 10.652271270751953, 'mae': 2.465121030807495, 'mse': 10.652271270751953}\n...Training: end of batch 3; log content: {'loss': 11.588926315307617, 'mae': 2.5203287601470947, 'mse': 11.588926315307617}\n...Training: end of batch 4; log content: {'loss': 11.56958293914795, 'mae': 2.5531208515167236, 'mse': 11.56958293914795}\n...Training: end of batch 5; log content: {'loss': 11.447117805480957, 'mae': 2.585233449935913, 'mse': 11.447117805480957}\n...Training: end of batch 6; log content: {'loss': 11.416496276855469, 'mae': 2.6086935997009277, 'mse': 11.416496276855469}\n...Training: end of batch 7; log content: {'loss': 11.572837829589844, 'mae': 2.612990379333496, 'mse': 11.572837829589844}\nEnd epoch 179 of training; log content: {'loss': 11.572837829589844, 'mae': 2.612990379333496, 'mse': 11.572837829589844, 'val_loss': 10.037246704101562, 'val_mae': 2.388094902038574, 'val_mse': 10.037246704101562}\n...Training: end of batch 0; log content: {'loss': 10.860647201538086, 'mae': 2.544431209564209, 'mse': 10.860647201538086}\n...Training: end of batch 1; log content: {'loss': 11.597414016723633, 'mae': 2.538588523864746, 'mse': 11.597414016723633}\n...Training: end of batch 2; log content: {'loss': 11.49509048461914, 'mae': 2.5776901245117188, 'mse': 11.49509048461914}\n...Training: end of batch 3; log content: {'loss': 10.958684921264648, 'mae': 2.5083136558532715, 'mse': 10.958684921264648}\n...Training: end of batch 4; log content: {'loss': 10.671549797058105, 'mae': 2.5065271854400635, 'mse': 10.671549797058105}\n...Training: end of batch 5; log content: {'loss': 10.816970825195312, 'mae': 2.523019313812256, 'mse': 10.816970825195312}\n...Training: end of batch 6; log content: {'loss': 11.332231521606445, 'mae': 2.5987966060638428, 'mse': 11.332231521606445}\n...Training: end of batch 7; log content: {'loss': 11.388763427734375, 'mae': 2.6195762157440186, 'mse': 11.388763427734375}\nEnd epoch 180 of training; log content: {'loss': 11.388763427734375, 'mae': 2.6195762157440186, 'mse': 11.388763427734375, 'val_loss': 10.06644344329834, 'val_mae': 2.454575777053833, 'val_mse': 10.06644344329834}\n...Training: end of batch 0; log content: {'loss': 11.030506134033203, 'mae': 2.6955935955047607, 'mse': 11.030506134033203}\n...Training: end of batch 1; log content: {'loss': 11.007217407226562, 'mae': 2.7013611793518066, 'mse': 11.007217407226562}\n...Training: end of batch 2; log content: {'loss': 11.698566436767578, 'mae': 2.7199935913085938, 'mse': 11.698566436767578}\n...Training: end of batch 3; log content: {'loss': 12.145471572875977, 'mae': 2.750056505203247, 'mse': 12.145471572875977}\n...Training: end of batch 4; log content: {'loss': 10.725131034851074, 'mae': 2.5271084308624268, 'mse': 10.725131034851074}\n...Training: end of batch 5; log content: {'loss': 11.01098918914795, 'mae': 2.5360379219055176, 'mse': 11.01098918914795}\n...Training: end of batch 6; log content: {'loss': 11.76476001739502, 'mae': 2.601536989212036, 'mse': 11.76476001739502}\n...Training: end of batch 7; log content: {'loss': 11.36896800994873, 'mae': 2.560046911239624, 'mse': 11.36896800994873}\nEnd epoch 181 of training; log content: {'loss': 11.36896800994873, 'mae': 2.560046911239624, 'mse': 11.36896800994873, 'val_loss': 10.044886589050293, 'val_mae': 2.4472968578338623, 'val_mse': 10.044886589050293}\n...Training: end of batch 0; log content: {'loss': 10.010817527770996, 'mae': 2.4557242393493652, 'mse': 10.010817527770996}\n...Training: end of batch 1; log content: {'loss': 7.799569129943848, 'mae': 2.252143621444702, 'mse': 7.799569129943848}\n...Training: end of batch 2; log content: {'loss': 8.848548889160156, 'mae': 2.3508477210998535, 'mse': 8.848548889160156}\n...Training: end of batch 3; log content: {'loss': 9.923237800598145, 'mae': 2.4051060676574707, 'mse': 9.923237800598145}\n...Training: end of batch 4; log content: {'loss': 11.150463104248047, 'mae': 2.5832531452178955, 'mse': 11.150463104248047}\n...Training: end of batch 5; log content: {'loss': 10.979819297790527, 'mae': 2.5589001178741455, 'mse': 10.979819297790527}\n...Training: end of batch 6; log content: {'loss': 11.507512092590332, 'mae': 2.5949554443359375, 'mse': 11.507512092590332}\n...Training: end of batch 7; log content: {'loss': 11.310765266418457, 'mae': 2.5694355964660645, 'mse': 11.310765266418457}\nEnd epoch 182 of training; log content: {'loss': 11.310765266418457, 'mae': 2.5694355964660645, 'mse': 11.310765266418457, 'val_loss': 10.071159362792969, 'val_mae': 2.4275119304656982, 'val_mse': 10.071159362792969}\n...Training: end of batch 0; log content: {'loss': 18.899179458618164, 'mae': 3.0641632080078125, 'mse': 18.899179458618164}\n...Training: end of batch 1; log content: {'loss': 12.452582359313965, 'mae': 2.4986462593078613, 'mse': 12.452582359313965}\n...Training: end of batch 2; log content: {'loss': 11.818011283874512, 'mae': 2.5206809043884277, 'mse': 11.818011283874512}\n...Training: end of batch 3; log content: {'loss': 12.902260780334473, 'mae': 2.749669313430786, 'mse': 12.902260780334473}\n...Training: end of batch 4; log content: {'loss': 12.722397804260254, 'mae': 2.7539830207824707, 'mse': 12.722397804260254}\n...Training: end of batch 5; log content: {'loss': 11.89254379272461, 'mae': 2.665735960006714, 'mse': 11.89254379272461}\n...Training: end of batch 6; log content: {'loss': 11.785455703735352, 'mae': 2.651798963546753, 'mse': 11.785455703735352}\n...Training: end of batch 7; log content: {'loss': 11.335494995117188, 'mae': 2.6013877391815186, 'mse': 11.335494995117188}\nEnd epoch 183 of training; log content: {'loss': 11.335494995117188, 'mae': 2.6013877391815186, 'mse': 11.335494995117188, 'val_loss': 10.01076889038086, 'val_mae': 2.418879270553589, 'val_mse': 10.01076889038086}\n...Training: end of batch 0; log content: {'loss': 10.710779190063477, 'mae': 2.665019989013672, 'mse': 10.710779190063477}\n...Training: end of batch 1; log content: {'loss': 11.047752380371094, 'mae': 2.658442258834839, 'mse': 11.047752380371094}\n...Training: end of batch 2; log content: {'loss': 11.297257423400879, 'mae': 2.639927625656128, 'mse': 11.297257423400879}\n...Training: end of batch 3; log content: {'loss': 11.068123817443848, 'mae': 2.594456911087036, 'mse': 11.068123817443848}\n...Training: end of batch 4; log content: {'loss': 11.185919761657715, 'mae': 2.6144371032714844, 'mse': 11.185919761657715}\n...Training: end of batch 5; log content: {'loss': 11.010672569274902, 'mae': 2.5733349323272705, 'mse': 11.010672569274902}\n...Training: end of batch 6; log content: {'loss': 11.163351058959961, 'mae': 2.565896511077881, 'mse': 11.163351058959961}\n...Training: end of batch 7; log content: {'loss': 11.313985824584961, 'mae': 2.578061103820801, 'mse': 11.313985824584961}\nEnd epoch 184 of training; log content: {'loss': 11.313985824584961, 'mae': 2.578061103820801, 'mse': 11.313985824584961, 'val_loss': 10.176248550415039, 'val_mae': 2.4588122367858887, 'val_mse': 10.176248550415039}\n...Training: end of batch 0; log content: {'loss': 12.156949996948242, 'mae': 2.5146584510803223, 'mse': 12.156949996948242}\n...Training: end of batch 1; log content: {'loss': 9.65148639678955, 'mae': 2.2924137115478516, 'mse': 9.65148639678955}\n...Training: end of batch 2; log content: {'loss': 12.022374153137207, 'mae': 2.542185068130493, 'mse': 12.022374153137207}\n...Training: end of batch 3; log content: {'loss': 11.952051162719727, 'mae': 2.569626808166504, 'mse': 11.952051162719727}\n...Training: end of batch 4; log content: {'loss': 12.299440383911133, 'mae': 2.619100332260132, 'mse': 12.299440383911133}\n...Training: end of batch 5; log content: {'loss': 12.073593139648438, 'mae': 2.6180334091186523, 'mse': 12.073593139648438}\n...Training: end of batch 6; log content: {'loss': 11.613677978515625, 'mae': 2.573538064956665, 'mse': 11.613677978515625}\n...Training: end of batch 7; log content: {'loss': 11.31631851196289, 'mae': 2.563560724258423, 'mse': 11.31631851196289}\nEnd epoch 185 of training; log content: {'loss': 11.31631851196289, 'mae': 2.563560724258423, 'mse': 11.31631851196289, 'val_loss': 9.960661888122559, 'val_mae': 2.441446542739868, 'val_mse': 9.960661888122559}\n...Training: end of batch 0; log content: {'loss': 7.170766830444336, 'mae': 2.2462172508239746, 'mse': 7.170766830444336}\n...Training: end of batch 1; log content: {'loss': 8.625864028930664, 'mae': 2.3097777366638184, 'mse': 8.625864028930664}\n...Training: end of batch 2; log content: {'loss': 13.424304962158203, 'mae': 2.7670371532440186, 'mse': 13.424304962158203}\n...Training: end of batch 3; log content: {'loss': 12.820706367492676, 'mae': 2.6723179817199707, 'mse': 12.820706367492676}\n...Training: end of batch 4; log content: {'loss': 11.768342971801758, 'mae': 2.5763301849365234, 'mse': 11.768342971801758}\n...Training: end of batch 5; log content: {'loss': 11.71701717376709, 'mae': 2.5922818183898926, 'mse': 11.71701717376709}\n...Training: end of batch 6; log content: {'loss': 11.310460090637207, 'mae': 2.5524654388427734, 'mse': 11.310460090637207}\n...Training: end of batch 7; log content: {'loss': 11.316512107849121, 'mae': 2.5709030628204346, 'mse': 11.316512107849121}\nEnd epoch 186 of training; log content: {'loss': 11.316512107849121, 'mae': 2.5709030628204346, 'mse': 11.316512107849121, 'val_loss': 9.931154251098633, 'val_mae': 2.4376120567321777, 'val_mse': 9.931154251098633}\n...Training: end of batch 0; log content: {'loss': 10.335029602050781, 'mae': 2.41636061668396, 'mse': 10.335029602050781}\n...Training: end of batch 1; log content: {'loss': 11.706104278564453, 'mae': 2.6235055923461914, 'mse': 11.706104278564453}\n...Training: end of batch 2; log content: {'loss': 11.258903503417969, 'mae': 2.627131938934326, 'mse': 11.258903503417969}\n...Training: end of batch 3; log content: {'loss': 11.128082275390625, 'mae': 2.574845552444458, 'mse': 11.128082275390625}\n...Training: end of batch 4; log content: {'loss': 11.52487564086914, 'mae': 2.6461379528045654, 'mse': 11.52487564086914}\n...Training: end of batch 5; log content: {'loss': 10.483525276184082, 'mae': 2.5044333934783936, 'mse': 10.483525276184082}\n...Training: end of batch 6; log content: {'loss': 11.483758926391602, 'mae': 2.591215133666992, 'mse': 11.483758926391602}\n...Training: end of batch 7; log content: {'loss': 11.408536911010742, 'mae': 2.5945656299591064, 'mse': 11.408536911010742}\nEnd epoch 187 of training; log content: {'loss': 11.408536911010742, 'mae': 2.5945656299591064, 'mse': 11.408536911010742, 'val_loss': 9.920571327209473, 'val_mae': 2.4153554439544678, 'val_mse': 9.920571327209473}\n...Training: end of batch 0; log content: {'loss': 10.956067085266113, 'mae': 2.7244274616241455, 'mse': 10.956067085266113}\n...Training: end of batch 1; log content: {'loss': 10.7359619140625, 'mae': 2.4530954360961914, 'mse': 10.7359619140625}\n...Training: end of batch 2; log content: {'loss': 10.724589347839355, 'mae': 2.5009281635284424, 'mse': 10.724589347839355}\n...Training: end of batch 3; log content: {'loss': 11.666692733764648, 'mae': 2.57076358795166, 'mse': 11.666692733764648}\n...Training: end of batch 4; log content: {'loss': 10.84626579284668, 'mae': 2.5038256645202637, 'mse': 10.84626579284668}\n...Training: end of batch 5; log content: {'loss': 10.07482624053955, 'mae': 2.4135549068450928, 'mse': 10.07482624053955}\n...Training: end of batch 6; log content: {'loss': 11.039634704589844, 'mae': 2.5442423820495605, 'mse': 11.039634704589844}\n...Training: end of batch 7; log content: {'loss': 11.32557487487793, 'mae': 2.581951141357422, 'mse': 11.32557487487793}\nEnd epoch 188 of training; log content: {'loss': 11.32557487487793, 'mae': 2.581951141357422, 'mse': 11.32557487487793, 'val_loss': 10.098315238952637, 'val_mae': 2.446504592895508, 'val_mse': 10.098315238952637}\n...Training: end of batch 0; log content: {'loss': 13.89045524597168, 'mae': 2.7548062801361084, 'mse': 13.89045524597168}\n...Training: end of batch 1; log content: {'loss': 14.045276641845703, 'mae': 2.880577564239502, 'mse': 14.045276641845703}\n...Training: end of batch 2; log content: {'loss': 13.302401542663574, 'mae': 2.8752963542938232, 'mse': 13.302401542663574}\n...Training: end of batch 3; log content: {'loss': 11.67186164855957, 'mae': 2.6956169605255127, 'mse': 11.67186164855957}\n...Training: end of batch 4; log content: {'loss': 11.274958610534668, 'mae': 2.6032092571258545, 'mse': 11.274958610534668}\n...Training: end of batch 5; log content: {'loss': 11.533195495605469, 'mae': 2.5794808864593506, 'mse': 11.533195495605469}\n...Training: end of batch 6; log content: {'loss': 11.530477523803711, 'mae': 2.5979652404785156, 'mse': 11.530477523803711}\n...Training: end of batch 7; log content: {'loss': 11.288313865661621, 'mae': 2.564206600189209, 'mse': 11.288313865661621}\nEnd epoch 189 of training; log content: {'loss': 11.288313865661621, 'mae': 2.564206600189209, 'mse': 11.288313865661621, 'val_loss': 10.030559539794922, 'val_mae': 2.453382968902588, 'val_mse': 10.030559539794922}\n...Training: end of batch 0; log content: {'loss': 10.889518737792969, 'mae': 2.663450241088867, 'mse': 10.889518737792969}\n...Training: end of batch 1; log content: {'loss': 10.668334007263184, 'mae': 2.5173258781433105, 'mse': 10.668334007263184}\n...Training: end of batch 2; log content: {'loss': 10.420572280883789, 'mae': 2.500552177429199, 'mse': 10.420572280883789}\n...Training: end of batch 3; log content: {'loss': 11.942867279052734, 'mae': 2.594574213027954, 'mse': 11.942867279052734}\n...Training: end of batch 4; log content: {'loss': 11.694787979125977, 'mae': 2.564116954803467, 'mse': 11.694787979125977}\n...Training: end of batch 5; log content: {'loss': 11.848968505859375, 'mae': 2.615145683288574, 'mse': 11.848968505859375}\n...Training: end of batch 6; log content: {'loss': 11.352322578430176, 'mae': 2.5756402015686035, 'mse': 11.352322578430176}\n...Training: end of batch 7; log content: {'loss': 11.4319429397583, 'mae': 2.5783493518829346, 'mse': 11.4319429397583}\nEnd epoch 190 of training; log content: {'loss': 11.4319429397583, 'mae': 2.5783493518829346, 'mse': 11.4319429397583, 'val_loss': 10.018858909606934, 'val_mae': 2.4492390155792236, 'val_mse': 10.018858909606934}\n...Training: end of batch 0; log content: {'loss': 12.893989562988281, 'mae': 2.9776315689086914, 'mse': 12.893989562988281}\n...Training: end of batch 1; log content: {'loss': 15.15043830871582, 'mae': 3.1311919689178467, 'mse': 15.15043830871582}\n...Training: end of batch 2; log content: {'loss': 14.979942321777344, 'mae': 3.0003530979156494, 'mse': 14.979942321777344}\n...Training: end of batch 3; log content: {'loss': 13.24548625946045, 'mae': 2.8223791122436523, 'mse': 13.24548625946045}\n...Training: end of batch 4; log content: {'loss': 12.039040565490723, 'mae': 2.6458230018615723, 'mse': 12.039040565490723}\n...Training: end of batch 5; log content: {'loss': 12.583718299865723, 'mae': 2.7130610942840576, 'mse': 12.583718299865723}\n...Training: end of batch 6; log content: {'loss': 12.092070579528809, 'mae': 2.6970715522766113, 'mse': 12.092070579528809}\n...Training: end of batch 7; log content: {'loss': 11.527531623840332, 'mae': 2.6455025672912598, 'mse': 11.527531623840332}\nEnd epoch 191 of training; log content: {'loss': 11.527531623840332, 'mae': 2.6455025672912598, 'mse': 11.527531623840332, 'val_loss': 10.017593383789062, 'val_mae': 2.393460512161255, 'val_mse': 10.017593383789062}\n...Training: end of batch 0; log content: {'loss': 13.144662857055664, 'mae': 2.5868148803710938, 'mse': 13.144662857055664}\n...Training: end of batch 1; log content: {'loss': 11.970497131347656, 'mae': 2.479170083999634, 'mse': 11.970497131347656}\n...Training: end of batch 2; log content: {'loss': 12.431102752685547, 'mae': 2.6835901737213135, 'mse': 12.431102752685547}\n...Training: end of batch 3; log content: {'loss': 12.699860572814941, 'mae': 2.696746826171875, 'mse': 12.699860572814941}\n...Training: end of batch 4; log content: {'loss': 12.171792984008789, 'mae': 2.6604816913604736, 'mse': 12.171792984008789}\n...Training: end of batch 5; log content: {'loss': 11.869942665100098, 'mae': 2.626854181289673, 'mse': 11.869942665100098}\n...Training: end of batch 6; log content: {'loss': 11.232975959777832, 'mae': 2.5653021335601807, 'mse': 11.232975959777832}\n...Training: end of batch 7; log content: {'loss': 11.270176887512207, 'mae': 2.584883213043213, 'mse': 11.270176887512207}\nEnd epoch 192 of training; log content: {'loss': 11.270176887512207, 'mae': 2.584883213043213, 'mse': 11.270176887512207, 'val_loss': 10.011590003967285, 'val_mae': 2.4477226734161377, 'val_mse': 10.011590003967285}\n...Training: end of batch 0; log content: {'loss': 18.09479522705078, 'mae': 3.2611896991729736, 'mse': 18.09479522705078}\n...Training: end of batch 1; log content: {'loss': 13.319002151489258, 'mae': 2.8126883506774902, 'mse': 13.319002151489258}\n...Training: end of batch 2; log content: {'loss': 13.257779121398926, 'mae': 2.730658769607544, 'mse': 13.257779121398926}\n...Training: end of batch 3; log content: {'loss': 12.209757804870605, 'mae': 2.638478994369507, 'mse': 12.209757804870605}\n...Training: end of batch 4; log content: {'loss': 11.475103378295898, 'mae': 2.5523955821990967, 'mse': 11.475103378295898}\n...Training: end of batch 5; log content: {'loss': 11.064095497131348, 'mae': 2.499495267868042, 'mse': 11.064095497131348}\n...Training: end of batch 6; log content: {'loss': 11.610214233398438, 'mae': 2.5901527404785156, 'mse': 11.610214233398438}\n...Training: end of batch 7; log content: {'loss': 11.452547073364258, 'mae': 2.5631179809570312, 'mse': 11.452547073364258}\nEnd epoch 193 of training; log content: {'loss': 11.452547073364258, 'mae': 2.5631179809570312, 'mse': 11.452547073364258, 'val_loss': 10.274883270263672, 'val_mae': 2.47552227973938, 'val_mse': 10.274883270263672}\n...Training: end of batch 0; log content: {'loss': 8.411665916442871, 'mae': 2.368035316467285, 'mse': 8.411665916442871}\n...Training: end of batch 1; log content: {'loss': 7.6459245681762695, 'mae': 2.205575466156006, 'mse': 7.6459245681762695}\n...Training: end of batch 2; log content: {'loss': 12.684544563293457, 'mae': 2.6172492504119873, 'mse': 12.684544563293457}\n...Training: end of batch 3; log content: {'loss': 11.128491401672363, 'mae': 2.4709019660949707, 'mse': 11.128491401672363}\n...Training: end of batch 4; log content: {'loss': 10.357407569885254, 'mae': 2.4345486164093018, 'mse': 10.357407569885254}\n...Training: end of batch 5; log content: {'loss': 10.149569511413574, 'mae': 2.4064042568206787, 'mse': 10.149569511413574}\n...Training: end of batch 6; log content: {'loss': 10.932205200195312, 'mae': 2.4949963092803955, 'mse': 10.932205200195312}\n...Training: end of batch 7; log content: {'loss': 11.263784408569336, 'mae': 2.540478467941284, 'mse': 11.263784408569336}\nEnd epoch 194 of training; log content: {'loss': 11.263784408569336, 'mae': 2.540478467941284, 'mse': 11.263784408569336, 'val_loss': 9.988774299621582, 'val_mae': 2.4112911224365234, 'val_mse': 9.988774299621582}\n...Training: end of batch 0; log content: {'loss': 13.303007125854492, 'mae': 2.9126923084259033, 'mse': 13.303007125854492}\n...Training: end of batch 1; log content: {'loss': 12.706836700439453, 'mae': 2.768955945968628, 'mse': 12.706836700439453}\n...Training: end of batch 2; log content: {'loss': 12.77249526977539, 'mae': 2.7331535816192627, 'mse': 12.77249526977539}\n...Training: end of batch 3; log content: {'loss': 11.276640892028809, 'mae': 2.6116392612457275, 'mse': 11.276640892028809}\n...Training: end of batch 4; log content: {'loss': 10.87334156036377, 'mae': 2.4980053901672363, 'mse': 10.87334156036377}\n...Training: end of batch 5; log content: {'loss': 11.017112731933594, 'mae': 2.534858465194702, 'mse': 11.017112731933594}\n...Training: end of batch 6; log content: {'loss': 11.095329284667969, 'mae': 2.5610477924346924, 'mse': 11.095329284667969}\n...Training: end of batch 7; log content: {'loss': 11.682337760925293, 'mae': 2.6611380577087402, 'mse': 11.682337760925293}\nEnd epoch 195 of training; log content: {'loss': 11.682337760925293, 'mae': 2.6611380577087402, 'mse': 11.682337760925293, 'val_loss': 9.976396560668945, 'val_mae': 2.4069738388061523, 'val_mse': 9.976396560668945}\n...Training: end of batch 0; log content: {'loss': 11.92460823059082, 'mae': 2.65749454498291, 'mse': 11.92460823059082}\n...Training: end of batch 1; log content: {'loss': 9.687929153442383, 'mae': 2.307471752166748, 'mse': 9.687929153442383}\n...Training: end of batch 2; log content: {'loss': 11.12741470336914, 'mae': 2.568523645401001, 'mse': 11.12741470336914}\n...Training: end of batch 3; log content: {'loss': 10.54334831237793, 'mae': 2.5183374881744385, 'mse': 10.54334831237793}\n...Training: end of batch 4; log content: {'loss': 12.172150611877441, 'mae': 2.629615545272827, 'mse': 12.172150611877441}\n...Training: end of batch 5; log content: {'loss': 11.71884822845459, 'mae': 2.5969526767730713, 'mse': 11.71884822845459}\n...Training: end of batch 6; log content: {'loss': 11.318061828613281, 'mae': 2.5801713466644287, 'mse': 11.318061828613281}\n...Training: end of batch 7; log content: {'loss': 11.254826545715332, 'mae': 2.5834710597991943, 'mse': 11.254826545715332}\nEnd epoch 196 of training; log content: {'loss': 11.254826545715332, 'mae': 2.5834710597991943, 'mse': 11.254826545715332, 'val_loss': 10.18259048461914, 'val_mae': 2.4756734371185303, 'val_mse': 10.18259048461914}\n...Training: end of batch 0; log content: {'loss': 11.556892395019531, 'mae': 2.6353073120117188, 'mse': 11.556892395019531}\n...Training: end of batch 1; log content: {'loss': 8.778450012207031, 'mae': 2.3082234859466553, 'mse': 8.778450012207031}\n...Training: end of batch 2; log content: {'loss': 9.038290977478027, 'mae': 2.372509241104126, 'mse': 9.038290977478027}\n...Training: end of batch 3; log content: {'loss': 11.60081672668457, 'mae': 2.57218599319458, 'mse': 11.60081672668457}\n...Training: end of batch 4; log content: {'loss': 11.59294605255127, 'mae': 2.5910637378692627, 'mse': 11.59294605255127}\n...Training: end of batch 5; log content: {'loss': 10.992366790771484, 'mae': 2.4983439445495605, 'mse': 10.992366790771484}\n...Training: end of batch 6; log content: {'loss': 10.711426734924316, 'mae': 2.466998338699341, 'mse': 10.711426734924316}\n...Training: end of batch 7; log content: {'loss': 11.449220657348633, 'mae': 2.554680347442627, 'mse': 11.449220657348633}\nEnd epoch 197 of training; log content: {'loss': 11.449220657348633, 'mae': 2.554680347442627, 'mse': 11.449220657348633, 'val_loss': 10.268985748291016, 'val_mae': 2.4731454849243164, 'val_mse': 10.268985748291016}\n...Training: end of batch 0; log content: {'loss': 10.628802299499512, 'mae': 2.620109796524048, 'mse': 10.628802299499512}\n...Training: end of batch 1; log content: {'loss': 10.143600463867188, 'mae': 2.5435192584991455, 'mse': 10.143600463867188}\n...Training: end of batch 2; log content: {'loss': 10.126933097839355, 'mae': 2.584285259246826, 'mse': 10.126933097839355}\n...Training: end of batch 3; log content: {'loss': 9.047457695007324, 'mae': 2.402186393737793, 'mse': 9.047457695007324}\n...Training: end of batch 4; log content: {'loss': 8.83821964263916, 'mae': 2.364543914794922, 'mse': 8.83821964263916}\n...Training: end of batch 5; log content: {'loss': 9.734293937683105, 'mae': 2.4591360092163086, 'mse': 9.734293937683105}\n...Training: end of batch 6; log content: {'loss': 10.568252563476562, 'mae': 2.53448224067688, 'mse': 10.568252563476562}\n...Training: end of batch 7; log content: {'loss': 11.268345832824707, 'mae': 2.570556879043579, 'mse': 11.268345832824707}\nEnd epoch 198 of training; log content: {'loss': 11.268345832824707, 'mae': 2.570556879043579, 'mse': 11.268345832824707, 'val_loss': 9.947859764099121, 'val_mae': 2.3989226818084717, 'val_mse': 9.947859764099121}\n...Training: end of batch 0; log content: {'loss': 8.63300895690918, 'mae': 2.380335807800293, 'mse': 8.63300895690918}\n...Training: end of batch 1; log content: {'loss': 8.580171585083008, 'mae': 2.367427110671997, 'mse': 8.580171585083008}\n...Training: end of batch 2; log content: {'loss': 7.650222301483154, 'mae': 2.184840440750122, 'mse': 7.650222301483154}\n...Training: end of batch 3; log content: {'loss': 8.033283233642578, 'mae': 2.233978748321533, 'mse': 8.033283233642578}\n...Training: end of batch 4; log content: {'loss': 8.718062400817871, 'mae': 2.2637577056884766, 'mse': 8.718062400817871}\n...Training: end of batch 5; log content: {'loss': 10.1848726272583, 'mae': 2.4445695877075195, 'mse': 10.1848726272583}\n...Training: end of batch 6; log content: {'loss': 11.269768714904785, 'mae': 2.550537347793579, 'mse': 11.269768714904785}\n...Training: end of batch 7; log content: {'loss': 11.37527847290039, 'mae': 2.5982468128204346, 'mse': 11.37527847290039}\nEnd epoch 199 of training; log content: {'loss': 11.37527847290039, 'mae': 2.5982468128204346, 'mse': 11.37527847290039, 'val_loss': 9.941184997558594, 'val_mae': 2.4139516353607178, 'val_mse': 9.941184997558594}\nStop training; log content: {'loss': 11.37527847290039, 'mae': 2.5982468128204346, 'mse': 11.37527847290039, 'val_loss': 9.941184997558594, 'val_mae': 2.4139516353607178, 'val_mse': 9.941184997558594}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Custom Callback N. 2\n\nLet us write another example of custom callback. This time, let's implement a custom early stopping mechanism on a pre-defined Validation MAE value.","metadata":{"cell_id":"ab4c781aad6d41268dd855432806c077","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"class MyEarlyStopping(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        if(logs['val_mae']< 10.0):\n            print(\"\\nReached MAE < 10.0, so cancelling training!\")\n            self.model.stop_training = True\n","metadata":{"cell_id":"45ef8a4d550b4981ad3e4427314f2801","source_hash":"1b00bb66","execution_start":1667314459156,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":17},{"cell_type":"code","source":"my_es_callback = MyEarlyStopping()\n\nmodel = get_model(train_features)\nhistory = model.fit(train_features, train_labels, epochs=200, validation_split = 0.2, callbacks=[my_es_callback])","metadata":{"cell_id":"2038562d4b8849cdb55af1ffe2b9532e","source_hash":"ed667925","execution_start":1667314462224,"execution_millis":1491,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch 1/200\n8/8 [==============================] - 0s 20ms/step - loss: 577.8655 - mae: 22.8908 - mse: 577.8655 - val_loss: 558.1935 - val_mae: 22.9385 - val_mse: 558.1935\nEpoch 2/200\n8/8 [==============================] - 0s 4ms/step - loss: 513.8099 - mae: 22.0938 - mse: 513.8099 - val_loss: 502.0301 - val_mae: 22.0637 - val_mse: 502.0301\nEpoch 3/200\n8/8 [==============================] - 0s 4ms/step - loss: 467.7130 - mae: 21.2513 - mse: 467.7130 - val_loss: 466.1808 - val_mae: 21.2829 - val_mse: 466.1808\nEpoch 4/200\n8/8 [==============================] - 0s 11ms/step - loss: 437.0400 - mae: 20.5381 - mse: 437.0400 - val_loss: 435.1846 - val_mae: 20.5282 - val_mse: 435.1846\nEpoch 5/200\n8/8 [==============================] - 0s 4ms/step - loss: 404.9682 - mae: 19.7394 - mse: 404.9682 - val_loss: 405.5331 - val_mae: 19.8247 - val_mse: 405.5331\nEpoch 6/200\n8/8 [==============================] - 0s 9ms/step - loss: 374.3518 - mae: 18.9648 - mse: 374.3518 - val_loss: 378.6115 - val_mae: 19.1381 - val_mse: 378.6115\nEpoch 7/200\n8/8 [==============================] - 0s 3ms/step - loss: 345.8747 - mae: 18.2056 - mse: 345.8747 - val_loss: 352.5949 - val_mae: 18.4423 - val_mse: 352.5949\nEpoch 8/200\n8/8 [==============================] - 0s 10ms/step - loss: 319.3569 - mae: 17.4576 - mse: 319.3569 - val_loss: 328.2958 - val_mae: 17.7603 - val_mse: 328.2958\nEpoch 9/200\n8/8 [==============================] - 0s 4ms/step - loss: 294.6160 - mae: 16.7413 - mse: 294.6160 - val_loss: 305.1707 - val_mae: 17.0852 - val_mse: 305.1707\nEpoch 10/200\n8/8 [==============================] - 0s 10ms/step - loss: 272.0023 - mae: 16.0443 - mse: 272.0023 - val_loss: 282.0620 - val_mae: 16.3970 - val_mse: 282.0620\nEpoch 11/200\n8/8 [==============================] - 0s 3ms/step - loss: 250.4865 - mae: 15.3640 - mse: 250.4865 - val_loss: 260.2694 - val_mae: 15.7277 - val_mse: 260.2694\nEpoch 12/200\n8/8 [==============================] - 0s 3ms/step - loss: 230.2960 - mae: 14.7039 - mse: 230.2960 - val_loss: 239.6697 - val_mae: 15.0684 - val_mse: 239.6697\nEpoch 13/200\n8/8 [==============================] - 0s 4ms/step - loss: 212.2672 - mae: 14.0791 - mse: 212.2672 - val_loss: 222.1355 - val_mae: 14.4594 - val_mse: 222.1355\nEpoch 14/200\n8/8 [==============================] - 0s 5ms/step - loss: 193.9885 - mae: 13.4171 - mse: 193.9885 - val_loss: 203.5594 - val_mae: 13.8199 - val_mse: 203.5594\nEpoch 15/200\n8/8 [==============================] - 0s 8ms/step - loss: 177.8307 - mae: 12.8122 - mse: 177.8307 - val_loss: 186.2678 - val_mae: 13.1964 - val_mse: 186.2678\nEpoch 16/200\n8/8 [==============================] - 0s 3ms/step - loss: 162.8561 - mae: 12.2237 - mse: 162.8561 - val_loss: 171.8281 - val_mae: 12.6297 - val_mse: 171.8281\nEpoch 17/200\n8/8 [==============================] - 0s 3ms/step - loss: 149.0029 - mae: 11.6520 - mse: 149.0029 - val_loss: 157.0088 - val_mae: 12.0432 - val_mse: 157.0088\nEpoch 18/200\n8/8 [==============================] - 0s 10ms/step - loss: 136.8024 - mae: 11.1141 - mse: 136.8024 - val_loss: 144.2221 - val_mae: 11.4894 - val_mse: 144.2221\nEpoch 19/200\n8/8 [==============================] - 0s 3ms/step - loss: 124.6310 - mae: 10.5621 - mse: 124.6310 - val_loss: 131.3373 - val_mae: 10.9378 - val_mse: 131.3373\nEpoch 20/200\n8/8 [==============================] - 0s 5ms/step - loss: 113.5090 - mae: 10.0394 - mse: 113.5090 - val_loss: 120.6375 - val_mae: 10.4312 - val_mse: 120.6375\nEpoch 21/200\n1/8 [==>...........................] - ETA: 0s - loss: 107.9358 - mae: 9.6960 - mse: 107.9358\nReached MAE < 10.0, so cancelling training!\n8/8 [==============================] - 0s 3ms/step - loss: 103.6177 - mae: 9.5317 - mse: 103.6177 - val_loss: 110.7492 - val_mae: 9.9359 - val_mse: 110.7492\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"0b8637753f804db993d7a1eaf748cac6","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7817b2ad-42a3-441f-8072-b020be286d3c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python","version":"3.9.14","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"f3ad8b1b3e0f4b1cb3a3b00ad53bdcf2","deepnote_execution_queue":[]}}