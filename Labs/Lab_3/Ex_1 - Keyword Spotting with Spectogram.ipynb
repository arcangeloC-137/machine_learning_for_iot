{"cells":[{"cell_type":"markdown","source":"## Keyword Spotting with Spectogram\n\nTrain and Evaluate a model for keyword spotting on the ***Mini Dpeech Command*** dataset.\n\n### 1.1 ***Data Ingestion and Pre-processing***\n\nCompute resized spectogram features with following hyperparameters:\n- Downsampling Rate -> `16000 H`\n- STFT frame lenght -> `40ms`\n- STFT frame overlap -> `50%`\n- Resize to `32x32`\n","metadata":{"tags":[],"cell_id":"f95b04993b044cf19965b36b1f188dbc","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"tags":[],"cell_id":"e25f028964cc423283b5ef56662a6795","source_hash":"7a93dab8","execution_start":1670752267616,"execution_millis":4968,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-12-11 09:51:07.588013: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-11 09:51:07.715154: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2022-12-11 09:51:07.719618: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2022-12-11 09:51:07.719631: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-12-11 09:51:07.753495: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2022-12-11 09:51:09.426070: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2022-12-11 09:51:09.426127: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2022-12-11 09:51:09.426133: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Note: if frame lenght is `40m`, and we want a frame overlap of `50%`, then the frame step will be half of the frame lenght (`20ms`)","metadata":{"tags":[],"cell_id":"380a3b331130421fac001f0a25a1bb3d","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"PREPROCESSING_ARGS = {\n    'downsampling_rate' : 16000,\n    'frame_lenght_in_s' : 0.04,\n    'frame_step_in_s' : 0.02,\n}\n\nTRAINING_ARGS = {\n    'batch_size' : 20,\n    'initial_learining_rate' : 0.01,\n    'end_learining_rate' : 1.e-5,\n    'epochs' : 10,\n}","metadata":{"tags":[],"cell_id":"00c0587814dc41f1a5fd876fb9784253","source_hash":"acfef63d","execution_start":1670755371912,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"We create `Train`, `Val` and `Test` datasets:","metadata":{"tags":[],"cell_id":"ca3e839ddade4eb4a7c227916f8904cf","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"!ls /datasets/minispeechcommands","metadata":{"tags":[],"cell_id":"157974bc0a244efcbe0eaaf4627d34a4","source_hash":"a4e64d0a","execution_start":1670752272632,"execution_millis":220,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"msc-test.zip  msc-train.zip  msc-val.zip\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#!unzip -q /datasets/minispeechcommands/msc-test.zip\n#!unzip -q /datasets/minispeechcommands/msc-train.zip\n#!unzip -q /datasets/minispeechcommands/msc-val.zip","metadata":{"tags":[],"cell_id":"e48a30b3b6a04646ac7bca0c5f89623c","source_hash":"abe44230","execution_start":1670752272895,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_ds = tf.data.Dataset.list_files('msc-train/*')\nval_ds = tf.data.Dataset.list_files('msc-val/*')\ntest_ds = tf.data.Dataset.list_files('msc-test/*')\n\nfrom preprocessing import LABELS\nfrom preprocessing import get_spectrogram\nfrom functools import partial\n\ndef get_spectrogram_and_labels(filename, downsampling_rate, frame_lenght_in_s, frame_step_in_s):\n    \n    spectrogram, sampling_rate, label = get_spectrogram(filename, downsampling_rate, frame_lenght_in_s, frame_step_in_s)\n\n    return spectrogram, label\n\n# partial function freezes some arguments, while others can be passed as insput\nget_frozen_spectrogram = partial(get_spectrogram_and_labels, **PREPROCESSING_ARGS)\n\nfor spectrogram, label in train_ds.map(get_frozen_spectrogram).take(1):\n    SHAPE = spectrogram.shape","metadata":{"tags":[],"cell_id":"44b7cab481104cf6a499792549ee3305","source_hash":"2226e29a","execution_start":1670752273627,"execution_millis":1424,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-12-11 09:51:13.606336: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2022-12-11 09:51:13.606371: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2022-12-11 09:51:13.606386: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-1d083ad3-985d-4856-9229-610932999833): /proc/driver/nvidia/version does not exist\n2022-12-11 09:51:13.606716: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-11 09:51:14.251498: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n2022-12-11 09:51:14.251721: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 AVX512F FMA\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-11 09:51:14.670948: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-11 09:51:14.672697: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-11 09:51:14.672889: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Pre-processing data retrieved:","metadata":{"tags":[],"cell_id":"d578eee60ce44f008ae2a1db0f3fe7e0","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"print(SHAPE)","metadata":{"tags":[],"cell_id":"05a3d781a1ac4fefa460b78e4517b825","source_hash":"7b8512f5","execution_start":1670752276228,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"(49, 321)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def preprocess(filename):\n    signal, label = get_frozen_spectrogram(filename)\n\n    print(type(signal))\n    signal.set_shape(SHAPE)\n    signal = tf.expand_dims(signal, -1)\n\n    # we resize the signal to the 32x23 shape\n    signal = tf.image.resize(signal, [32,32])\n\n    label_id = tf.argmax(label == LABELS)\n\n    return signal, label_id\n\nbatch_size = TRAINING_ARGS['batch_size']\nepochs = TRAINING_ARGS['epochs']\n\ntrain_ds = train_ds.map(preprocess).batch(batch_size).cache()\nval_ds = val_ds.map(preprocess).batch(batch_size).cache()\ntest_ds = test_ds.map(preprocess).batch(batch_size).cache()\n","metadata":{"tags":[],"cell_id":"1e6a34fca0b54e7bb84e5363b51d0801","source_hash":"cba50c93","execution_start":1670752278296,"execution_millis":706,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n<class 'tensorflow.python.framework.ops.Tensor'>\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-11 09:51:18.491001: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-11 09:51:18.492518: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-11 09:51:18.492729: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n<class 'tensorflow.python.framework.ops.Tensor'>\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-11 09:51:18.711282: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-11 09:51:18.712761: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-11 09:51:18.712942: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n<class 'tensorflow.python.framework.ops.Tensor'>\n2022-12-11 09:51:18.930569: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-11 09:51:18.932130: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-11 09:51:18.932318: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"for example_batch, example_labels in train_ds.take(1):\n    print('Batch Shape:', example_batch.shape)\n    print('Data Shape:', example_batch.shape[1:])\n    print('Labels:', example_labels)","metadata":{"tags":[],"cell_id":"c298655ea63b4e53866c63a59d021474","source_hash":"7a87aeec","execution_start":1670752415847,"execution_millis":320,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Batch Shape: (20, 32, 32, 1)\nData Shape: (32, 32, 1)\nLabels: tf.Tensor([7 1 6 2 2 3 2 3 3 0 4 7 4 1 0 1 2 2 7 5], shape=(20,), dtype=int64)\n2022-12-11 09:53:36.149547: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### 1.2 ***Model Creation***\n\nWe develope a Convolutional Neural Network (CNN) using `Sequential` function, which groups a linear stack of layers into a `tf.keras.Model`.","metadata":{"tags":[],"cell_id":"03a649ffd5ed448f9498529ce96d3a80","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n    tf.keras.layers.Conv2D(filters=128, kernel_size=[3,3], strides=[2,2], use_bias=False, padding='valid'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Conv2D(filters=128, kernel_size=[3,3], strides=[1,1], use_bias=False, padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Conv2D(filters=128, kernel_size=[3,3], strides=[1,1], use_bias=False, padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(units=len(LABELS)),\n    tf.keras.layers.Softmax()\n])","metadata":{"tags":[],"cell_id":"459b652f01d14368843a704a1278a466","source_hash":"ddc99c28","execution_start":1670753060334,"execution_millis":34,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model.summary()","metadata":{"tags":[],"cell_id":"5bbc3309ad48461f93110dec57fc43fa","source_hash":"4e6a3b95","execution_start":1670753072187,"execution_millis":28,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 15, 15, 128)       1152      \n                                                                 \n batch_normalization (BatchN  (None, 15, 15, 128)      512       \n ormalization)                                                   \n                                                                 \n re_lu (ReLU)                (None, 15, 15, 128)       0         \n                                                                 \n conv2d_1 (Conv2D)           (None, 15, 15, 128)       147456    \n                                                                 \n batch_normalization_1 (Batc  (None, 15, 15, 128)      512       \n hNormalization)                                                 \n                                                                 \n re_lu_1 (ReLU)              (None, 15, 15, 128)       0         \n                                                                 \n conv2d_2 (Conv2D)           (None, 15, 15, 128)       147456    \n                                                                 \n batch_normalization_2 (Batc  (None, 15, 15, 128)      512       \n hNormalization)                                                 \n                                                                 \n re_lu_2 (ReLU)              (None, 15, 15, 128)       0         \n                                                                 \n global_average_pooling2d (G  (None, 128)              0         \n lobalAveragePooling2D)                                          \n                                                                 \n dense (Dense)               (None, 8)                 1032      \n                                                                 \n softmax (Softmax)           (None, 8)                 0         \n                                                                 \n=================================================================\nTotal params: 298,632\nTrainable params: 297,864\nNon-trainable params: 768\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### 1.3 ***Model Training***\n\nWe now train the model created above, using the following setup:\n- `SparseCategoricalCrossEntropy` with `from_logits=False` as loss function\n- `Adam` as Optimizer, setting a linear decay schedule for the learning rate\n- `SparseCategoricalAccuracy` to evaluate the prediciton quality ","metadata":{"tags":[],"cell_id":"79f7d3b8eb0f4d9b94201c75673632c2","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\ninitial_learning_rate = TRAINING_ARGS['initial_learining_rate']\nend_learining_rate = TRAINING_ARGS['end_learining_rate']\n\n# note: by passing decay_steps as done below we are telling keras \n# to monotonically decrease learing rate over all the training time\n\nlinear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=initial_learning_rate,\n    end_learning_rate=end_learining_rate,\n    decay_steps=len(train_ds) * epochs\n)\n\noptimizer = tf.optimizers.Adam(learning_rate=linear_decay)\nmetrics = [tf.metrics.SparseCategoricalAccuracy()]\nmodel.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\nhistory = model.fit(train_ds, epochs=epochs, validation_data=val_ds)","metadata":{"tags":[],"cell_id":"81eaffebfcad432e8e558b86573d588d","source_hash":"43a48e01","execution_start":1670755404431,"execution_millis":436145,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch 1/10\n320/320 [==============================] - 43s 132ms/step - loss: 0.3424 - sparse_categorical_accuracy: 0.8859 - val_loss: 2.9663 - val_sparse_categorical_accuracy: 0.5175\nEpoch 2/10\n320/320 [==============================] - 42s 132ms/step - loss: 0.2820 - sparse_categorical_accuracy: 0.9052 - val_loss: 1.7325 - val_sparse_categorical_accuracy: 0.5888\nEpoch 3/10\n320/320 [==============================] - 42s 132ms/step - loss: 0.2233 - sparse_categorical_accuracy: 0.9280 - val_loss: 1.4479 - val_sparse_categorical_accuracy: 0.6475\nEpoch 4/10\n320/320 [==============================] - 52s 161ms/step - loss: 0.1916 - sparse_categorical_accuracy: 0.9380 - val_loss: 0.7861 - val_sparse_categorical_accuracy: 0.7688\nEpoch 5/10\n320/320 [==============================] - 46s 145ms/step - loss: 0.1641 - sparse_categorical_accuracy: 0.9514 - val_loss: 1.3678 - val_sparse_categorical_accuracy: 0.6650\nEpoch 6/10\n320/320 [==============================] - 42s 132ms/step - loss: 0.1558 - sparse_categorical_accuracy: 0.9528 - val_loss: 0.8373 - val_sparse_categorical_accuracy: 0.7812\nEpoch 7/10\n320/320 [==============================] - 42s 131ms/step - loss: 0.1275 - sparse_categorical_accuracy: 0.9641 - val_loss: 0.6505 - val_sparse_categorical_accuracy: 0.8225\nEpoch 8/10\n320/320 [==============================] - 42s 131ms/step - loss: 0.1064 - sparse_categorical_accuracy: 0.9711 - val_loss: 0.5616 - val_sparse_categorical_accuracy: 0.8438\nEpoch 9/10\n320/320 [==============================] - 42s 132ms/step - loss: 0.0922 - sparse_categorical_accuracy: 0.9759 - val_loss: 0.4699 - val_sparse_categorical_accuracy: 0.8700\nEpoch 10/10\n320/320 [==============================] - 42s 132ms/step - loss: 0.0832 - sparse_categorical_accuracy: 0.9786 - val_loss: 0.4383 - val_sparse_categorical_accuracy: 0.8700\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"history.history","metadata":{"tags":[],"cell_id":"3eadefa5a9e9422a9865ffb039a0a724","source_hash":"af97ebf4","execution_start":1670755861966,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"{'loss': [0.3424413204193115,\n  0.28197720646858215,\n  0.22325679659843445,\n  0.19155332446098328,\n  0.16407480835914612,\n  0.1558287888765335,\n  0.12745708227157593,\n  0.10640266537666321,\n  0.09218808263540268,\n  0.08317052572965622],\n 'sparse_categorical_accuracy': [0.885937511920929,\n  0.9051562547683716,\n  0.9279687404632568,\n  0.9379687309265137,\n  0.9514062404632568,\n  0.9528124928474426,\n  0.964062511920929,\n  0.9710937738418579,\n  0.9759374856948853,\n  0.9785937666893005],\n 'val_loss': [2.966348648071289,\n  1.7325356006622314,\n  1.4478559494018555,\n  0.786061704158783,\n  1.367796778678894,\n  0.837324321269989,\n  0.650453507900238,\n  0.5616068840026855,\n  0.46985331177711487,\n  0.4382949471473694],\n 'val_sparse_categorical_accuracy': [0.5174999833106995,\n  0.5887500047683716,\n  0.6474999785423279,\n  0.768750011920929,\n  0.6650000214576721,\n  0.78125,\n  0.8224999904632568,\n  0.84375,\n  0.8700000047683716,\n  0.8700000047683716]}"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"### 1.4 ***Model Testing***\n\nWe now test our model over the `test_ds`.","metadata":{"tags":[],"cell_id":"d031efda0e5742c199b8b157ee3b7796","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(test_ds)","metadata":{"tags":[],"cell_id":"53825700500c46a18f76022cfe3d5747","source_hash":"e8cd931b","execution_start":1670755921584,"execution_millis":9046,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"40/40 [==============================] - 9s 216ms/step - loss: 0.4258 - sparse_categorical_accuracy: 0.8763\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"training_loss = history.history['loss'][-1]\ntraining_accuracy = history.history['sparse_categorical_accuracy'][-1]\nval_loss = history.history['val_loss'][-1]\nval_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n\nprint(f'Training Loss: {training_loss:.4f}')\nprint(f'Training Accuracy: {training_accuracy*100.:.2f}%')\nprint()\nprint(f'Validation Loss: {val_loss:.4f}')\nprint(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\nprint()\nprint(f'Test Loss: {test_loss:.4f}')\nprint(f'Test Accuracy: {test_accuracy*100.:.2f}%')","metadata":{"tags":[],"cell_id":"fae8dd7679a8475a82f385ee25b7e292","source_hash":"d8271375","execution_start":1670755943610,"execution_millis":5,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Training Loss: 0.0832\nTraining Accuracy: 97.86%\n\nValidation Loss: 0.4383\nValidation Accuracy: 87.00%\n\nTest Loss: 0.4258\nTest Accuracy: 87.63%\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### 1.5 ***Model Saving***\n\nWe eventually save our model:","metadata":{"tags":[],"cell_id":"c8a6f19cd425439caae2c642c311c206","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"from time import time\nimport os\n\ntimestamp = int(time())\n\nsaved_model_dir = f'./saved_models/{timestamp}'\nif not os.path.exists(saved_model_dir):\n    os.makedirs(saved_model_dir)\nmodel.save(saved_model_dir)","metadata":{"tags":[],"cell_id":"c89694f2ee5644e48822908dc75dd7b6","source_hash":"d8018753","execution_start":1670756047578,"execution_millis":1008,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./saved_models/1670756047/assets\nINFO:tensorflow:Assets written to: ./saved_models/1670756047/assets\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"And the Hyper-Parameters, with their results:","metadata":{"tags":[],"cell_id":"f73d3a28d118482682be94ad6f90a3cc","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import pandas as pd\n\noutput_dict = {\n    'timestamp': timestamp,\n    **PREPROCESSING_ARGS,\n    **TRAINING_ARGS,\n    'test_accuracy': test_accuracy\n}\n\ndf = pd.DataFrame([output_dict])\n\noutput_path='./spectrogram_results.csv'\ndf.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)","metadata":{"tags":[],"cell_id":"5cb406d51e41429e98cf9762eb13c3c2","source_hash":"dddbd8d9","execution_start":1670756146655,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=1d083ad3-985d-4856-9229-610932999833' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"7f9418815d76400299788c31b155ccf9","deepnote_execution_queue":[]}}